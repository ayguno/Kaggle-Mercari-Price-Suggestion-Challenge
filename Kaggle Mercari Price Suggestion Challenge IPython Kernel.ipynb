{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and processing the data sets\n",
    "\n",
    "Let's start with loading the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " '.git',\n",
       " '.ipynb_checkpoints',\n",
       " '.Rapp.history',\n",
       " '.RDataTmp',\n",
       " '.Rhistory',\n",
       " 'GBM.rds',\n",
       " 'GBM.stack.rds',\n",
       " 'Kaggle Mercari Price Suggestion Challenge IPython Kernel.ipynb',\n",
       " 'KaggleMercariPriceSuggestionChallengeKernel.Rmd',\n",
       " 'Mercari Price Suggestion Challenge | Kaggle.pdf',\n",
       " 'merged_table.rds',\n",
       " 'mini_subtrain.rds',\n",
       " 'mini_subtrain_locked.rds',\n",
       " 'mini_train.rds',\n",
       " 'new_stack_set.rds',\n",
       " 'new_stack_set2.rds',\n",
       " 'RF.BC.rds',\n",
       " 'RF.rds',\n",
       " 'RF.stack.rds',\n",
       " 'RF.stack2nd.rds',\n",
       " 'sample_submission.csv',\n",
       " 'sample_submission.csv.7z',\n",
       " 'second.stack.set.rds',\n",
       " 'subtrain.rds',\n",
       " 'subtrain_locked.rds',\n",
       " 'SVM.rds',\n",
       " 'SVM.stack.rds',\n",
       " 'test.csv',\n",
       " 'test.rds',\n",
       " 'test.tsv.7z',\n",
       " 'test_locked.rds',\n",
       " 'train.csv',\n",
       " 'train.rds',\n",
       " 'train.tsv.7z',\n",
       " 'v.txt',\n",
       " 'validation.rds',\n",
       " 'validation_locked.rds',\n",
       " 'X_holdout.csv',\n",
       " 'X_train.csv',\n",
       " 'xGBM.BC.rds',\n",
       " 'xGBM.BC2.rds',\n",
       " 'xGBM.rds',\n",
       " 'xGBM.stack.rds',\n",
       " 'xGBM.stack2nd.rds',\n",
       " 'y_holdout.csv',\n",
       " 'y_train.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MLB Cincinnati Reds T Shirt Size XL</td>\n",
       "      <td>3</td>\n",
       "      <td>Men/Tops/T-shirts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>No description yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Razer BlackWidow Chroma Keyboard</td>\n",
       "      <td>3</td>\n",
       "      <td>Electronics/Computers &amp; Tablets/Components &amp; P...</td>\n",
       "      <td>Razer</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>This keyboard is in great condition and works ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AVA-VIV Blouse</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Tops &amp; Blouses/Blouse</td>\n",
       "      <td>Target</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Adorable top with a hint of lace and a key hol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Leather Horse Statues</td>\n",
       "      <td>1</td>\n",
       "      <td>Home/Home Décor/Home Décor Accents</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>New with tags. Leather horses. Retail for [rm]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>24K GOLD plated rose</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Jewelry/Necklaces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Complete with certificate of authenticity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_id                                 name  item_condition_id  \\\n",
       "0         0  MLB Cincinnati Reds T Shirt Size XL                  3   \n",
       "1         1     Razer BlackWidow Chroma Keyboard                  3   \n",
       "2         2                       AVA-VIV Blouse                  1   \n",
       "3         3                Leather Horse Statues                  1   \n",
       "4         4                 24K GOLD plated rose                  1   \n",
       "\n",
       "                                       category_name brand_name  price  \\\n",
       "0                                  Men/Tops/T-shirts        NaN   10.0   \n",
       "1  Electronics/Computers & Tablets/Components & P...      Razer   52.0   \n",
       "2                        Women/Tops & Blouses/Blouse     Target   10.0   \n",
       "3                 Home/Home Décor/Home Décor Accents        NaN   35.0   \n",
       "4                            Women/Jewelry/Necklaces        NaN   44.0   \n",
       "\n",
       "   shipping                                   item_description  \n",
       "0         1                                 No description yet  \n",
       "1         0  This keyboard is in great condition and works ...  \n",
       "2         1  Adorable top with a hint of lace and a key hol...  \n",
       "3         1  New with tags. Leather horses. Retail for [rm]...  \n",
       "4         0          Complete with certificate of authenticity  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1482535 entries, 0 to 1482534\n",
      "Data columns (total 8 columns):\n",
      "train_id             1482535 non-null int64\n",
      "name                 1482535 non-null object\n",
      "item_condition_id    1482535 non-null int64\n",
      "category_name        1476208 non-null object\n",
      "brand_name           849853 non-null object\n",
      "price                1482535 non-null float64\n",
      "shipping             1482535 non-null int64\n",
      "item_description     1482531 non-null object\n",
      "dtypes: float64(1), int64(3), object(4)\n",
      "memory usage: 90.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's remove the train_id from the training set, and map the numeric and character variables to being with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLB Cincinnati Reds T Shirt Size XL</td>\n",
       "      <td>3</td>\n",
       "      <td>Men/Tops/T-shirts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>No description yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Razer BlackWidow Chroma Keyboard</td>\n",
       "      <td>3</td>\n",
       "      <td>Electronics/Computers &amp; Tablets/Components &amp; P...</td>\n",
       "      <td>Razer</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>This keyboard is in great condition and works ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVA-VIV Blouse</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Tops &amp; Blouses/Blouse</td>\n",
       "      <td>Target</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Adorable top with a hint of lace and a key hol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leather Horse Statues</td>\n",
       "      <td>1</td>\n",
       "      <td>Home/Home Décor/Home Décor Accents</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>New with tags. Leather horses. Retail for [rm]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24K GOLD plated rose</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Jewelry/Necklaces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Complete with certificate of authenticity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  name  item_condition_id  \\\n",
       "0  MLB Cincinnati Reds T Shirt Size XL                  3   \n",
       "1     Razer BlackWidow Chroma Keyboard                  3   \n",
       "2                       AVA-VIV Blouse                  1   \n",
       "3                Leather Horse Statues                  1   \n",
       "4                 24K GOLD plated rose                  1   \n",
       "\n",
       "                                       category_name brand_name  price  \\\n",
       "0                                  Men/Tops/T-shirts        NaN   10.0   \n",
       "1  Electronics/Computers & Tablets/Components & P...      Razer   52.0   \n",
       "2                        Women/Tops & Blouses/Blouse     Target   10.0   \n",
       "3                 Home/Home Décor/Home Décor Accents        NaN   35.0   \n",
       "4                            Women/Jewelry/Necklaces        NaN   44.0   \n",
       "\n",
       "   shipping                                   item_description  \n",
       "0         1                                 No description yet  \n",
       "1         0  This keyboard is in great condition and works ...  \n",
       "2         1  Adorable top with a hint of lace and a key hol...  \n",
       "3         1  New with tags. Leather horses. Retail for [rm]...  \n",
       "4         0          Complete with certificate of authenticity  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.drop(\"train_id\",axis = 1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1482535 entries, 0 to 1482534\n",
      "Data columns (total 7 columns):\n",
      "name                 1482535 non-null object\n",
      "item_condition_id    1482535 non-null int64\n",
      "category_name        1476208 non-null object\n",
      "brand_name           849853 non-null object\n",
      "price                1482535 non-null float64\n",
      "shipping             1482535 non-null int64\n",
      "item_description     1482531 non-null object\n",
      "dtypes: float64(1), int64(2), object(4)\n",
      "memory usage: 79.2+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "Numeric_features_select = np.logical_or((pd.Series(train.dtypes)) == \"int64\",(pd.Series(train.dtypes) == \"float64\")).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "Numeric_features = pd.Series(train.columns)[Numeric_features_select].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "Numeric_features.remove('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['item_condition_id', 'shipping']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_features = pd.Series(train.columns)[np.logical_not(Numeric_features_select)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'category_name', 'brand_name', 'item_description']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model evaluation will be based on the log of the target variable price, we convert it at this stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.price = pd.Series(np.log(train.price + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLB Cincinnati Reds T Shirt Size XL</td>\n",
       "      <td>3</td>\n",
       "      <td>Men/Tops/T-shirts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>1</td>\n",
       "      <td>No description yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Razer BlackWidow Chroma Keyboard</td>\n",
       "      <td>3</td>\n",
       "      <td>Electronics/Computers &amp; Tablets/Components &amp; P...</td>\n",
       "      <td>Razer</td>\n",
       "      <td>3.970292</td>\n",
       "      <td>0</td>\n",
       "      <td>This keyboard is in great condition and works ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVA-VIV Blouse</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Tops &amp; Blouses/Blouse</td>\n",
       "      <td>Target</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>1</td>\n",
       "      <td>Adorable top with a hint of lace and a key hol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leather Horse Statues</td>\n",
       "      <td>1</td>\n",
       "      <td>Home/Home Décor/Home Décor Accents</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.583519</td>\n",
       "      <td>1</td>\n",
       "      <td>New with tags. Leather horses. Retail for [rm]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24K GOLD plated rose</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Jewelry/Necklaces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.806662</td>\n",
       "      <td>0</td>\n",
       "      <td>Complete with certificate of authenticity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  name  item_condition_id  \\\n",
       "0  MLB Cincinnati Reds T Shirt Size XL                  3   \n",
       "1     Razer BlackWidow Chroma Keyboard                  3   \n",
       "2                       AVA-VIV Blouse                  1   \n",
       "3                Leather Horse Statues                  1   \n",
       "4                 24K GOLD plated rose                  1   \n",
       "\n",
       "                                       category_name brand_name     price  \\\n",
       "0                                  Men/Tops/T-shirts        NaN  2.397895   \n",
       "1  Electronics/Computers & Tablets/Components & P...      Razer  3.970292   \n",
       "2                        Women/Tops & Blouses/Blouse     Target  2.397895   \n",
       "3                 Home/Home Décor/Home Décor Accents        NaN  3.583519   \n",
       "4                            Women/Jewelry/Necklaces        NaN  3.806662   \n",
       "\n",
       "   shipping                                   item_description  \n",
       "0         1                                 No description yet  \n",
       "1         0  This keyboard is in great condition and works ...  \n",
       "2         1  Adorable top with a hint of lace and a key hol...  \n",
       "3         1  New with tags. Leather horses. Retail for [rm]...  \n",
       "4         0          Complete with certificate of authenticity  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to split the training data set into training and holdout sets in order to start building our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train.drop(\"price\", axis = 1)\n",
    "y = train.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_holdout,y_train,y_holdout = train_test_split(X,y,test_size = 0.4, random_state = 425)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(889521, 6)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(593014, 6)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_holdout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(889521,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(593014,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_holdout.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, let's lockdown and save the datasets on which we will train and validate our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8a103d2ed782>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X_train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mX_holdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X_holdout.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y_train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "X_train.to_csv(\"X_train.csv\",index=False)\n",
    "X_holdout.to_csv(\"X_holdout.csv\", index = False)\n",
    "y_train.to_csv(\"y_train.csv\", index= False)\n",
    "y_holdout.to_csv(\"y_holdout.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the pickled feature names\n",
    "import pickle\n",
    "with open(\"Numeric_features.pkl\", 'wb') as f:\n",
    "    pickle.dump(Numeric_features,f)\n",
    "f.close()\n",
    "\n",
    "with open(\"Text_features.pkl\", 'wb') as f:\n",
    "    pickle.dump(Text_features,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing and model training pipeline\n",
    "\n",
    "At this stage we will make high-level design for our data processing and model training pipeline. At minimum, our pipeline will need to include the following steps:\n",
    "\n",
    "1. We need to process text and numeric features seperately, then combine them using FeatureUnion()\n",
    "\n",
    "    2. Text  subpipeline should include:\n",
    "        - A function that will merge all text in a given row to make it ready for tokenization. \n",
    "            - This function should take care of converting missing values to empty strings. \n",
    "            - It should also convert the text to lowercase.\n",
    "        - We should remove common stop words and perform text stemming\n",
    "        - We should tokenize using alphanumeric characters only (white space + punctuation are used as delimiters)\n",
    "        - We try to include up to 3-grams\n",
    "\n",
    "    3. Numeric subpipeline should include:\n",
    "        - An imputation step to fill any missing values using mean of the column\n",
    "        - A scaling step that will scale the numeric values between -1 and 1\n",
    "        \n",
    "4. After merging the numeric and text features we will add the following common steps:\n",
    "    5. Adding interaction terms\n",
    "    6. Perform a simple feature selection using X-square method we learned\n",
    "    7. We will try to include a hashing step to improve computational efficiency\n",
    "7. Finally, we will put a model training step, we will start with a regularized linear regression.\n",
    "\n",
    "Once our pipeline is ready, our goals will be:\n",
    "\n",
    "- to get some idea about the initial performance of the pipeline using the simple model in holdout set, using default hyperparameters\n",
    "- then try to perform hyperparameter tuning (such as GridSearchCV or RandomSearchCV) to see if we can come up with a better model. \n",
    "- save the model that has the best overall performance ( based on rmse using holdout set)\n",
    "\n",
    "Finally, we will repeat these steps using the same pipeline, but changing model structure to train:\n",
    "\n",
    "- Elastic Net model\n",
    "- Support vector machines\n",
    "- RandomForest regression\n",
    "- Gradient boosting\n",
    "\n",
    "After these steps, we will explore ensembling these models to see if we can get a better model.\n",
    "\n",
    "At the end of this exercise, we expect to get more competent on:\n",
    "\n",
    "1. experiencing developing sklearn pipelines and incorporating custom functions\n",
    "2. basic NLP tasks we can perform with our current knowledge\n",
    "3. developing an intuition about the performance of different models, \n",
    "4. experiencing the sklearn API for hyperparameter tuning using important algorithms \n",
    "\n",
    "Finally, we will try to use the same sets to train Deep Neural Networks to see how they compare to the performance of shallow learning approaches.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Re-read the training data\n",
    "X_train = pd.read_csv(\"X_train.csv\")\n",
    "y_train = pd.read_csv(\"y_train.csv\", header=None, names= [\"price\"])\n",
    "\n",
    "y_train = y_train.price.values\n",
    "\n",
    "# Re-read the pickled feature names\n",
    "import pickle\n",
    "with open(\"Numeric_features.pkl\", 'rb') as f:\n",
    "    Numeric_features = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "with open(\"Text_features.pkl\", 'rb') as f:\n",
    "    Text_features = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A custom function for text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_text_processer(df,text_columns = Text_features):\n",
    "    \"\"\"\"A function that will merge/join all text in a given row to make it ready for tokenization. \n",
    "    - This function should take care of converting missing values to empty strings. \n",
    "    - It should also convert the text to lowercase.\n",
    "    df= pandas dataframe\n",
    "    text_columns = names of the text features in df\n",
    "    \"\"\" \n",
    "    # Select only non-text columns that are in the df\n",
    "    text_data = df[text_columns]\n",
    "    \n",
    "    # Fill the missing values in text_data using empty strings\n",
    "    text_data.fillna(\"\",inplace=True)\n",
    "    \n",
    "    # Join all the strings in a given row to make a vector\n",
    "    text_vector = text_data.apply(lambda x: \" \".join(x), axis = 1)\n",
    "    \n",
    "    # Convert all the text to lowercase and return as pd.Series object to enter the tokenization pipeline\n",
    "    return text_vector.apply(lambda x: x.lower())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model training pipeline\n",
    "\n",
    "We will start by loading the necessary functions from sklearn submodules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler, Imputer, FunctionTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_selection import SelectKBest, f_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we build two utility functions to parse numeric and text data, and wrap them using FunctionTransformer, so that they can be integrated into a sklearn pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_numeric_data = FunctionTransformer(func = lambda x: x[Numeric_features], validate=False) #Note x is by default the tensor that contains all features\n",
    "get_text_data = FunctionTransformer(column_text_processer,validate=False) # Note how we avoid putting any arguments into column_text_processer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to create our regex token pattern to use in CountVectorizer. CountVectorizer will use this regex pattern to create tokens and n-grams we specified. It will automatically convert these into dummy features and stores in the form of a sparsemartix. Note that we will use HashingVectorizer to improve computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   #Note this regex will match either a whitespace or a punctuation to tokenize the string vector on these preferences  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to redefine the default feature selection function for regression to properly place into our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_regression(X,Y):\n",
    "    import sklearn\n",
    "    return sklearn.feature_selection.f_regression(X,Y,center = False) # default is center = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start building the actual pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl1 = Pipeline([\n",
    "    (\"union\",FeatureUnion(        #Note that FeatureUnion() accepts list of tuples, the first half of each tuple is the name of the transformer\n",
    "        transformer_list = [\n",
    "            (\"numeric_subpipeline\", Pipeline([        #Note we have subpipeline branches inside the main pipeline\n",
    "                \n",
    "                (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                (\"imputer\",Imputer()), # Step2: impute missing values\n",
    "            \n",
    "            ])), # Branching point of the FeatureUnion\n",
    "            \n",
    "            (\"text_subpipeline\",Pipeline([\n",
    "            \n",
    "                (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC,\n",
    "                                             stop_words = \"english\",# We will remove English stop words before tokenization\n",
    "                                             ngram_range = (1,3),\n",
    "                                             non_negative=True, norm=None, binary=False  \n",
    "                                            )), # Step2: use CountVectorizer for automated tokenization and feature extraction\n",
    "                                            ('dim_red', SelectKBest(f_regression, 300)) # Step3: use dimension reduction to select 300 best features\n",
    "                \n",
    "            ]))\n",
    "        ]\n",
    "    \n",
    "    )),# Branching point to the main pipeline: at this point all fearures are numeric\n",
    "    \n",
    "    #(\"int\", SparseInteractions(degree=2)), # Add polynomial interaction terms :POSTPONED\n",
    "    (\"scaler\",MaxAbsScaler()), # Scale the features\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/pandas/core/frame.py:2852: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= row_norms(X.T)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1818: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n"
     ]
    }
   ],
   "source": [
    "# We fit_transform X outside of the pipeline to obtain transformed X for hyperparameter search, \n",
    "# since transformation step takes long time and we want to avoid repeating everytime \n",
    "X_train_transformed = pl1.fit_transform(X_train,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We start with ridge regression\n",
    "model1 = Ridge(alpha=0.5)\n",
    "model1.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = model1.predict(X_train_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63839928042826788"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "np.sqrt(mean_squared_error(y_train,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD9CAYAAACcJ53WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x152cfdeb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(y_pred1,y_train, s = 2, c = \"r\", alpha = 0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple pipeline already looks very promising!\n",
    "\n",
    "## Ridge Hyperparameter tuning\n",
    "\n",
    "First, we compile the data loading, utility functions and pipeline steps for easy starting up later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "#############################################################################\n",
    "# Re-read the training data\n",
    "X_train = pd.read_csv(\"X_train.csv\")\n",
    "y_train = pd.read_csv(\"y_train.csv\", header=None, names= [\"price\"])\n",
    "\n",
    "y_train = y_train.price.values\n",
    "\n",
    "#############################################################################\n",
    "# Re-read the pickled feature names\n",
    "import pickle\n",
    "with open(\"Numeric_features.pkl\", 'rb') as f:\n",
    "    Numeric_features = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "with open(\"Text_features.pkl\", 'rb') as f:\n",
    "    Text_features = pickle.load(f)\n",
    "f.close()\n",
    "#############################################################################\n",
    "\n",
    "def column_text_processer(df,text_columns = Text_features):\n",
    "    \"\"\"\"A function that will merge/join all text in a given row to make it ready for tokenization. \n",
    "    - This function should take care of converting missing values to empty strings. \n",
    "    - It should also convert the text to lowercase.\n",
    "    df= pandas dataframe\n",
    "    text_columns = names of the text features in df\n",
    "    \"\"\" \n",
    "    # Select only non-text columns that are in the df\n",
    "    text_data = df[text_columns]\n",
    "    \n",
    "    # Fill the missing values in text_data using empty strings\n",
    "    text_data.fillna(\"\",inplace=True)\n",
    "    \n",
    "    # Join all the strings in a given row to make a vector\n",
    "    text_vector = text_data.apply(lambda x: \" \".join(x), axis = 1)\n",
    "    \n",
    "    # Convert all the text to lowercase and return as pd.Series object to enter the tokenization pipeline\n",
    "    return text_vector.apply(lambda x: x.lower())  \n",
    "\n",
    "#############################################################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler, Imputer, FunctionTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "#############################################################################\n",
    "# Utility functions to parse text and numeric features\n",
    "get_numeric_data = FunctionTransformer(func = lambda x: x[Numeric_features], validate=False) #Note x is by default the tensor that contains all features\n",
    "get_text_data = FunctionTransformer(column_text_processer,validate=False) # Note how we avoid putting any arguments into column_text_processer\n",
    "#############################################################################\n",
    "\n",
    "#############################################################################\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   #Note this regex will match either a whitespace or a punctuation to tokenize the string vector on these preferences  \n",
    "\n",
    "#############################################################################\n",
    "# Define f_regression for feature selection with center = True default\n",
    "def f_regression(X,Y):\n",
    "    import sklearn\n",
    "    return sklearn.feature_selection.f_regression(X,Y,center = False) # default is center = True\n",
    "\n",
    "#############################################################################\n",
    "# Prepare the actual pipeline:\n",
    "\n",
    "pl1 = Pipeline([\n",
    "    (\"union\",FeatureUnion(        #Note that FeatureUnion() accepts list of tuples, the first half of each tuple is the name of the transformer\n",
    "        transformer_list = [\n",
    "            (\"numeric_subpipeline\", Pipeline([        #Note we have subpipeline branches inside the main pipeline\n",
    "                \n",
    "                (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                (\"imputer\",Imputer()), # Step2: impute missing values\n",
    "            \n",
    "            ])), # Branching point of the FeatureUnion\n",
    "            \n",
    "            (\"text_subpipeline\",Pipeline([\n",
    "            \n",
    "                (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC,\n",
    "                                             stop_words = \"english\",# We will remove English stop words before tokenization\n",
    "                                             ngram_range = (1,3),\n",
    "                                             non_negative=True, norm=None, binary=False  \n",
    "                                            )), # Step2: use CountVectorizer for automated tokenization and feature extraction\n",
    "                                            ('dim_red', SelectKBest(f_regression, 300)) # Step3: use dimension reduction to select 300 best features\n",
    "                \n",
    "            ]))\n",
    "        ]\n",
    "    \n",
    "    )),# Branching point to the main pipeline: at this point all fearures are numeric\n",
    "    \n",
    "    #(\"int\", SparseInteractions(degree=2)), # Add polynomial interaction terms :POSTPONED\n",
    "    (\"scaler\",MaxAbsScaler()), # Scale the features\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/pandas/core/frame.py:2852: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= row_norms(X.T)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1818: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n"
     ]
    }
   ],
   "source": [
    "# We fit_transform X outside of the pipeline to obtain transformed X for hyperparameter search, \n",
    "# since transformation step takes long time and we want to avoid repeating everytime \n",
    "X_train_transformed = pl1.fit_transform(X_train,y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start hyperparameter optimization. Let's first start by looking into the tunable hyperparameters in ridge model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Ridge in module sklearn.linear_model.ridge:\n",
      "\n",
      "class Ridge(_BaseRidge, sklearn.base.RegressorMixin)\n",
      " |  Linear least squares with l2 regularization.\n",
      " |  \n",
      " |  This model solves a regression model where the loss function is\n",
      " |  the linear least squares function and regularization is given by\n",
      " |  the l2-norm. Also known as Ridge Regression or Tikhonov regularization.\n",
      " |  This estimator has built-in support for multi-variate regression\n",
      " |  (i.e., when y is a 2d-array of shape [n_samples, n_targets]).\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  alpha : {float, array-like}, shape (n_targets)\n",
      " |      Regularization strength; must be a positive float. Regularization\n",
      " |      improves the conditioning of the problem and reduces the variance of\n",
      " |      the estimates. Larger values specify stronger regularization.\n",
      " |      Alpha corresponds to ``C^-1`` in other linear models such as \n",
      " |      LogisticRegression or LinearSVC. If an array is passed, penalties are\n",
      " |      assumed to be specific to the targets. Hence they must correspond in\n",
      " |      number.\n",
      " |  \n",
      " |  copy_X : boolean, optional, default True\n",
      " |      If True, X will be copied; else, it may be overwritten.\n",
      " |  \n",
      " |  fit_intercept : boolean\n",
      " |      Whether to calculate the intercept for this model. If set\n",
      " |      to false, no intercept will be used in calculations\n",
      " |      (e.g. data is expected to be already centered).\n",
      " |  \n",
      " |  max_iter : int, optional\n",
      " |      Maximum number of iterations for conjugate gradient solver.\n",
      " |      For 'sparse_cg' and 'lsqr' solvers, the default value is determined\n",
      " |      by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\n",
      " |  \n",
      " |  normalize : boolean, optional, default False\n",
      " |      If True, the regressors X will be normalized before regression.\n",
      " |      This parameter is ignored when `fit_intercept` is set to False.\n",
      " |      When the regressors are normalized, note that this makes the\n",
      " |      hyperparameters learnt more robust and almost independent of the number\n",
      " |      of samples. The same property is not valid for standardized data.\n",
      " |      However, if you wish to standardize, please use\n",
      " |      `preprocessing.StandardScaler` before calling `fit` on an estimator\n",
      " |      with `normalize=False`.\n",
      " |  \n",
      " |  solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag'}\n",
      " |      Solver to use in the computational routines:\n",
      " |  \n",
      " |      - 'auto' chooses the solver automatically based on the type of data.\n",
      " |  \n",
      " |      - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
      " |        coefficients. More stable for singular matrices than\n",
      " |        'cholesky'.\n",
      " |  \n",
      " |      - 'cholesky' uses the standard scipy.linalg.solve function to\n",
      " |        obtain a closed-form solution.\n",
      " |  \n",
      " |      - 'sparse_cg' uses the conjugate gradient solver as found in\n",
      " |        scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
      " |        more appropriate than 'cholesky' for large-scale data\n",
      " |        (possibility to set `tol` and `max_iter`).\n",
      " |  \n",
      " |      - 'lsqr' uses the dedicated regularized least-squares routine\n",
      " |        scipy.sparse.linalg.lsqr. It is the fastest but may not be available\n",
      " |        in old scipy versions. It also uses an iterative procedure.\n",
      " |  \n",
      " |      - 'sag' uses a Stochastic Average Gradient descent. It also uses an\n",
      " |        iterative procedure, and is often faster than other solvers when\n",
      " |        both n_samples and n_features are large. Note that 'sag' fast\n",
      " |        convergence is only guaranteed on features with approximately the\n",
      " |        same scale. You can preprocess the data with a scaler from\n",
      " |        sklearn.preprocessing.\n",
      " |  \n",
      " |      All last four solvers support both dense and sparse data. However,\n",
      " |      only 'sag' supports sparse input when `fit_intercept` is True.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Stochastic Average Gradient descent solver.\n",
      " |  \n",
      " |  tol : float\n",
      " |      Precision of the solution.\n",
      " |  \n",
      " |  random_state : int seed, RandomState instance, or None (default)\n",
      " |      The seed of the pseudo random number generator to use when\n",
      " |      shuffling the data. Used only in 'sag' solver.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *random_state* to support Stochastic Average Gradient.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : array, shape (n_features,) or (n_targets, n_features)\n",
      " |      Weight vector(s).\n",
      " |  \n",
      " |  intercept_ : float | array, shape = (n_targets,)\n",
      " |      Independent term in decision function. Set to 0.0 if\n",
      " |      ``fit_intercept = False``.\n",
      " |  \n",
      " |  n_iter_ : array or None, shape (n_targets,)\n",
      " |      Actual number of iterations for each target. Available only for\n",
      " |      sag and lsqr solvers. Other solvers will return None.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  RidgeClassifier, RidgeCV, :class:`sklearn.kernel_ridge.KernelRidge`\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.linear_model import Ridge\n",
      " |  >>> import numpy as np\n",
      " |  >>> n_samples, n_features = 10, 5\n",
      " |  >>> np.random.seed(0)\n",
      " |  >>> y = np.random.randn(n_samples)\n",
      " |  >>> X = np.random.randn(n_samples, n_features)\n",
      " |  >>> clf = Ridge(alpha=1.0)\n",
      " |  >>> clf.fit(X, y) # doctest: +NORMALIZE_WHITESPACE\n",
      " |  Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      " |        normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Ridge\n",
      " |      _BaseRidge\n",
      " |      abc.NewBase\n",
      " |      sklearn.linear_model.base.LinearModel\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit Ridge regression model\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          Training data\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_targets]\n",
      " |          Target values\n",
      " |      \n",
      " |      sample_weight : float or numpy array of shape [n_samples]\n",
      " |          Individual weights for each sample\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      " |  \n",
      " |  decision_function(*args, **kwargs)\n",
      " |      DEPRECATED:  and will be removed in 0.19.\n",
      " |      \n",
      " |      Decision function of the linear model.\n",
      " |      \n",
      " |              Parameters\n",
      " |              ----------\n",
      " |              X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      " |                  Samples.\n",
      " |      \n",
      " |              Returns\n",
      " |              -------\n",
      " |              C : array, shape = (n_samples,)\n",
      " |                  Returns predicted values.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the linear model\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape = (n_samples,)\n",
      " |          Returns predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      Best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.5,\n",
       " 'copy_X': True,\n",
       " 'fit_intercept': True,\n",
       " 'max_iter': None,\n",
       " 'normalize': False,\n",
       " 'random_state': None,\n",
       " 'solver': 'auto',\n",
       " 'tol': 0.001}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking into parameters of our first estimator:\n",
    "\n",
    "model1.get_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plan to start by tuning the alpha, which is the most important hyperparameter that defines the strength of regularization. A sensible value is between 1 and 0. We will use Exhaustive search over specified parameter values for an estimator, which is performed by **GridSearchCV** using 5 fold cross-validation. Since we only perform search in one hyperparameter, this is a one-dimensional grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  0.94736842,  0.89473684,  0.84210526,  0.78947368,\n",
       "        0.73684211,  0.68421053,  0.63157895,  0.57894737,  0.52631579,\n",
       "        0.47368421,  0.42105263,  0.36842105,  0.31578947,  0.26315789,\n",
       "        0.21052632,  0.15789474,  0.10526316,  0.05263158,  0.        ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We prepare 20 grids of alpha in a linear space\n",
    "alpha_space = np.linspace(1,0,20)\n",
    "alpha_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GridSearchCV in module sklearn.model_selection._search:\n",
      "\n",
      "class GridSearchCV(BaseSearchCV)\n",
      " |  Exhaustive search over specified parameter values for an estimator.\n",
      " |  \n",
      " |  Important members are fit, predict.\n",
      " |  \n",
      " |  GridSearchCV implements a \"fit\" and a \"score\" method.\n",
      " |  It also implements \"predict\", \"predict_proba\", \"decision_function\",\n",
      " |  \"transform\" and \"inverse_transform\" if they are implemented in the\n",
      " |  estimator used.\n",
      " |  \n",
      " |  The parameters of the estimator used to apply these methods are optimized\n",
      " |  by cross-validated grid-search over a parameter grid.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <grid_search>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  estimator : estimator object.\n",
      " |      This is assumed to implement the scikit-learn estimator interface.\n",
      " |      Either estimator needs to provide a ``score`` function,\n",
      " |      or ``scoring`` must be passed.\n",
      " |  \n",
      " |  param_grid : dict or list of dictionaries\n",
      " |      Dictionary with parameters names (string) as keys and lists of\n",
      " |      parameter settings to try as values, or a list of such\n",
      " |      dictionaries, in which case the grids spanned by each dictionary\n",
      " |      in the list are explored. This enables searching over any sequence\n",
      " |      of parameter settings.\n",
      " |  \n",
      " |  scoring : string, callable or None, default=None\n",
      " |      A string (see model evaluation documentation) or\n",
      " |      a scorer callable object / function with signature\n",
      " |      ``scorer(estimator, X, y)``.\n",
      " |      If ``None``, the ``score`` method of the estimator is used.\n",
      " |  \n",
      " |  fit_params : dict, optional\n",
      " |      Parameters to pass to the fit method.\n",
      " |  \n",
      " |  n_jobs : int, default=1\n",
      " |      Number of jobs to run in parallel.\n",
      " |  \n",
      " |  pre_dispatch : int, or string, optional\n",
      " |      Controls the number of jobs that get dispatched during parallel\n",
      " |      execution. Reducing this number can be useful to avoid an\n",
      " |      explosion of memory consumption when more jobs get dispatched\n",
      " |      than CPUs can process. This parameter can be:\n",
      " |  \n",
      " |          - None, in which case all the jobs are immediately\n",
      " |            created and spawned. Use this for lightweight and\n",
      " |            fast-running jobs, to avoid delays due to on-demand\n",
      " |            spawning of the jobs\n",
      " |  \n",
      " |          - An int, giving the exact number of total jobs that are\n",
      " |            spawned\n",
      " |  \n",
      " |          - A string, giving an expression as a function of n_jobs,\n",
      " |            as in '2*n_jobs'\n",
      " |  \n",
      " |  iid : boolean, default=True\n",
      " |      If True, the data is assumed to be identically distributed across\n",
      " |      the folds, and the loss minimized is the total loss per sample,\n",
      " |      and not the mean loss across the folds.\n",
      " |  \n",
      " |  cv : int, cross-validation generator or an iterable, optional\n",
      " |      Determines the cross-validation splitting strategy.\n",
      " |      Possible inputs for cv are:\n",
      " |        - None, to use the default 3-fold cross validation,\n",
      " |        - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
      " |        - An object to be used as a cross-validation generator.\n",
      " |        - An iterable yielding train, test splits.\n",
      " |  \n",
      " |      For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      " |      either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      " |      other cases, :class:`KFold` is used.\n",
      " |  \n",
      " |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      " |      cross-validation strategies that can be used here.\n",
      " |  \n",
      " |  refit : boolean, default=True\n",
      " |      Refit the best estimator with the entire dataset.\n",
      " |      If \"False\", it is impossible to make predictions using\n",
      " |      this GridSearchCV instance after fitting.\n",
      " |  \n",
      " |  verbose : integer\n",
      " |      Controls the verbosity: the higher, the more messages.\n",
      " |  \n",
      " |  error_score : 'raise' (default) or numeric\n",
      " |      Value to assign to the score if an error occurs in estimator fitting.\n",
      " |      If set to 'raise', the error is raised. If a numeric value is given,\n",
      " |      FitFailedWarning is raised. This parameter does not affect the refit\n",
      " |      step, which will always raise the error.\n",
      " |  \n",
      " |  return_train_score : boolean, default=True\n",
      " |      If ``'False'``, the ``cv_results_`` attribute will not include training\n",
      " |      scores.\n",
      " |  \n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn import svm, datasets\n",
      " |  >>> from sklearn.model_selection import GridSearchCV\n",
      " |  >>> iris = datasets.load_iris()\n",
      " |  >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      " |  >>> svr = svm.SVC()\n",
      " |  >>> clf = GridSearchCV(svr, parameters)\n",
      " |  >>> clf.fit(iris.data, iris.target)\n",
      " |  ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n",
      " |  GridSearchCV(cv=None, error_score=...,\n",
      " |         estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,\n",
      " |                       decision_function_shape=None, degree=..., gamma=...,\n",
      " |                       kernel='rbf', max_iter=-1, probability=False,\n",
      " |                       random_state=None, shrinking=True, tol=...,\n",
      " |                       verbose=False),\n",
      " |         fit_params={}, iid=..., n_jobs=1,\n",
      " |         param_grid=..., pre_dispatch=..., refit=..., return_train_score=...,\n",
      " |         scoring=..., verbose=...)\n",
      " |  >>> sorted(clf.cv_results_.keys())\n",
      " |  ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n",
      " |  ['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n",
      " |   'mean_train_score', 'param_C', 'param_kernel', 'params',...\n",
      " |   'rank_test_score', 'split0_test_score',...\n",
      " |   'split0_train_score', 'split1_test_score', 'split1_train_score',...\n",
      " |   'split2_test_score', 'split2_train_score',...\n",
      " |   'std_fit_time', 'std_score_time', 'std_test_score', 'std_train_score'...]\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  cv_results_ : dict of numpy (masked) ndarrays\n",
      " |      A dict with keys as column headers and values as columns, that can be\n",
      " |      imported into a pandas ``DataFrame``.\n",
      " |  \n",
      " |      For instance the below given table\n",
      " |  \n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_....|\n",
      " |      +============+===========+============+=================+===+=========+\n",
      " |      |  'poly'    |     --    |      2     |        0.8      |...|    2    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |  'poly'    |     --    |      3     |        0.7      |...|    4    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |  'rbf'     |     0.1   |     --     |        0.8      |...|    3    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |  'rbf'     |     0.2   |     --     |        0.9      |...|    1    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |  \n",
      " |      will be represented by a ``cv_results_`` dict of::\n",
      " |  \n",
      " |          {\n",
      " |          'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n",
      " |                                       mask = [False False False False]...)\n",
      " |          'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n",
      " |                                      mask = [ True  True False False]...),\n",
      " |          'param_degree': masked_array(data = [2.0 3.0 -- --],\n",
      " |                                       mask = [False False  True  True]...),\n",
      " |          'split0_test_score'  : [0.8, 0.7, 0.8, 0.9],\n",
      " |          'split1_test_score'  : [0.82, 0.5, 0.7, 0.78],\n",
      " |          'mean_test_score'    : [0.81, 0.60, 0.75, 0.82],\n",
      " |          'std_test_score'     : [0.02, 0.01, 0.03, 0.03],\n",
      " |          'rank_test_score'    : [2, 4, 3, 1],\n",
      " |          'split0_train_score' : [0.8, 0.9, 0.7],\n",
      " |          'split1_train_score' : [0.82, 0.5, 0.7],\n",
      " |          'mean_train_score'   : [0.81, 0.7, 0.7],\n",
      " |          'std_train_score'    : [0.03, 0.03, 0.04],\n",
      " |          'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n",
      " |          'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n",
      " |          'mean_score_time'    : [0.007, 0.06, 0.04, 0.04],\n",
      " |          'std_score_time'     : [0.001, 0.002, 0.003, 0.005],\n",
      " |          'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n",
      " |          }\n",
      " |  \n",
      " |      NOTE that the key ``'params'`` is used to store a list of parameter\n",
      " |      settings dict for all the parameter candidates.\n",
      " |  \n",
      " |      The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n",
      " |      ``std_score_time`` are all in seconds.\n",
      " |  \n",
      " |  best_estimator_ : estimator\n",
      " |      Estimator that was chosen by the search, i.e. estimator\n",
      " |      which gave highest score (or smallest loss if specified)\n",
      " |      on the left out data. Not available if refit=False.\n",
      " |  \n",
      " |  best_score_ : float\n",
      " |      Score of best_estimator on the left out data.\n",
      " |  \n",
      " |  best_params_ : dict\n",
      " |      Parameter setting that gave the best results on the hold out data.\n",
      " |  \n",
      " |  best_index_ : int\n",
      " |      The index (of the ``cv_results_`` arrays) which corresponds to the best\n",
      " |      candidate parameter setting.\n",
      " |  \n",
      " |      The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n",
      " |      the parameter setting for the best model, that gives the highest\n",
      " |      mean score (``search.best_score_``).\n",
      " |  \n",
      " |  scorer_ : function\n",
      " |      Scorer function used on the held out data to choose the best\n",
      " |      parameters for the model.\n",
      " |  \n",
      " |  n_splits_ : int\n",
      " |      The number of cross-validation splits (folds/iterations).\n",
      " |  \n",
      " |  Notes\n",
      " |  ------\n",
      " |  The parameters selected are those that maximize the score of the left out\n",
      " |  data, unless an explicit score is passed in which case it is used instead.\n",
      " |  \n",
      " |  If `n_jobs` was set to a value higher than one, the data is copied for each\n",
      " |  point in the grid (and not `n_jobs` times). This is done for efficiency\n",
      " |  reasons if individual jobs take very little time, but may raise errors if\n",
      " |  the dataset is large and not enough memory is available.  A workaround in\n",
      " |  this case is to set `pre_dispatch`. Then, the memory is copied only\n",
      " |  `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n",
      " |  n_jobs`.\n",
      " |  \n",
      " |  See Also\n",
      " |  ---------\n",
      " |  :class:`ParameterGrid`:\n",
      " |      generates all the combinations of a hyperparameter grid.\n",
      " |  \n",
      " |  :func:`sklearn.model_selection.train_test_split`:\n",
      " |      utility function to split the data into a development set usable\n",
      " |      for fitting a GridSearchCV instance and an evaluation set for\n",
      " |      its final evaluation.\n",
      " |  \n",
      " |  :func:`sklearn.metrics.make_scorer`:\n",
      " |      Make a scorer from a performance metric or loss function.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GridSearchCV\n",
      " |      BaseSearchCV\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, estimator, param_grid, scoring=None, fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score='raise', return_train_score=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None, groups=None)\n",
      " |      Run fit with all sets of parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |          Training vector, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n",
      " |          Target relative to X for classification or regression;\n",
      " |          None for unsupervised learning.\n",
      " |      \n",
      " |      groups : array-like, with shape (n_samples,), optional\n",
      " |          Group labels for the samples used while splitting the dataset into\n",
      " |          train/test set.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseSearchCV:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Call decision_function on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``decision_function``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  inverse_transform(self, Xt)\n",
      " |      Call inverse_transform on the estimator with the best found params.\n",
      " |      \n",
      " |      Only available if the underlying estimator implements\n",
      " |      ``inverse_transform`` and ``refit=True``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      Xt : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Call predict on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Call predict_log_proba on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict_log_proba``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Call predict_proba on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict_proba``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  score(self, X, y=None)\n",
      " |      Returns the score on the given data, if the estimator has been refit.\n",
      " |      \n",
      " |      This uses the score defined by ``scoring`` where provided, and the\n",
      " |      ``best_estimator_.score`` method otherwise.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |          Input data, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n",
      " |          Target relative to X for classification or regression;\n",
      " |          None for unsupervised learning.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Call transform on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if the underlying estimator supports ``transform`` and\n",
      " |      ``refit=True``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseSearchCV:\n",
      " |  \n",
      " |  best_params_\n",
      " |  \n",
      " |  best_score_\n",
      " |  \n",
      " |  grid_scores_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "help(GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"alpha\":alpha_space}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To select a scoring parameter in sklearn, see:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "- GridSearchCV and RandomizedSearchCV evaluate each parameter setting independently. Computations can be run in parallel if your OS supports it, by using the keyword n_jobs=-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the GridSearchCV object:\n",
    "ridgemodel = Ridge()\n",
    "ridge_cv = GridSearchCV(estimator= ridgemodel,param_grid= param_grid,scoring='neg_mean_squared_error',cv = 5, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to training data to start parameter search:\n",
    "ridge_cv.fit(X_train_transformed,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
