---
title: "KaggleMercariPriceSuggestionChallengeKernel"
author: "Ozan Aygun"
date: "12/15/2017"
output: 
   html_document:
        toc: true
        number_sections: true
        depth: 4
        theme: cerulean
        highlight: tango
        df_print: paged
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(results = "markup", fig.align = "center",
                      fig.width = 5, fig.height = 4,message=FALSE,warning=FALSE)
```


# Introduction and summary

Here

# Loading the data

```{r, cache=TRUE}
# Need to download files from kaggle (links available only after login!)
# Need to get 'archive' package to make a connection to 7z zipped files!
# devtools::install_github("jimhester/archive")
#library(archive);library(readr)
#train <- data.frame(read_delim(archive_read("train.tsv.7z"), delim = "\t"))
#test <- data.frame(read_delim(archive_read("test.tsv.7z"), delim = "\t"))
#sample_submission <- data.frame(read_delim(archive_read("sample_submission.csv.7z"), delim = ","))
# 
# # Save the training and test sets for easy loading in future
#saveRDS(train, "train.rds");saveRDS(test,"test.rds")
# write.csv(sample_submission,"sample_submission.csv", row.names = FALSE)
unlink(dir(pattern = ".7z")) # Finally delete the 7z files from directory
```

# Summarizing the data
```{r,cache=TRUE}
# Read from the rds objects
train <- readRDS("train.rds") ; test <- readRDS("test.rds") 
summary(train)
summary(test)
```
We have a continuous outcome(price), so this is a regression problem. Most of the features appear strings. So we are likely to perform some text mining and feature engineering.

# Partitioning the training set

The training set we have is fairly large, so it will not be wasteful if we split this data set into two for sub-training and validation respectively. We will explore and train the model on the sub-training set, perform initial validation in the validation set, and submit predictions for the test set provided.

```{r,cache=TRUE}
library(caret)
set.seed(12345)
subtrainID <- createDataPartition(y = train$price, p = 0.5, list = FALSE)
subtrain <- train[subtrainID[,1],]
validation <- train[-subtrainID[,1],]
```

# Exploratory data analysis using sub-training set

For the ease of making plots, I will explore a randomly selected subset (5%) of the subtraining set, I will call this set as **mini.subtrain**. Note that even 10% of this data set contains ~37,000 observations!

```{r,cache= TRUE, fig.align= "center", fig.width=12}
set.seed(12345)
mini.subtrainID <- createDataPartition(y = subtrain$price, p = 0.05, list = FALSE)
mini.subtrain <- subtrain[mini.subtrainID[,1],]
```

Let's start looking at the data,

## Distribution of the outcome

```{r,cache=TRUE, fig.width=10}
par(mfrow = c(1,3))
hist(mini.subtrain$price, breaks = 50, col = "navy")
hist(log(mini.subtrain$price), breaks = 50, col = "navy")
```


## Checking for missing values
```{r, cache=TRUE}
apply(is.na(subtrain),2,sum)
```

It appears that most of the missing values are in the category name and brand name (half of the values are mising)

### Generic items are slightly cheaper

One obvious thing to look at is whether the price of items that are missing such features have significantly different prices:
```{r,cache=TRUE}
library(ggplot2)
ggplot(data = mini.subtrain, aes(y = log(price+1),fill =factor(is.na(category_name)), x = factor(is.na(category_name))))+
        geom_boxplot()

ggplot(data = mini.subtrain, aes(y = log(price+1),fill =factor(is.na(brand_name)), x = factor(is.na(brand_name))))+
        geom_boxplot()
```

It is perhaps expected the items with no brand name will be slightly cheaper. Therefore, this looks like a new feature to add into the data sets:

```{r,cache=TRUE}
library(dplyr)
mini.subtrain <- mutate(mini.subtrain,no.brand_name = ifelse(is.na(brand_name),1,0))
subtrain <- mutate(subtrain,no.brand_name = ifelse(is.na(brand_name),1,0))
validation <- mutate(validation,no.brand_name = ifelse(is.na(brand_name),1,0))
test <- mutate(test,no.brand_name = ifelse(is.na(brand_name),1,0))
```

###Uniqueness profile of features

```{r}
uniqueness <- apply(subtrain,2,function(x){return(length(unique(x)))})
names(uniqueness) <- colnames(subtrain)
uniqueness
```

Except 2 binary variables, and one potential categorical variable (item_condition_id), the rest of the features can be regarded as string, and we will create new features from them using text mining.

First, store the ID variables for all data sets and remove them:

```{r}
mini.subtrain.train_id <- mini.subtrain$train_id
mini.subtrain <- dplyr::select(mini.subtrain,-train_id)

subtrain.train_id <- subtrain$train_id
subtrain <- dplyr::select(subtrain,-train_id)

validation.train_id <- validation$train_id
validation <- dplyr::select(validation,-train_id)

test.test_id <- test$test_id
test <- dplyr::select(test,-test_id)
```

### item condition id in relation to shipping and having a brand name

```{r, fig.width=10, cache=TRUE}
ggplot(data = mini.subtrain, aes(y = log(price+1), x = factor(item_condition_id),
                                fill = factor(item_condition_id)))+
                                facet_grid(factor(shipping) ~ factor(no.brand_name))+
                                geom_boxplot()+
                                geom_jitter(alpha = 0.05,size = 0.2, color = "navy")
```

These features alone do not seem to explain too much variability in the price. We need stronger features!

###Text mining and feature engineering

#### Starting simple: special characters and length of the text features

Let's try to understand if there is any relationship between the outcome and the length of various textual features:

```{r,fig.width= 10, fig.height=10}
length.name <- sapply(mini.subtrain$name,nchar) 
length.category <- sapply(mini.subtrain$category_name,nchar)
length.brand <- sapply(mini.subtrain$brand_name,nchar)
length.description <- sapply(mini.subtrain$item_description,nchar)
pairs(log(mini.subtrain$price) ~ length.name + length.category + length.brand + log(length.description),
      pch = 19, col = "navy", cex = 0.2)
```

This pretty much looks like noise. Let's have a look at some special characters:

```{r,fig.width=10}
# Exclamation mark (!)
library(stringr)
excl.name <- str_count(mini.subtrain$name, "!")
excl.category <- str_count(mini.subtrain$category_name,"!")
excl.brand <- str_count(mini.subtrain$brand_name,"!")
excl.description <- str_count(mini.subtrain$item_description,"!")
pairs(log(mini.subtrain$price) ~ excl.name + excl.category + excl.brand + log(excl.description),
      pch = 19, col = "purple", cex = 0.2)

```

It would be worth looking further into the ecxl.name and excl.description:

```{r,fig.width=10}
ggplot(data = mini.subtrain, aes(y = log(price),x= excl.name))+
        geom_point(col = "purple", alpha = 0.4,size =0.9)+
        facet_grid(factor(shipping) ~ factor(item_condition_id))
```

What happens in excl.name X item_condition_id interaction?


```{r,fig.width=10}
ggplot(data = mini.subtrain, aes(y = log(price),x= excl.name * item_condition_id))+
        geom_point(col = "blue", alpha = 0.4,size =1)+
        facet_grid( . ~ factor(shipping))
```

How about excl.description?


```{r,fig.width=10}
ggplot(data = mini.subtrain, aes(y = log(price),x= log(excl.description)))+
        geom_point(col = "purple", alpha = 0.4,size =0.9)+
        facet_grid(factor(shipping) ~ factor(item_condition_id))
```

What happens in excl.name X item_condition_id interaction?


```{r,fig.width=10}
ggplot(data = mini.subtrain, aes(y = log(price),x= log(excl.description) * item_condition_id))+
        geom_point(col = "blue", alpha = 0.4,size =1)+
        facet_grid( . ~ factor(shipping))
```

My intuition is the interaction with item condition makes these two new features somewhat stronger. 

Let's add these two new features into the main data sets:

```{r}
mini.subtrain$log.excl.description <- log(excl.description + 1) * mini.subtrain$item_condition_id
mini.subtrain$excl.name <- excl.name * mini.subtrain$item_condition_id

subtrain$log.excl.description <- log(str_count(subtrain$item_description, "!") + 1) * subtrain$item_condition_id
subtrain$excl.name <- str_count(subtrain$name, "!") * subtrain$item_condition_id

validation$log.excl.description <- log(str_count(validation$item_description, "!") + 1) * validation$item_condition_id
validation$excl.name <- str_count(validation$name, "!") * validation$item_condition_id

test$log.excl.description <- log(str_count(test$item_description, "!") + 1) * test$item_condition_id
test$excl.name <- str_count(test$name, "!") * test$item_condition_id
```

How about any characters excluding letters, commas or numbers?

```{r,fig.width= 8}
special.character.description <- sapply(mini.subtrain$item_description,function(x){
return(nchar(str_trim(gsub("[a-zA-Z]|,|\\.|[0-9]","",x))))})
ggplot(data = mini.subtrain, aes(y = log(price),x= log(special.character.description+1)))+
        geom_point(col = "purple", alpha = 0.4,size =0.9)+
        facet_grid(factor(shipping) ~ factor(item_condition_id))       

```
The special character count in item description looks like just noise.


```{r,fig.width= 8}
special.character.name <- sapply(mini.subtrain$name,function(x){
return(nchar(str_trim(gsub("[a-zA-Z]|,|\\.|[0-9]","",x))))})
ggplot(data = mini.subtrain, aes(y = log(price),x= log(special.character.name+1)))+
        geom_point(col = "navy", alpha = 0.4,size =0.9)+
        facet_grid(factor(shipping) ~ factor(item_condition_id))       

```

What happens in special.character.name X item_condition_id interaction?


```{r,fig.width=10}
ggplot(data = mini.subtrain, aes(y = log(price),x= log(special.character.name+1) * item_condition_id))+
        geom_point(col = "blue", alpha = 0.4,size =1)+
        facet_grid( . ~ factor(shipping))
```

In the case of name, this looks like not a bad feature. 

```{r}
cor(log(special.character.name+1) * mini.subtrain$item_condition_id,mini.subtrain$excl.name)
```
Sounds like we are not adding a highly correlated feature,either.

How about just counting the stars (*) ?

```{r,fig.width= 10, fig.height=10}
star.name <- str_count(mini.subtrain$name,"\\*") 
star.description <- str_count(mini.subtrain$item_description,"\\*")
pairs(log(mini.subtrain$price) ~ log(star.name +1) + log(star.description+1),pch = 19, col = "navy", cex = 0.2)
```

How about just counting the text starting with stars (*) ?

```{r,fig.width= 5, }
start.name <- str_count(mini.subtrain$name,"^\\*") 
start.description <- str_count(mini.subtrain$item_description,"^\\*")
ggplot(mini.subtrain,aes(y = log(price+1), x = factor(start.name))) +
        geom_boxplot()
ggplot(mini.subtrain,aes(y = log(price+1), x = factor(start.description))) +
        geom_boxplot() 
```

These are not so impressive features. 

How about we just count the numbers?


```{r,fig.width= 10 }
number.name <- str_count(mini.subtrain$name,"[0-9]") 
number.description <- str_count(mini.subtrain$item_description,"[0-9]")
pairs(log(mini.subtrain$price) ~ log(number.name +1) + log(number.description+1),pch = 19, col = "navy", cex = 0.2)
```

How about we just count the $ sign?

```{r,fig.width= 10, }
dollar.name <- str_count(mini.subtrain$name,"\\$") 
dollar.description <- str_count(mini.subtrain$item_description,"\\$")
pairs(log(mini.subtrain$price) ~ log(dollar.name +1) + dollar.description,pch = 19, col = "navy", cex = 0.2)
```

dollar.description looks like a good feature!

```{r,fig.width= 8}
ggplot(data = mini.subtrain, aes(y = log(price),x= dollar.description))+
        geom_point(col = "navy", alpha = 0.4,size =0.9)+
        facet_grid(. ~ factor(shipping))       

```

Let's add dollar.description into our data sets:

```{r}
mini.subtrain$dollar.description <- str_count(mini.subtrain$item_description,"\\$")
subtrain$dollar.description <-  str_count(subtrain$item_description,"\\$")
validation$dollar.description <- str_count(validation$item_description,"\\$")
test$dollar.description <- str_count(test$item_description,"\\$")
```

####Category name

We noted that there are limited category names. Some of these categories may explain certain amount of variance in the price. Let's look at more closely:

```{r, fig.width=10}
summary.category_name <- mini.subtrain %>% group_by(category_name) %>% summarise(median.log.price = median(log(price)), size = n(), perct_fraction = n()/nrow(mini.subtrain)) %>% arrange(desc(median.log.price))
head(summary.category_name,10)
hist( summary.category_name$median.log.price, col ="navy", breaks = 50)
```
It looks like the median log prices binned by category names have a near-gaussian distribution, we have certain categries that could explain some of the most expensive and cheapest items. Let's define these categories:

```{r}
fancy.categories <- summary.category_name$category_name[summary.category_name$median.log.price > quantile(log(mini.subtrain$price),0.95)]
cheap.categories <- summary.category_name$category_name[summary.category_name$median.log.price < quantile(log(mini.subtrain$price),0.05)]

fancy.categories;cheap.categories
```

Let's make categorical features using these category names:
```{r}
mini.subtrain$fancy.categories <- sapply(mini.subtrain$category_name,function(x){
        if (any(x %in% fancy.categories))
                return(1)
        else
                return(0)
})

ggplot(mini.subtrain, aes(y = log(price), x = factor(fancy.categories)))+
        geom_jitter(alpha= 0.4, col ="green")+
        geom_boxplot(fill="navy")
```

```{r}
mini.subtrain$cheap.categories <- sapply(mini.subtrain$category_name,function(x){
        if (any(x %in% cheap.categories))
                return(1)
        else
                return(0)
})

ggplot(mini.subtrain, aes(y = log(price), x = factor(cheap.categories)))+
        geom_jitter(alpha= 0.4, col ="green")+
        geom_boxplot(fill="navy")
```

These could be some useful features to add into our data sets:

```{r}

subtrain$fancy.categories <- sapply(subtrain$category_name,function(x){
        if (any(x %in% fancy.categories))
                return(1)
        else
                return(0)
})

test$fancy.categories <- sapply(test$category_name,function(x){
        if (any(x %in% fancy.categories))
                return(1)
        else
                return(0)
})

validation$fancy.categories <- sapply(validation$category_name,function(x){
        if (any(x %in% fancy.categories))
                return(1)
        else
                return(0)
})



subtrain$cheap.categories <- sapply(subtrain$category_name,function(x){
        if (any(x %in% cheap.categories))
                return(1)
        else
                return(0)
})

test$cheap.categories <- sapply(test$category_name,function(x){
        if (any(x %in% cheap.categories))
                return(1)
        else
                return(0)
})

validation$cheap.categories <- sapply(validation$category_name,function(x){
        if (any(x %in% cheap.categories))
                return(1)
        else
                return(0)
})
```

#### Brand names

We also noted that there are limited brand names. Some of these may explain certain amount of variance in the price. Let's look at more closely:

```{r, fig.width=10}
summary.brand_name <- mini.subtrain %>% group_by(brand_name) %>% summarise(median.log.price = median(log(price)), size = n(), perct_fraction = n()/nrow(mini.subtrain)) %>% arrange(desc(median.log.price))
head(summary.brand_name,10)
hist( summary.brand_name$median.log.price, col ="navy", breaks = 50)
```

```{r}
fancy.brands <- summary.brand_name$brand_name[summary.brand_name$median.log.price > quantile(log(mini.subtrain$price),0.95)]
cheap.brands <- summary.brand_name$brand_name[summary.brand_name$median.log.price < quantile(log(mini.subtrain$price),0.05)]

fancy.brands;cheap.brands
```

Let's make categorical features using these brand names:
```{r}
mini.subtrain$fancy.brands <- sapply(mini.subtrain$brand_name,function(x){
        if (any(x %in% fancy.brands))
                return(1)
        else
                return(0)
})

ggplot(mini.subtrain, aes(y = log(price), x = factor(fancy.brands)))+
        geom_jitter(alpha= 0.4, col ="green")+
        geom_boxplot(fill="navy")
```

```{r}
mini.subtrain$cheap.brands <- sapply(mini.subtrain$brand_name,function(x){
        if (any(x %in% cheap.brands))
                return(1)
        else
                return(0)
})

ggplot(mini.subtrain, aes(y = log(price), x = factor(cheap.brands)))+
        geom_jitter(alpha= 0.4, col ="green")+
        geom_boxplot(fill="navy")
```

These could be some useful features to add into our data sets:

```{r}

subtrain$fancy.brands <- sapply(subtrain$brand_name,function(x){
        if (any(x %in% fancy.brands))
                return(1)
        else
                return(0)
})

test$fancy.brands <- sapply(test$brand_name,function(x){
        if (any(x %in% fancy.brands))
                return(1)
        else
                return(0)
})

validation$fancy.brands <- sapply(validation$brand_name,function(x){
        if (any(x %in% fancy.brands))
                return(1)
        else
                return(0)
})



subtrain$cheap.brands <- sapply(subtrain$brand_name,function(x){
        if (any(x %in% cheap.brands))
                return(1)
        else
                return(0)
})

test$cheap.brands <- sapply(test$brand_name,function(x){
        if (any(x %in% cheap.brands))
                return(1)
        else
                return(0)
})

validation$cheap.brands <- sapply(validation$brand_name,function(x){
        if (any(x %in% cheap.brands))
                return(1)
        else
                return(0)
})
```

#### Suspect marketing words

Here, lets look at some usual suspect marketing words in the item description:

```{r, fig.width=12, fig.height=12}
suspect <- data.frame(log.price = log(mini.subtrain$price+1))
suspect$sale <- str_count(tolower(mini.subtrain$item_description), "sale")
suspect$free <- str_count(tolower(mini.subtrain$item_description), "free")
suspect$save <- str_count(tolower(mini.subtrain$item_description), "save")
suspect$deal <- str_count(tolower(mini.subtrain$item_description), "deal")
suspect$good <- str_count(tolower(mini.subtrain$item_description), "good")
suspect$steal <- str_count(tolower(mini.subtrain$item_description), "steal")
suspect$now <- str_count(tolower(mini.subtrain$item_description), "now")
suspect$cheap <- str_count(tolower(mini.subtrain$item_description), "cheap")
suspect$buy <- str_count(tolower(mini.subtrain$item_description), "buy")
suspect$excellent <- str_count(tolower(mini.subtrain$item_description), "excellent")
suspect$great <- str_count(tolower(mini.subtrain$item_description), "great")

pairs(log.price ~ .,pch = 19, col = "navy", cex = 0.2, data = suspect)
```

Some of these features can be useful. Let's look at if there is any correlation between them and the outcome:

```{r, fig.width=10,fig.height=10}
library(corrplot)
corrplot(cor(suspect), method = c("shade"), bg = "gray", addgrid.col = "gray")
```

They don't appear to be highly correlated. What happens if we put them into a linear model along with the response variable:

```{r,fig.width=7,fig.height=7}
lmfit <- lm(log.price ~ . , data = suspect)
summary(lmfit)
par(mfrow = c(2,2))
plot(lmfit)
```

Even though the model is not perfect, some of these features could be useful to explain variability. I will add them into our data sets:

```{r}

# Write a function to consistently add the features into data sets:

suspect.marketing.feature.engineer <- function(data.set){

        suspect <- data.frame(dummy= 1:nrow(data.set))
        suspect$sale <- str_count(tolower(data.set$item_description), "sale")
        suspect$free <- str_count(tolower(data.set$item_description), "free")
        suspect$save <- str_count(tolower(data.set$item_description), "save")
        suspect$deal <- str_count(tolower(data.set$item_description), "deal")
        suspect$good <- str_count(tolower(data.set$item_description), "good")
        suspect$steal <- str_count(tolower(data.set$item_description), "steal")
        suspect$now <- str_count(tolower(data.set$item_description), "now")
        suspect$cheap <- str_count(tolower(data.set$item_description), "cheap")
        suspect$buy <- str_count(tolower(data.set$item_description), "buy")
        suspect$excellent <- str_count(tolower(data.set$item_description), "excellent")
        suspect$great <- str_count(tolower(data.set$item_description), "great")
        suspect <- suspect[,-1]
        data.set <- cbind(data.set,suspect)
        
        return(data.set)

}

mini.subtrain <- suspect.marketing.feature.engineer(mini.subtrain)
subtrain <- suspect.marketing.feature.engineer(subtrain)
validation <- suspect.marketing.feature.engineer(validation)
test <- suspect.marketing.feature.engineer(test)
```



####Looking for 'free' or 'unpriced' items v.s. most expensive items

Our first suspect would be there are certain keywords that are frequently associated with free items, which could be a good feature to predict these items:
```{r,cache=TRUE}
sum(subtrain$price == 0)
```

There are about 400 free items in the subtraining set.

```{r}
library(tm); library(SnowballC);library(wordcloud)

# Gratefully learned from:
# http://www.sthda.com/english/wiki/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know

text <- subtrain$item_description[subtrain$price == 0]
docs <- Corpus(VectorSource(text))

######################
# Text transformation
######################
# Transformation is performed using tm_map() function to replace, 
# for example, special characters from the text.

#Replacing “/”, “@” and “|” with space:
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")

####################
# Cleaning the text
####################

# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector if needed
# docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
docs <- tm_map(docs, stemDocument)

################################
# Build a term-document matrix
################################

# Document matrix is a table containing the frequency of the words. Column names are words and row names are documents. The function TermDocumentMatrix() from text mining package can be used as follow :

dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)

###########################
#Generate the Word cloud
###########################

# The importance of words can be illustrated as a word cloud as follow :

set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

# Arguments of the word cloud generator function :
# words : the words to be plotted
# freq : their frequencies
# min.freq : words with frequency below min.freq will not be plotted
# max.words : maximum number of words to be plotted
# random.order : plot words in random order. If false, they will be plotted in decreasing frequency
# rot.per : proportion words with 90 degree rotation (vertical text)
# colors : color words from least to most frequent. Use, for example, colors =“black” for single color.

#################################################
#Explore frequent terms and their associations
#################################################

# You can have a look at the frequent terms in the term-document matrix as follow. In the example below we want to find words that occur at least 50 times :

findFreqTerms(dtm, lowfreq = 50)

# You can analyze the association between frequent terms (i.e., terms which correlate) using findAssocs() function. The R code below identifies which words are associated with “size” in the current text :

findAssocs(dtm, terms = "size", corlimit = 0.3)
# This makes good sense!

########################
#Plot word frequencies
#######################

# The frequency of the first 10 frequent words are plotted :

barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col ="navy", main ="Most frequent words",
        xlab = "Word frequencies", horiz = T)
```


How about we look at the top 1% most expensive items?

```{r}
text <- subtrain$item_description[subtrain$price >= quantile(subtrain$price,0.99)]
docs <- Corpus(VectorSource(text))

######################
# Text transformation
######################
# Transformation is performed using tm_map() function to replace, 
# for example, special characters from the text.

#Replacing “/”, “@” and “|” with space:
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")

####################
# Cleaning the text
####################

# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector if needed
# docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
docs <- tm_map(docs, stemDocument)

################################
# Build a term-document matrix
################################

# Document matrix is a table containing the frequency of the words. Column names are words and row names are documents. The function TermDocumentMatrix() from text mining package can be used as follow :

dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)

###########################
#Generate the Word cloud
###########################

# The importance of words can be illustrated as a word cloud as follow :

set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

# Arguments of the word cloud generator function :
# words : the words to be plotted
# freq : their frequencies
# min.freq : words with frequency below min.freq will not be plotted
# max.words : maximum number of words to be plotted
# random.order : plot words in random order. If false, they will be plotted in decreasing frequency
# rot.per : proportion words with 90 degree rotation (vertical text)
# colors : color words from least to most frequent. Use, for example, colors =“black” for single color.

#################################################
#Explore frequent terms and their associations
#################################################

# You can have a look at the frequent terms in the term-document matrix as follow. In the example below we want to find words that occur at least 500 times :

findFreqTerms(dtm, lowfreq = 500)

# You can analyze the association between frequent terms (i.e., terms which correlate) using findAssocs() function. The R code below identifies which words are associated with “size” in the current text :

findAssocs(dtm, terms = "size", corlimit = 0.3)
# This makes good sense!

########################
#Plot word frequencies
#######################

# The frequency of the first 10 frequent words are plotted :

barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col ="navy", main ="Most frequent words",
        xlab = "Word frequencies", horiz = T)
```

Now let's look at the words in free and expensive items together:

```{r}
# Write a function to streamline text mininig

parse_text <- function(text){
        docs <- Corpus(VectorSource(text))

        ######################
        # Text transformation
        ######################
        # Transformation is performed using tm_map() function to replace, 
        # for example, special characters from the text.
        
        #Replacing “/”, “@” and “|” with space:
        toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
        docs <- tm_map(docs, toSpace, "/")
        docs <- tm_map(docs, toSpace, "@")
        docs <- tm_map(docs, toSpace, "\\|")
        
        ####################
        # Cleaning the text
        ####################
        
        # Convert the text to lower case
        docs <- tm_map(docs, content_transformer(tolower))
        # Remove numbers
        docs <- tm_map(docs, removeNumbers)
        # Remove english common stopwords
        docs <- tm_map(docs, removeWords, stopwords("english"))
        # Remove your own stop word
        # specify your stopwords as a character vector if needed
        # docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
        # Remove punctuations
        docs <- tm_map(docs, removePunctuation)
        # Eliminate extra white spaces
        docs <- tm_map(docs, stripWhitespace)
        # Text stemming
        docs <- tm_map(docs, stemDocument)
        
        ################################
        # Build a term-document matrix
        ################################
        
        # Document matrix is a table containing the frequency of the words. Column names are words and row names are documents. The function TermDocumentMatrix() from text mining package can be used as follow :
        
        dtm <- TermDocumentMatrix(docs)
        m <- as.matrix(dtm)
        v <- sort(rowSums(m),decreasing=TRUE)
        d <- data.frame(Word = as.character(names(v)),freq=v, Freq = (v/sum(v)) * 100)
        
        return(d)
}
```


```{r, fig.width=12, fig.height=7}
sorted.table.free <- parse_text(text = mini.subtrain$item_description[mini.subtrain$price == 0])
sorted.table.expensive <- parse_text(text = subtrain$item_description[subtrain$price >= quantile(subtrain$price,0.99)])

# Get top 100 most frequent words from each group 
sorted.table.free.100 <- head(sorted.table.free,100)
sorted.table.expensive.100 <- head(sorted.table.expensive,100)

merged.table <- merge(sorted.table.free.100,sorted.table.expensive.100, by = c("Word"), all = TRUE,
                      suffixes = c("free","expensive"))
row.names(merged.table) = merged.table$Word
merged.table <- as.matrix(merged.table[,which(grepl("^Freq",colnames(merged.table)))])
#Replace NA's with zero
merged.table[is.na(merged.table[,1]),1] <- 0
merged.table[is.na(merged.table[,2]),2] <- 0
library(pheatmap)
pheatmap(t(merged.table))
```

There could be some potential to seperate price segments from each other by using the frequencies of different words. Let's perform a more extensive segmentation analysis by writing a feature creation algorithm.

####Feature engineering in item description through price segmentation

```{r,eval=FALSE}
####################
# Long operation! 
####################

# Divide the subtraining set into 19 log(price) quantiles + 1 "free" item bin
# Totally 19 segments
percentiles <- seq(0.05, 0.99, 0.05)
merged.table <- parse_text(mini.subtrain$item_description[mini.subtrain$price == 0])[,c(1,3)]
colnames(merged.table)[2] <- paste0("price_percentile_0")
merged.table <- rbind(head(merged.table,10),tail(merged.table,10))
mini.subtrain.nonzero <- mini.subtrain[mini.subtrain$price != 0,]

for(i in seq_along(percentiles)){
        # Perform text frequency mining within the first segment and hold the results
        text <- mini.subtrain.nonzero$item_description[log(mini.subtrain.nonzero$price) >= quantile(log(mini.subtrain.nonzero$price),percentiles[i])]
        sorted.table <- parse_text(text)
        sorted.table <- sorted.table[,c(1,3)]
        colnames(sorted.table)[2] <- paste0("price_percentile_%",percentiles[i]*100)
        sorted.table <- rbind(head(sorted.table,10),tail(sorted.table,10))
        merged.table <- merge(merged.table,sorted.table, by = "Word", all = TRUE)
        # continue until all price segments are finished
} 


merged.table <- merged.table[!is.na(merged.table$Word),]

row.names(merged.table) = merged.table$Word
merged.table <- dplyr::select(merged.table, -Word)
merged.table <- as.matrix(merged.table)

#Replace NA's with zero
for (i in 1:20){merged.table[is.na(merged.table[,i]),i] <- 0}
#Save for future use
saveRDS(merged.table,"merged_table.rds")
```

```{r,fig.width=10, fig.height= 7}
# easy load
merged.table <- readRDS("merged_table.rds")
pheatmap(t.merged.table, cluster_rows = FALSE)
```

The item description text mining this way pretty much brings us noise.

####Feature engineering in brand name through price segmentation

```{r,eval=FALSE}
####################
# Long operation! 
####################

# Divide the subtraining set into 19 log(price) quantiles + 1 "free" item bin
# Totally 19 segments
percentiles <- seq(0.05, 0.99, 0.05)
merged.table <- parse_text(mini.subtrain$brand_name[mini.subtrain$price == 0])[,c(1,3)]
colnames(merged.table)[2] <- paste0("price_percentile_0")
merged.table <- rbind(head(merged.table,10),tail(merged.table,10))

mini.subtrain.nonzero <- mini.subtrain[mini.subtrain$price != 0,]

for(i in seq_along(percentiles)){
        # Perform text frequency mining within the first segment and hold the results
        text <- mini.subtrain.nonzero$brand_name[log(mini.subtrain.nonzero$price) >= quantile(log(mini.subtrain.nonzero$price),percentiles[i])]
        sorted.table <- parse_text(text)
        sorted.table <- sorted.table[,c(1,3)]
        colnames(sorted.table)[2] <- paste0("price_percentile_%",percentiles[i]*100)
        sorted.table <- rbind(head(sorted.table,10),tail(sorted.table,10))
        merged.table <- merge(merged.table,sorted.table, by = "Word", all = TRUE)
        # continue until all price segments are finished
} 


merged.table <- merged.table[!is.na(merged.table$Word),]
merged.table <- unique.data.frame(merged.table)

row.names(merged.table) = merged.table$Word
merged.table <- dplyr::select(merged.table, -Word)
merged.table <- as.matrix(merged.table)

#Replace NA's with zero
for (i in 1:20){merged.table[is.na(merged.table[,i]),i] <- 0}
#Save for future use
saveRDS(merged.table,"merged_table.rds")
```

```{r,fig.width=10, fig.height= 7}
# easy load
merged.table <- readRDS("merged_table.rds")
pheatmap(t(merged.table), cluster_rows = FALSE, scale = "row")
```

A slightly different version of the algorithm is just to compare Top50 most frequent words in brand names:


```{r,eval=FALSE}
####################
# Long operation! 
####################

# Divide the subtraining set into 19 log(price) quantiles + 1 "free" item bin
# Totally 19 segments
percentiles <- seq(0.05, 0.99, 0.05)
merged.table <- parse_text(mini.subtrain$brand_name[mini.subtrain$price == 0])[,c(1,3)]
colnames(merged.table)[2] <- paste0("price_percentile_0")
merged.table <- head(merged.table,50)

mini.subtrain.nonzero <- mini.subtrain[mini.subtrain$price != 0,]

for(i in seq_along(percentiles)){
        # Perform text frequency mining within the first segment and hold the results
        text <- mini.subtrain.nonzero$brand_name[log(mini.subtrain.nonzero$price) >= quantile(log(mini.subtrain.nonzero$price),percentiles[i])]
        sorted.table <- parse_text(text)
        sorted.table <- sorted.table[,c(1,3)]
        colnames(sorted.table)[2] <- paste0("price_percentile_%",percentiles[i]*100)
        sorted.table <- head(sorted.table,50)
        merged.table <- merge(merged.table,sorted.table, by = "Word", all = TRUE)
        # continue until all price segments are finished
} 


merged.table <- merged.table[!is.na(merged.table$Word),]
merged.table <- unique.data.frame(merged.table)

row.names(merged.table) = merged.table$Word
merged.table <- dplyr::select(merged.table, -Word)
merged.table <- as.matrix(merged.table)

#Replace NA's with zero
for (i in 1:20){merged.table[is.na(merged.table[,i]),i] <- 0}
#Save for future use
saveRDS(merged.table,"merged_table.rds")
```

```{r,fig.width=10, fig.height= 7}
# easy load
merged.table <- readRDS("merged_table.rds")
pheatmap(t(merged.table), cluster_rows = F, scale = "row")
```

Aha! Here we notice a few things that can be potentially interesting. 

- appl : enriched in most expensive items
- victoria, secret, lululemon,lularo, pink, michael, kor: these are words that can potentially form gradients across the price segments.

Let's engineer new features to add them into our data sets:

```{r}
# Write a function to streamline feature engineering
word.feature.engineer <- function(text.vector,pattern){
        #returns a vector that gives the counts of given pattern in each of the element of the text vector  
        text.vector <- tolower(text.vector)
        counts <- 0
        for(i in seq_along(text.vector)){
                counts[i] <- str_count(text.vector[i],pattern)
        }
        counts[is.na(counts)] <- 0
        return(counts)
}

victoria.test <- word.feature.engineer(mini.subtrain$brand_name,"victoria")
secret.test <- word.feature.engineer(mini.subtrain$brand_name,"secret")
cor(victoria.test,secret.test) # These two words are tighly associated
```

```{r}
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(victoria.test)))+
        geom_boxplot(fill = c("navy","red"))
```

This doesn't seem to be a fascinating feature.

```{r}
pink.test <- word.feature.engineer(mini.subtrain$brand_name,"pink")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(pink.test)))+
        geom_boxplot(fill = c("navy","red"))
```

```{r}
appl.test <- word.feature.engineer(mini.subtrain$brand_name,"appl")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(appl.test)))+
        geom_boxplot(fill = c("navy","red"))
```
```{r}
michael.test <- word.feature.engineer(mini.subtrain$brand_name,"michael")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(michael.test)))+
        geom_boxplot(fill = c("navy","red"))
```

michael in brandname looks like a useful feature. Let's add into our data sets:

```{r}
# Write a function to streamline feature engineering as binary variable
word.feature.engineer.binary <- function(text.vector,pattern){
        #returns a vector that checks a given pattern in each of the element of the text vector  
        text.vector <- tolower(text.vector)
        binary.vector <- sapply(text.vector,function(x){
                return(ifelse(grepl(pattern,x),1,0))
        })
        return(binary.vector)
} 
```



```{r,eval=FALSE} 
# Long operation!
mini.subtrain$michael.brand <- word.feature.engineer.binary(mini.subtrain$brand_name,"michael")
subtrain$michael.brand <- word.feature.engineer.binary(subtrain$brand_name,"michael")
validation$michael.brand <- word.feature.engineer.binary(validation$brand_name,"michael")
test$michael.brand <- word.feature.engineer.binary(test$brand_name,"michael")
```

At this point it is necessary to save engineered data sets for future easy loading:

```{r}
saveRDS(mini.subtrain,"mini_subtrain.rds")
saveRDS(subtrain,"subtrain.rds")
saveRDS(validation,"validation.rds")
saveRDS(test,"test.rds")
```

####Feature engineering in category name through price segmentation

Let;s now look at the category names

```{r,eval=FALSE}
####################
# Long operation! 
####################

# Divide the subtraining set into 19 log(price) quantiles + 1 "free" item bin
# Totally 19 segments
percentiles <- seq(0.05, 0.99, 0.05)
merged.table <- parse_text(mini.subtrain$category_name[mini.subtrain$price == 0])[,c(1,3)]
colnames(merged.table)[2] <- paste0("price_percentile_0")
merged.table <- head(merged.table,50)

mini.subtrain.nonzero <- mini.subtrain[mini.subtrain$price != 0,]

for(i in seq_along(percentiles)){
        # Perform text frequency mining within the first segment and hold the results
        text <- mini.subtrain.nonzero$category_name[log(mini.subtrain.nonzero$price) >= quantile(log(mini.subtrain.nonzero$price),percentiles[i])]
        sorted.table <- parse_text(text)
        sorted.table <- sorted.table[,c(1,3)]
        colnames(sorted.table)[2] <- paste0("price_percentile_%",percentiles[i]*100)
        sorted.table <- head(sorted.table,50)
        merged.table <- merge(merged.table,sorted.table, by = "Word", all = TRUE)
        # continue until all price segments are finished
} 


merged.table <- merged.table[!is.na(merged.table$Word),]
merged.table <- unique.data.frame(merged.table)

row.names(merged.table) = merged.table$Word
merged.table <- dplyr::select(merged.table, -Word)
merged.table <- as.matrix(merged.table)

#Replace NA's with zero
for (i in 1:20){merged.table[is.na(merged.table[,i]),i] <- 0}
#Save for future use
saveRDS(merged.table,"merged_table.rds")
```

```{r,fig.width=10, fig.height= 7}
# easy load
merged.table <- readRDS("merged_table.rds")
pheatmap(t(merged.table), cluster_rows = F, scale = "row")
```

Applying a slighly different version of the algorithm:

```{r,eval=FALSE}
####################
# Long operation! 
####################

# Divide the subtraining set into 19 log(price) quantiles + 1 "free" item bin
# Totally 19 segments
percentiles <- seq(0.05, 0.99, 0.05)
merged.table <- parse_text(mini.subtrain$category_name[mini.subtrain$price == 0])[,c(1,3)]
colnames(merged.table)[2] <- paste0("price_percentile_0")
merged.table <- rbind(head(merged.table,10),tail(merged.table,10))

mini.subtrain.nonzero <- mini.subtrain[mini.subtrain$price != 0,]

for(i in seq_along(percentiles)){
        # Perform text frequency mining within the first segment and hold the results
        text <- mini.subtrain.nonzero$category_name[log(mini.subtrain.nonzero$price) >= quantile(log(mini.subtrain.nonzero$price),percentiles[i])]
        sorted.table <- parse_text(text)
        sorted.table <- sorted.table[,c(1,3)]
        colnames(sorted.table)[2] <- paste0("price_percentile_%",percentiles[i]*100)
        sorted.table <- rbind(head(sorted.table,10),tail(sorted.table,10))
        merged.table <- merge(merged.table,sorted.table, by = "Word", all = TRUE)
        # continue until all price segments are finished
} 


merged.table <- merged.table[!is.na(merged.table$Word),]
merged.table <- unique.data.frame(merged.table)

row.names(merged.table) = merged.table$Word
merged.table <- dplyr::select(merged.table, -Word)
merged.table <- as.matrix(merged.table)

#Replace NA's with zero
for (i in 1:20){merged.table[is.na(merged.table[,i]),i] <- 0}
#Save for future use
saveRDS(merged.table,"merged_table.rds")
```

```{r,fig.width=10, fig.height= 7}
# easy load
merged.table <- readRDS("merged_table.rds")
pheatmap(t(merged.table), cluster_rows = FALSE, scale = "row")
```

The category name also looks pretty much noise when explored in this way.

####Feature engineering in name through price segmentation 

```{r,eval=FALSE}
####################
# Long operation! 
####################

# Divide the subtraining set into 19 log(price) quantiles + 1 "free" item bin
# Totally 19 segments
percentiles <- seq(0.05, 0.99, 0.05)
merged.table <- parse_text(mini.subtrain$name[mini.subtrain$price == 0])[,c(1,3)]
colnames(merged.table)[2] <- paste0("price_percentile_0")
merged.table <- rbind(head(merged.table,10),tail(merged.table,10))

mini.subtrain.nonzero <- mini.subtrain[mini.subtrain$price != 0,]

for(i in seq_along(percentiles)){
        # Perform text frequency mining within the first segment and hold the results
        text <- mini.subtrain.nonzero$name[log(mini.subtrain.nonzero$price) >= quantile(log(mini.subtrain.nonzero$price),percentiles[i])]
        sorted.table <- parse_text(text)
        sorted.table <- sorted.table[,c(1,3)]
        colnames(sorted.table)[2] <- paste0("price_percentile_%",percentiles[i]*100)
        sorted.table <- rbind(head(sorted.table,10),tail(sorted.table,10))
        merged.table <- merge(merged.table,sorted.table, by = "Word", all = TRUE)
        # continue until all price segments are finished
} 


merged.table <- merged.table[!is.na(merged.table$Word),]
merged.table <- unique.data.frame(merged.table)

row.names(merged.table) = merged.table$Word
merged.table <- dplyr::select(merged.table, -Word)
merged.table <- as.matrix(merged.table)

#Replace NA's with zero
for (i in 1:20){merged.table[is.na(merged.table[,i]),i] <- 0}
#Save for future use
saveRDS(merged.table,"merged_table.rds")
```

```{r,fig.width=10, fig.height= 7}
# easy load
merged.table <- readRDS("merged_table.rds")
pheatmap(t(merged.table), cluster_rows = FALSE, scale = "row")
```

```{r}
pink.test <- word.feature.engineer.binary(mini.subtrain$name,"pink")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(pink.test)))+
        geom_boxplot(fill = c("navy","red"))
```

```{r}
size.test <- word.feature.engineer.binary(mini.subtrain$name,"size")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(size.test)))+
        geom_boxplot(fill = c("navy","red"))
```

```{r}
jordan.test <- word.feature.engineer.binary(mini.subtrain$name,"jordan")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(jordan.test)))+
        geom_boxplot(fill = c("navy","red"))
```

```{r}
shirt.test <- word.feature.engineer.binary(mini.subtrain$name,"shirt")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(shirt.test)))+
        geom_boxplot(fill = c("navy","red"))
```

```{r}
michael.test <- word.feature.engineer.binary(mini.subtrain$name,"michael")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(michael.test)))+
        geom_boxplot(fill = c("navy","red"))
cor(michael.test,mini.subtrain$michael.brand)
```

```{r}
iphon.test <- word.feature.engineer.binary(mini.subtrain$name,"iphon")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(iphon.test)))+
        geom_boxplot(fill = c("navy","red"))
```

```{r}
bundl.test <- word.feature.engineer.binary(mini.subtrain$name,"bundl")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(bundl.test)))+
        geom_boxplot(fill = c("navy","red"))
```

Let's add 3 of these features into our data sets as well:

```{r, eval=FALSE}
# Long operation!
mini.subtrain$jordan.name <- word.feature.engineer.binary(mini.subtrain$name,"jordan")
subtrain$jordan.name <- word.feature.engineer.binary(subtrain$name,"jordan")
validation$jordan.name <- word.feature.engineer.binary(validation$name,"jordan")
test$jordan.name <- word.feature.engineer.binary(test$brand,"jordan")

mini.subtrain$iphon.name <- word.feature.engineer.binary(mini.subtrain$name,"iphon")
subtrain$iphon.name <- word.feature.engineer.binary(subtrain$name,"iphon")
validation$iphon.name <- word.feature.engineer.binary(validation$name,"iphon")
test$iphon.name <- word.feature.engineer.binary(test$brand,"iphon")

mini.subtrain$bundl.name <- word.feature.engineer.binary(mini.subtrain$name,"bundl")
subtrain$bundl.name <- word.feature.engineer.binary(subtrain$name,"bundl")
validation$bundl.name <- word.feature.engineer.binary(validation$name,"bundl")
test$bundl.name <- word.feature.engineer.binary(test$brand,"bundl")

#At this point it is necessary to save engineered data sets again for future easy loading:
saveRDS(mini.subtrain,"mini_subtrain.rds")
saveRDS(subtrain,"subtrain.rds")
saveRDS(validation,"validation.rds")
saveRDS(test,"test.rds")
```

#### Looking at [rm] names and descriptions

In the data description it was mentioned that this string was used to replce whereever the price was mentioned in the item description or name. Let's see if the occurance of this placeholder can explain some variance in price:

```{r}
placeholder.test <- word.feature.engineer.binary(mini.subtrain$name,"\\[rm]")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(placeholder.test)))+
        geom_boxplot(fill = c("navy","red"))
```

```{r}
placeholder.test <- word.feature.engineer.binary(mini.subtrain$item_description,"\\[rm]")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(placeholder.test)))+
        geom_boxplot(fill = c("navy","red"))
```

```{r}
placeholder.test <- word.feature.engineer(mini.subtrain$name,"\\[rm]")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(placeholder.test)))+
        geom_boxplot(fill = c("navy","red","lightgreen"))
```

This doesn't seem to be a good feature.

#### Capital letters

This is something that we haven't looked yet. Let's see if the count the occurance of capital letters that could explain anything in the price:

```{r}
capital.letter.counter <- function(text.vector){
        # Return the occurance of capital letters in the given text vector
       capital.letter.count <- sapply(text.vector,function(x){
        return(length(unlist(str_extract_all(x,"[A-Z]"))))})
        
}

capital.letter.test <- capital.letter.counter(mini.subtrain$item_description)
plot(y =log(mini.subtrain$price), x = log(capital.letter.test),pch = 19, col = "navy", cex = 0.2)
```

This pretty much looks like noise.

```{r}
capital.letter.test <- capital.letter.counter(mini.subtrain$name)
plot(y =log(mini.subtrain$price), x = log(capital.letter.test),pch = 19, col = "navy", cex = 0.2)
```

This pretty much looks like noise.

```{r}
capital.letter.test <- capital.letter.counter(mini.subtrain$brand_name)
plot(y =log(mini.subtrain$price), x = log(capital.letter.test),pch = 19, col = "navy", cex = 0.2)
```

Capital letter count in brand_name might have some trend.

```{r}
capital.letter.test <- capital.letter.counter(mini.subtrain$category_name)
plot(y =log(mini.subtrain$price), x = log(capital.letter.test),pch = 19, col = "navy", cex = 0.2)
```

This pretty much looks like noise.

Let's add the capital letter count in the brand name as a new feature to our data sets:

```{r, eval=FALSE}
# Long operation!
mini.subtrain$cap.letter.brand <- capital.letter.counter(mini.subtrain$brand_name)
subtrain$cap.letter.brand <- capital.letter.counter(subtrain$brand_name)
validation$cap.letter.brand <- capital.letter.counter(validation$brand_name)
test$cap.letter.brand <- capital.letter.counter(test$brand_name)

#At this point it is necessary to save engineered data sets again for future easy loading:
saveRDS(mini.subtrain,"mini_subtrain.rds")
saveRDS(subtrain,"subtrain.rds")
saveRDS(validation,"validation.rds")
saveRDS(test,"test.rds")
```
#### Locking down the data sets

At this point we will finalize feature selection and end up with the final data sets to use them for training of the models and validation.

First, we need to remove string features from data sets:

```{r}
mini.subtrain <-dplyr::select(mini.subtrain, -name, -item_description,-category_name, -brand_name)
subtrain <-dplyr::select(subtrain, -name, -item_description,-category_name, -brand_name)
validation <-dplyr::select(validation, -name, -item_description,-category_name, -brand_name)
test <-dplyr::select(test, -name, -item_description,-category_name, -brand_name)
```

Then, let's look at the correlation between the remaining features:

```{r, fig.width=10,fig.height=10}
library(corrplot)
corrplot(cor(mini.subtrain), method = c("shade"), bg = "gray", addgrid.col = "gray")
```

We don't notice strong correlation between the remaining features. Therefore, we will remain with the final features and lock down the data sets at this point:

```{r,eval=FALSE}
#At this point it is necessary to save engineered data sets again for future easy loading:
saveRDS(mini.subtrain,"mini_subtrain_locked.rds")
saveRDS(subtrain,"subtrain_locked.rds")
saveRDS(validation,"validation_locked.rds")
saveRDS(test,"test_locked.rds")
```

Reload the locked data sets for training our models:

```{r}
mini.subtrain<- readRDS("mini_subtrain_locked.rds")
subtrain <- readRDS("subtrain_locked.rds")
validation <- readRDS("validation_locked.rds")
test <- readRDS("test_locked.rds")
```

We will fit our models using the log(price+1) as the response and will convert back when making our predictions. Let's make this conversion in the data sets:

```{r}
library(dplyr)
mini.subtrain<- mini.subtrain %>% mutate(log.price = log(price +1)) %>% select(-price)
subtrain <- subtrain %>% mutate(log.price = log(price +1)) %>% select(-price)
validation <- validation %>% mutate(log.price = log(price +1)) %>% select(-price)
```

Have a last check at missing data before we move to modeling:

```{r}
apply(is.na(subtrain),2, sum)
apply(is.na(validation),2, sum)
apply(is.na(test),2, sum)
```

Thankfully, the test set is complete. We have very few missing cases in subtrain and validation sets. We will remove them since we have fairly large data sets to train our models.

```{r}
subtrain <- subtrain[complete.cases(subtrain),]
validation <- subtrain[complete.cases(validation),]

```


# Training predictive models for estimating item price

Let's start with training stand alone regression models to estimate the price. Our first approach will be training models using the subtrain set, and trying to estimate our cross-validated error. We will then estimate the error in the validation test to go back and fine tune the models as necessary.

## Lasso regression

We have a long data set, significantly more observations than the number of predictors. Lasso would be a good place to start to see how much we can perform by using a linear model.

```{r}

library(glmnet)

# We create a matrix of predictors
x.training = model.matrix(log.price ~ .-1, data = subtrain)
x.validation = model.matrix( ~ .-1, data = validation[,-27])

y = subtrain$log.price

# Fit lasso regression to training data
fit.lasso <- glmnet(x.training,y, family = "gaussian") #family = "gaussian" for regression
plot(fit.lasso, xvar = "lambda", label = TRUE)
plot(fit.lasso, xvar = "dev", label = TRUE)

```

###Choosing the optimal lambda by using cross-validation

It is best to choose the optimal lambda using cross-validation, with the aim of minimizing **mean squared error:**:

```{r,cache=TRUE}
#cv.lasso will be the list containing the coefficients and information of our optimal model fit using cross-validation (10 fold by default)
set.seed(123)
cv.lasso <- cv.glmnet(x.training,y, family = "gaussian")
plot(cv.lasso)
```

Notice that what we are doing here is actually computationally intense.

1. We get very fine grids of lambda values.
2. Using each lambda value, we perform 10-fold cross validation:

- We fit a model using the Sum of Squares convex optimization and obtain cross-validated error (average of 10 model fits)

3. We continue steps 1 and 2 for all fine grids of lambda values in the range.

The two vertical lines are produced in the plot, the left one marks the model with min error and the middle one shows a little more restricted model 1 standard error away from the minimum error.

**When choosing the optimal model, we might prefer to get the more restricted model that is indicated by the second vertical line, which is default in glmnet package, due to the 1 standard error conversion.**

The final (optimal) model coefficients we obtained by cross-validation and the non-zero features remaining:

```{r}
head(coef(cv.lasso))
```

Note that we only have few features remaining in the linear lasso model. It is nice to see that out of the 5 features remaining, 3 of them are those we engineered from the data set!

###Making predictions using the best lasso model fit

Let's first start with the subtrain set:

```{r, fig.align='center', fig.width=6, fig.height=5}
pred <- predict(cv.lasso, newx = x.training)

#Let's write a function to streamline our analyses of the various models we will train:

log.rmse.training <- function(pred,observed,method){

#pred: vector of predicted values (in log scale)
#observed: vector of observed values (in log scale)
#method: the method name used to built the predictive model        
        
#Calculating the root mean squared error:
rmse.training = sqrt(mean((pred - observed)^2))

#Calculating the Pearson correlation:
cor.training = cor(observed,pred)

plot(x = pred, y = observed, cex = 0.5, col = "navy", pch = 19, 
     main = method,xlab = "Log(Predicted)", ylab = "Log(Observed)")
text(x = max(pred), y = min(observed)+0.7,
     labels = paste0("RMSE: ",rmse.training))
text(x = max(pred), y = min(observed)+0.2,
     labels = paste0("Pr.cor: ",cor.training))
}

log.rmse.training(pred = pred, observed = subtrain$log.price, method = "Lasso")
```

It is clear that the linear model does not explain the prices well. As we expected, we need to pursue different approaches to build our model.

## Support vector machines with a radial kernel with cross validation

We will use parallel processing since training models on the large data sets is computationally intense. In the initial attempts I tried to train the entire subtraining set, but this is not feasible with a standard computer with limited processing power.

Instead, I recall training models with modestly size data sets (~10K observations) earlier. In this case, I will get a similar size new mini.training set to train the algorithms. This will increase bias, but there is not much else we can do at the moment. In the future attempts, it is a good idea to learn how to use clusters and big data computing.

```{r}


library(caret)
# Get the mini.training set
set.seed(12345)
mini.trainID <- createDataPartition(y = subtrain$log.price,p = 0.0148, list = FALSE) # Close to 11K observations

mini.train <- subtrain[mini.trainID,] # A new training set close to 11K observations
saveRDS(mini.train,"mini_train.rds") #Save for easy loading in future
```

```{r}
setwd("~/Desktop/2016/Data_science/Kaggle/Kaggle-Mercari-Price-Suggestion-Challenge")
mini.train <- readRDS("mini_train.rds")

# Long operation in a standard computer!
library(caret)
library(parallel);library(doParallel)
# Configure parallel computation
cluster <- makeCluster(detectCores() - 1) # Leave one core for the OS
registerDoParallel(cluster)

# Configure trainControl object
fitControl <- trainControl(method = "cv", number = 5, allowParallel = TRUE)

#Train the model
set.seed(12345)
SVM <- train(log.price ~ ., data = mini.train,method = "svmRadial", trControl = fitControl, verbose = FALSE) 
#save the model for future use
saveRDS(SVM,"SVM.rds")

# Stop and De-register parallel processing cluster
stopCluster(cluster)
registerDoSEQ()
```

This runs in quite a reasonable time. Let's check how well the SVM model is doing:

```{r}
# reload the saved model
SVM <- readRDS("SVM.rds")
log.rmse.training(pred = predict(SVM,mini.train),observed = mini.train$log.price,method = " Support Vector Machines")
```


A little better than lasso, let's try other algorithms in a similar way:


## Training Random Forest algorithm with cross validation

```{r}
setwd("~/Desktop/2016/Data_science/Kaggle/Kaggle-Mercari-Price-Suggestion-Challenge")
mini.train <- readRDS("mini_train.rds")

# Long operation in a standard computer!
library(caret)
library(parallel);library(doParallel)
# Configure parallel computation
cluster <- makeCluster(detectCores() - 1) # Leave one core for the OS
registerDoParallel(cluster)

# Configure trainControl object
fitControl <- trainControl(method = "cv", number = 5, allowParallel = TRUE)

#Train the model
set.seed(12345)
RF <- train(log.price ~ ., data = mini.train,method = "rf", trControl = fitControl, verbose = FALSE) 
#save the model for future use
saveRDS(RF,"RF.rds")

# Stop and De-register parallel processing cluster
stopCluster(cluster)
registerDoSEQ()
```

This took about 6 minites, which is also reasonable.

```{r}
# reload the saved model
RF <- readRDS("RF.rds")
log.rmse.training(pred = predict(RF,mini.train),observed = mini.train$log.price,method = "Random Forest")
```

Interestingly, random forest did not perform as good as the SVM.


## Training Gradient boosting algorithm with cross validation

```{r}
start.time <- Sys.time()
setwd("~/Desktop/2016/Data_science/Kaggle/Kaggle-Mercari-Price-Suggestion-Challenge")
mini.train <- readRDS("mini_train.rds")

# Long operation in a standard computer!
library(caret)
library(parallel);library(doParallel)
# Configure parallel computation
cluster <- makeCluster(detectCores() - 1) # Leave one core for the OS
registerDoParallel(cluster)

# Configure trainControl object
fitControl <- trainControl(method = "cv", number = 5, allowParallel = TRUE)

#Train the model
set.seed(12345)
GBM <- train(log.price ~ ., data = mini.train,method = "gbm", trControl = fitControl, verbose = FALSE) 
#save the model for future use
saveRDS(GBM,"GBM.rds")

# Stop and De-register parallel processing cluster
stopCluster(cluster)
registerDoSEQ()

end.time <- Sys.time()
end.time - start.time
```
This only took 19.37832 secs.

```{r}
# reload the saved model
GBM <- readRDS("GBM.rds")
log.rmse.training(pred = predict(GBM,mini.train),observed = mini.train$log.price,method = "Gradient Boosting")
```

Similar to random forest!

###Variable importance in Gradient Boosting:

```{r}
dotPlot(varImp(GBM))
```

## Extreme gradient boosting with cross validation

```{r}
start.time <- Sys.time()
setwd("~/Desktop/2016/Data_science/Kaggle/Kaggle-Mercari-Price-Suggestion-Challenge")
mini.train <- readRDS("mini_train.rds")

# Long operation in a standard computer!
library(caret)
library(parallel);library(doParallel)
# Configure parallel computation
cluster <- makeCluster(detectCores() - 1) # Leave one core for the OS
registerDoParallel(cluster)

# Configure trainControl object
fitControl <- trainControl(method = "cv", number = 5, allowParallel = TRUE)

#Train the model
set.seed(12345)
xGBM <- train(log.price ~ ., data = mini.train,method = "xgbLinear", trControl = fitControl, verbose = FALSE) 
#save the model for future use
saveRDS(xGBM,"xGBM.rds")

# Stop and De-register parallel processing cluster
stopCluster(cluster)
registerDoSEQ()

end.time <- Sys.time()
end.time - start.time
```

This takes about 1.501966 mins.

```{r}
# reload the saved model
xGBM <- readRDS("xGBM.rds")
log.rmse.training(pred = predict(xGBM,mini.train),observed = mini.train$log.price,method = "Extreme gradient Boosting")
```

xGBM is so far the best model we obtained, but we recall that it can also overfit quite a bit!

###Variable importance in Gradient Boosting:

```{r}
dotPlot(varImp(xGBM))
```


Next, the question is: can we improve the model performance by using slightly larger training set?

Since we have enough data remaining in the subtrain, we can try doubling the amount of the data in a new trainign set and re-train all the 4 models we tested. We can test whether this will bring any improvement.

## Model Ensemble approach 

Let's try to stack the predictions of these individual models to train additional algorithms in a seperate set that we will draw from the subtraining set.

```{r}
library(caret)
# Get the mini.training set Ids we used to train above models
set.seed(12345)
mini.trainID <- createDataPartition(y = subtrain$log.price,p = 0.0148, list = FALSE) # Close to 11K observations

set.seed(12345)
# Exclude these IDs and get a new data set with ~2 times as large
new.mini.train <- createDataPartition(y = subtrain$log.price[-mini.trainID],p = 0.029, list = FALSE) # A new training set close to 108K observations

#Get this new larger set
new.large.train <- subtrain[new.mini.train,]

# Read back the saved models:
xGBM <- readRDS("xGBM.rds")
GBM <- readRDS("GBM.rds")
SVM <- readRDS("SVM.rds")
RF <- readRDS("RF.rds")

# Let's first see how well these models are doing in this independent and larger data set:
par(mfrow = c(2,2))
log.rmse.training(pred = predict(xGBM,new.large.train),observed = new.large.train$log.price,method = "Extreme gradient Boosting")
log.rmse.training(pred = predict(GBM,new.large.train),observed = new.large.train$log.price,method = "Gradient Boosting")
log.rmse.training(pred = predict(RF,new.large.train),observed = new.large.train$log.price,method = "Random Forest")
log.rmse.training(pred = predict(SVM,new.large.train),observed = new.large.train$log.price,method = "Support Vector Machines")
```

As we predicted, models perform not as good as they perform in the training set.

```{r}
# Let's stack the predictions of the models on the new data set and train new models
new.stack.set <- data.frame(log.price = new.large.train$log.price,
                        RF.predict = predict(RF, new.large.train),
                        SVM.predict = predict(SVM, new.large.train),
                        GBM.predict = predict(GBM, new.large.train),
                        xGBM.predict = predict(xGBM, new.large.train))

# Let's save it for future easier loading
saveRDS(new.stack.set,"new_stack_set.rds")
```

Next, let's train new models using these stacked predictors in this larger data set:

### Support vector machines from stacked predictors

```{r}
start.time <- Sys.time()
setwd("~/Desktop/2016/Data_science/Kaggle/Kaggle-Mercari-Price-Suggestion-Challenge")
new.stack.set <- readRDS("new_stack_set.rds")

# Long operation in a standard computer!
library(caret)
library(parallel);library(doParallel)
# Configure parallel computation
cluster <- makeCluster(detectCores() - 1) # Leave one core for the OS
registerDoParallel(cluster)

# Configure trainControl object
fitControl <- trainControl(method = "cv", number = 5, allowParallel = TRUE)

#Train the model
set.seed(12345)
SVM.stack <- train(log.price ~ ., data = new.stack.set,method = "svmRadial", trControl = fitControl, verbose = FALSE) 
#save the model for future use
saveRDS(SVM.stack,"SVM.stack.rds")

# Stop and De-register parallel processing cluster
stopCluster(cluster)
registerDoSEQ()

end.time <- Sys.time()
end.time - start.time
```

It took 10.78221 mins to run this code chunk with some heating of my computer.

```{r}
# reload the saved model
SVM.stack <- readRDS("SVM.stack.rds")
log.rmse.training(pred = predict(SVM.stack,new.stack.set),observed = new.stack.set$log.price,method = "Support vector machines on the stack")
```

Doesn't seem to further reduce RMSE when we stacked in this way. Let's also train other algorithms and check if that makes any difference.

```{r}
dotPlot(varImp(SVM.stack))
```

We note that SVM uses almost no information from the predictor formed by the SVM.predicted values. This suggest that there is no orthagonalinformation provided once a model is recycled in this way.

### Gradient boosting from stacked predictors

```{r}
start.time <- Sys.time()
setwd("~/Desktop/2016/Data_science/Kaggle/Kaggle-Mercari-Price-Suggestion-Challenge")
new.stack.set <- readRDS("new_stack_set.rds")

# Long operation in a standard computer!
library(caret)
library(parallel);library(doParallel)
# Configure parallel computation
cluster <- makeCluster(detectCores() - 1) # Leave one core for the OS
registerDoParallel(cluster)

# Configure trainControl object
fitControl <- trainControl(method = "cv", number = 5, allowParallel = TRUE)

#Train the model
set.seed(12345)
GBM.stack <- train(log.price ~ ., data = new.stack.set,method = "gbm", trControl = fitControl, verbose = FALSE) 
#save the model for future use
saveRDS(GBM.stack,"GBM.stack.rds")

# Stop and De-register parallel processing cluster
stopCluster(cluster)
registerDoSEQ()

end.time <- Sys.time()
end.time - start.time
```

This took 17.95895 secs, much shorter than the 10K observations.

```{r}
# reload the saved model
GBM.stack <- readRDS("GBM.stack.rds")
log.rmse.training(pred = predict(GBM.stack,new.stack.set),observed = new.stack.set$log.price,method = "Gradient Boosting on the stack")
```

Again, not so much improvement.


```{r}
dotPlot(varImp(GBM.stack))
```

### Extreme Gradient boosting from stacked predictors 

```{r}
start.time <- Sys.time()
setwd("~/Desktop/2016/Data_science/Kaggle/Kaggle-Mercari-Price-Suggestion-Challenge")
new.stack.set <- readRDS("new_stack_set.rds")

# Long operation in a standard computer!
library(caret)
library(parallel);library(doParallel)
# Configure parallel computation
cluster <- makeCluster(detectCores() - 1) # Leave one core for the OS
registerDoParallel(cluster)

# Configure trainControl object
fitControl <- trainControl(method = "cv", number = 5, allowParallel = TRUE)

#Train the model
set.seed(12345)
xGBM.stack <- train(log.price ~ ., data = new.stack.set,method = "xgbLinear", trControl = fitControl, verbose = FALSE) 
#save the model for future use
saveRDS(xGBM.stack,"xGBM.stack.rds")

# Stop and De-register parallel processing cluster
stopCluster(cluster)
registerDoSEQ()

end.time <- Sys.time()
end.time - start.time
```

```{r}
# reload the saved model
xGBM.stack <- readRDS("xGBM.stack.rds")
log.rmse.training(pred = predict(xGBM.stack,new.stack.set),observed = new.stack.set$log.price,method = "Extreme Gradient Boosting on the stack")
```

Not so much better, either.

### Random Forest from stacked predictors 

```{r}
start.time <- Sys.time()
setwd("~/Desktop/2016/Data_science/Kaggle/Kaggle-Mercari-Price-Suggestion-Challenge")
new.stack.set <- readRDS("new_stack_set.rds")

# Long operation in a standard computer!
library(caret)
library(parallel);library(doParallel)
# Configure parallel computation
cluster <- makeCluster(detectCores() - 1) # Leave one core for the OS
registerDoParallel(cluster)

# Configure trainControl object
fitControl <- trainControl(method = "cv", number = 5, allowParallel = TRUE)

#Train the model
set.seed(12345)
RF.stack <- train(log.price ~ ., data = new.stack.set,method = "rf", trControl = fitControl, verbose = FALSE) 
#save the model for future use
saveRDS(RF.stack,"RF.stack.rds")

# Stop and De-register parallel processing cluster
stopCluster(cluster)
registerDoSEQ()

end.time <- Sys.time()
end.time - start.time
```

It took 12.36929 mins, approximately double amount of the time took for the smaller training set.

```{r}
# reload the saved model
RF.stack <- readRDS("RF.stack.rds")
log.rmse.training(pred = predict(RF.stack,new.stack.set),observed = new.stack.set$log.price,method = "Random Forest on the stack")
```


This looks quite some improvement amongst all the models we tested so far. 

Now the questions is: how far this can go? In other words, can we perform another iteration that might potentially improve the prediction?

Let's give a try:

## Stacking iteration 2:

Let's try to stack the predictions of these individual models to train additional algorithms in a seperate set that we will draw from the subtraining set.

```{r}
library(caret)
# Get the mini.training set Ids we used to train first generation models
set.seed(12345)
mini.trainID <- createDataPartition(y = subtrain$log.price,p = 0.0148, list = FALSE) # Close to 11K observations

set.seed(12345)
# Get the new.mini.train Ids we used to train second generation models
new.mini.trainID <- createDataPartition(y = subtrain$log.price[-mini.trainID],p = 0.029, list = FALSE) 

# A new training set close to 108K observations

# Exclude these IDs and get a new data set with ~2 times as large
second.stack.IDs <- createDataPartition(y = subtrain$log.price[-c(mini.trainID,new.mini.trainID)],p = 0.06, list = FALSE) 

#Get this new larger set, ~ 42K observations
second.stack.train <- subtrain[second.stack.IDs,]

# Read back the saved primary models:
xGBM <- readRDS("xGBM.rds")
GBM <- readRDS("GBM.rds")
SVM <- readRDS("SVM.rds")
RF <- readRDS("RF.rds")

# Let's stack the predictions of the models on the new data set and train new models
new.stack.set2 <- data.frame(log.price = second.stack.train$log.price,
                        RF.predict = predict(RF, second.stack.train),
                        SVM.predict = predict(SVM, second.stack.train),
                        GBM.predict = predict(GBM, second.stack.train),
                        xGBM.predict = predict(xGBM, second.stack.train))

# Let's save it for future easier loading
saveRDS(new.stack.set,"new_stack_set2.rds")

# Read back the saved secondary models:
xGBM.stack <- readRDS("xGBM.stack.rds")
GBM.stack <- readRDS("GBM.stack.rds")
SVM.stack <- readRDS("SVM.stack.rds")
RF.stack <- readRDS("RF.stack.rds")

# Let's first see how well these models are doing in this independent and larger data set:
par(mfrow = c(2,2))
log.rmse.training(pred = predict(xGBM.stack,new.stack.set2),observed = new.stack.set2$log.price,method = "Extreme gradient Boosting")
log.rmse.training(pred = predict(GBM.stack,new.stack.set2),observed = new.stack.set2$log.price,method = "Gradient Boosting")
log.rmse.training(pred = predict(RF.stack,new.stack.set2),observed = new.stack.set2$log.price,method = "Random Forest")
log.rmse.training(pred = predict(SVM.stack,new.stack.set2),observed = new.stack.set2$log.price,method = "Support Vector Machines")

```


Next, let's train new models using these stacked predictors in this larger data set:

### Random forest second stack:


```{r}
second.stack.set <- data.frame(log.price = new.stack.set2$log.price,
                        RF.predict = predict(RF.stack, new.stack.set2),
                        SVM.predict = predict(SVM.stack, new.stack.set2),
                        GBM.predict = predict(GBM.stack, new.stack.set2),
                        xGBM.predict = predict(xGBM.stack, new.stack.set2))

# Let's save it for future easier loading
saveRDS(second.stack.set,"second.stack.set.rds")
```

### Random Forest from stacked predictors 

```{r}
start.time <- Sys.time()
setwd("~/Desktop/2016/Data_science/Kaggle/Kaggle-Mercari-Price-Suggestion-Challenge")
second.stack.set <- readRDS("second.stack.set.rds")

# Long operation in a standard computer!
library(caret)
library(parallel);library(doParallel)
# Configure parallel computation
cluster <- makeCluster(detectCores() - 1) # Leave one core for the OS
registerDoParallel(cluster)

# Configure trainControl object
fitControl <- trainControl(method = "cv", number = 5, allowParallel = TRUE)

#Train the model
set.seed(12345)
RF.stack2nd <- train(log.price ~ ., data = second.stack.set,method = "rf", trControl = fitControl, verbose = FALSE) 
#save the model for future use
saveRDS(RF.stack2nd,"RF.stack2nd.rds")

# Stop and De-register parallel processing cluster
stopCluster(cluster)
registerDoSEQ()

end.time <- Sys.time()
end.time - start.time
```

This time it took 34.23155 mins.

```{r}
# reload the saved model
RF.stack2nd <- readRDS("RF.stack2nd.rds")
log.rmse.training(pred = predict(RF.stack2nd,second.stack.set),observed = second.stack.set$log.price,method = "Random Forest on the second stack")
```

Note that it did not get better than the first stack, it sounds like we are not explaining any orthagonal information.


### Extreme gradient boosting from stacked predictors 

```{r}
start.time <- Sys.time()
setwd("~/Desktop/2016/Data_science/Kaggle/Kaggle-Mercari-Price-Suggestion-Challenge")
second.stack.set <- readRDS("second.stack.set.rds")

# Long operation in a standard computer!
library(caret)
library(parallel);library(doParallel)
# Configure parallel computation
cluster <- makeCluster(detectCores() - 1) # Leave one core for the OS
registerDoParallel(cluster)

# Configure trainControl object
fitControl <- trainControl(method = "cv", number = 5, allowParallel = TRUE)

#Train the model
set.seed(12345)
xGBM.stack2nd <- train(log.price ~ ., data = second.stack.set,method = "xgbLinear", trControl = fitControl, verbose = FALSE) 
#save the model for future use
saveRDS(xGBM.stack2nd,"xGBM.stack2nd.rds")

# Stop and De-register parallel processing cluster
stopCluster(cluster)
registerDoSEQ()

end.time <- Sys.time()
end.time - start.time
```


```{r}
# reload the saved model
xGBM.stack2nd <- readRDS("xGBM.stack2nd.rds")
log.rmse.training(pred = predict(xGBM.stack2nd,second.stack.set),observed = second.stack.set$log.price,method = "Extreme Gradient Boosting on the second stack")
```

# Trying BoxCox transformation in various ways

```{r, fig.width=10, fig.height= 10}
par(mfrow = c(2,2))
hist(exp(mini.subtrain$log.price), breaks = 50, col = "navy")
hist(mini.subtrain$log.price, breaks = 50, col = "navy")
hist(BoxCox(mini.subtrain$log.price,lambda = BoxCox.lambda(mini.subtrain$log.price)), breaks = 50, col = "navy")
hist(BoxCox(exp(mini.subtrain$log.price),lambda = BoxCox.lambda(exp(mini.subtrain$log.price))), breaks = 50, col = "navy")

```

## Stacking iteration 3:

Now, let's try something a little different, and see if it is indeed different. 

