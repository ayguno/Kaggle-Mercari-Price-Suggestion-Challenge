---
title: "KaggleMercariPriceSuggestionChallengeKernel"
author: "Ozan Aygun"
date: "12/15/2017"
output: 
   html_document:
        toc: true
        number_sections: true
        depth: 4
        theme: cerulean
        highlight: tango
        df_print: paged
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(results = "markup", fig.align = "center",
                      fig.width = 5, fig.height = 4,message=FALSE,warning=FALSE)
```


# Introduction and summary



# Loading the data

```{r, cache=TRUE}
# Need to download files from kaggle (links available only after login!)
# Need to get 'archive' package to make a connection to 7z zipped files!
# devtools::install_github("jimhester/archive")
#library(archive);library(readr)
#train <- data.frame(read_delim(archive_read("train.tsv.7z"), delim = "\t"))
#test <- data.frame(read_delim(archive_read("test.tsv.7z"), delim = "\t"))
#sample_submission <- data.frame(read_delim(archive_read("sample_submission.csv.7z"), delim = ","))
# 
# # Save the training and test sets for easy loading in future
#saveRDS(train, "train.rds");saveRDS(test,"test.rds")
# write.csv(sample_submission,"sample_submission.csv", row.names = FALSE)
unlink(dir(pattern = ".7z")) # Finally delete the 7z files from directory
```

# Summarizing the data
```{r,cache=TRUE}
# Read from the rds objects
train <- readRDS("train.rds") ; test <- readRDS("test.rds") 
summary(train)
summary(test)
```
We have a continuous outcome(price), so this is a regression problem. Most of the features appear strings. So we are likely to perform some text mining and feature engineering.

# Partitioning the training set

The training set we have is fairly large, so it will not be wasteful if we split this data set into two for sub-training and validation respectively. We will explore and train the model on the sub-training set, perform initial validation in the validation set, and submit predictions for the test set provided.

```{r,cache=TRUE}
library(caret)
set.seed(12345)
subtrainID <- createDataPartition(y = train$price, p = 0.5, list = FALSE)
subtrain <- train[subtrainID[,1],]
validation <- train[-subtrainID[,1],]
```

# Exploratory data analysis using sub-training set

For the ease of making plots, I will explore a randomly selected subset (5%) of the subtraining set, I will call this set as **mini.subtrain**. Note that even 10% of this data set contains ~37,000 observations!

```{r,cache= TRUE, fig.align= "center", fig.width=12}
set.seed(12345)
mini.subtrainID <- createDataPartition(y = subtrain$price, p = 0.05, list = FALSE)
mini.subtrain <- subtrain[mini.subtrainID[,1],]
```

Let's start looking at the data,

## Distribution of the outcome

```{r,cache=TRUE, fig.width=10}
par(mfrow = c(1,3))
hist(mini.subtrain$price, breaks = 50, col = "navy")
hist(log(mini.subtrain$price), breaks = 50, col = "navy")
```


## Checking for missing values
```{r, cache=TRUE}
apply(is.na(subtrain),2,sum)
```

It appears that most of the missing values are in the category name and brand name (half of the values are mising)

### Generic items are slightly cheaper

One obvious thing to look at is whether the price of items that are missing such features have significantly different prices:
```{r,cache=TRUE}
library(ggplot2)
ggplot(data = mini.subtrain, aes(y = log(price+1),fill =factor(is.na(category_name)), x = factor(is.na(category_name))))+
        geom_boxplot()

ggplot(data = mini.subtrain, aes(y = log(price+1),fill =factor(is.na(brand_name)), x = factor(is.na(brand_name))))+
        geom_boxplot()
```

It is perhaps expected the items with no brand name will be slightly cheaper. Therefore, this looks like a new feature to add into the data sets:

```{r,cache=TRUE}
library(dplyr)
mini.subtrain <- mutate(mini.subtrain,no.brand_name = ifelse(is.na(brand_name),1,0))
subtrain <- mutate(subtrain,no.brand_name = ifelse(is.na(brand_name),1,0))
validation <- mutate(validation,no.brand_name = ifelse(is.na(brand_name),1,0))
test <- mutate(test,no.brand_name = ifelse(is.na(brand_name),1,0))
```

###Uniqueness profile of features

```{r}
uniqueness <- apply(subtrain,2,function(x){return(length(unique(x)))})
names(uniqueness) <- colnames(subtrain)
uniqueness
```

Except 2 binary variables, and one potential categorical variable (item_condition_id), the rest of the features can be regarded as string, and we will create new features from them using text mining.

First, store the ID variables for all data sets and remove them:

```{r}
mini.subtrain.train_id <- mini.subtrain$train_id
mini.subtrain <- dplyr::select(mini.subtrain,-train_id)

subtrain.train_id <- subtrain$train_id
subtrain <- dplyr::select(subtrain,-train_id)

validation.train_id <- validation$train_id
validation <- dplyr::select(validation,-train_id)

test.test_id <- test$test_id
test <- dplyr::select(test,-test_id)
```

### item condition id in relation to shipping and having a brand name

```{r, fig.width=10, cache=TRUE}
ggplot(data = mini.subtrain, aes(y = log(price+1), x = factor(item_condition_id),
                                fill = factor(item_condition_id)))+
                                facet_grid(factor(shipping) ~ factor(no.brand_name))+
                                geom_boxplot()+
                                geom_jitter(alpha = 0.05,size = 0.2, color = "navy")
```

These features alone do not seem to explain too much variability in the price. We need stronger features!

###Text mining and feature engineering

#### Starting simple: special characters and length of the text features

Let's try to understand if there is any relationship between the outcome and the length of various textual features:

```{r,fig.width= 10, fig.height=10}
length.name <- sapply(mini.subtrain$name,nchar) 
length.category <- sapply(mini.subtrain$category_name,nchar)
length.brand <- sapply(mini.subtrain$brand_name,nchar)
length.description <- sapply(mini.subtrain$item_description,nchar)
pairs(log(mini.subtrain$price) ~ length.name + length.category + length.brand + log(length.description),
      pch = 19, col = "navy", cex = 0.2)
```

This pretty much looks like noise. Let's have a look at some special characters:

```{r,fig.width=10}
# Exclamation mark (!)
library(stringr)
excl.name <- str_count(mini.subtrain$name, "!")
excl.category <- str_count(mini.subtrain$category_name,"!")
excl.brand <- str_count(mini.subtrain$brand_name,"!")
excl.description <- str_count(mini.subtrain$item_description,"!")
pairs(log(mini.subtrain$price) ~ excl.name + excl.category + excl.brand + log(excl.description),
      pch = 19, col = "purple", cex = 0.2)

```

It would be worth looking further into the ecxl.name and excl.description:

```{r,fig.width=10}
ggplot(data = mini.subtrain, aes(y = log(price),x= excl.name))+
        geom_point(col = "purple", alpha = 0.4,size =0.9)+
        facet_grid(factor(shipping) ~ factor(item_condition_id))
```

What happens in excl.name X item_condition_id interaction?


```{r,fig.width=10}
ggplot(data = mini.subtrain, aes(y = log(price),x= excl.name * item_condition_id))+
        geom_point(col = "blue", alpha = 0.4,size =1)+
        facet_grid( . ~ factor(shipping))
```

How about excl.description?


```{r,fig.width=10}
ggplot(data = mini.subtrain, aes(y = log(price),x= log(excl.description)))+
        geom_point(col = "purple", alpha = 0.4,size =0.9)+
        facet_grid(factor(shipping) ~ factor(item_condition_id))
```

What happens in excl.name X item_condition_id interaction?


```{r,fig.width=10}
ggplot(data = mini.subtrain, aes(y = log(price),x= log(excl.description) * item_condition_id))+
        geom_point(col = "blue", alpha = 0.4,size =1)+
        facet_grid( . ~ factor(shipping))
```

My intuition is the interaction with item condition makes these two new features somewhat stronger. 

Let's add these two new features into the main data sets:

```{r}
mini.subtrain$log.excl.description <- log(excl.description + 1) * mini.subtrain$item_condition_id
mini.subtrain$excl.name <- excl.name * mini.subtrain$item_condition_id

subtrain$log.excl.description <- log(str_count(subtrain$item_description, "!") + 1) * subtrain$item_condition_id
subtrain$excl.name <- str_count(subtrain$name, "!") * subtrain$item_condition_id

validation$log.excl.description <- log(str_count(validation$item_description, "!") + 1) * validation$item_condition_id
validation$excl.name <- str_count(validation$name, "!") * validation$item_condition_id

test$log.excl.description <- log(str_count(test$item_description, "!") + 1) * test$item_condition_id
test$excl.name <- str_count(test$name, "!") * test$item_condition_id
```

How about any characters excluding letters, commas or numbers?

```{r,fig.width= 8}
special.character.description <- sapply(mini.subtrain$item_description,function(x){
return(nchar(str_trim(gsub("[a-zA-Z]|,|\\.|[0-9]","",x))))})
ggplot(data = mini.subtrain, aes(y = log(price),x= log(special.character.description+1)))+
        geom_point(col = "purple", alpha = 0.4,size =0.9)+
        facet_grid(factor(shipping) ~ factor(item_condition_id))       

```
The special character count in item description looks like just noise.


```{r,fig.width= 8}
special.character.name <- sapply(mini.subtrain$name,function(x){
return(nchar(str_trim(gsub("[a-zA-Z]|,|\\.|[0-9]","",x))))})
ggplot(data = mini.subtrain, aes(y = log(price),x= log(special.character.name+1)))+
        geom_point(col = "navy", alpha = 0.4,size =0.9)+
        facet_grid(factor(shipping) ~ factor(item_condition_id))       

```

What happens in special.character.name X item_condition_id interaction?


```{r,fig.width=10}
ggplot(data = mini.subtrain, aes(y = log(price),x= log(special.character.name+1) * item_condition_id))+
        geom_point(col = "blue", alpha = 0.4,size =1)+
        facet_grid( . ~ factor(shipping))
```

In the case of name, this looks like not a bad feature. 

```{r}
cor(log(special.character.name+1) * mini.subtrain$item_condition_id,mini.subtrain$excl.name)
```
Sounds like we are not adding a highly correlated feature,either.

How about just counting the stars (*) ?

```{r,fig.width= 10, fig.height=10}
star.name <- str_count(mini.subtrain$name,"\\*") 
star.description <- str_count(mini.subtrain$item_description,"\\*")
pairs(log(mini.subtrain$price) ~ log(star.name +1) + log(star.description+1),pch = 19, col = "navy", cex = 0.2)
```

How about just counting the text starting with stars (*) ?

```{r,fig.width= 5, }
start.name <- str_count(mini.subtrain$name,"^\\*") 
start.description <- str_count(mini.subtrain$item_description,"^\\*")
ggplot(mini.subtrain,aes(y = log(price+1), x = factor(start.name))) +
        geom_boxplot()
ggplot(mini.subtrain,aes(y = log(price+1), x = factor(start.description))) +
        geom_boxplot() 
```

These are not so impressive features. 

How about we just count the numbers?


```{r,fig.width= 10 }
number.name <- str_count(mini.subtrain$name,"[0-9]") 
number.description <- str_count(mini.subtrain$item_description,"[0-9]")
pairs(log(mini.subtrain$price) ~ log(number.name +1) + log(number.description+1),pch = 19, col = "navy", cex = 0.2)
```

How about we just count the $ sign?

```{r,fig.width= 10, }
dollar.name <- str_count(mini.subtrain$name,"\\$") 
dollar.description <- str_count(mini.subtrain$item_description,"\\$")
pairs(log(mini.subtrain$price) ~ log(dollar.name +1) + dollar.description,pch = 19, col = "navy", cex = 0.2)
```

dollar.description looks like a good feature!

```{r,fig.width= 8}
ggplot(data = mini.subtrain, aes(y = log(price),x= dollar.description))+
        geom_point(col = "navy", alpha = 0.4,size =0.9)+
        facet_grid(. ~ factor(shipping))       

```

Let's add dollar.description into our data sets:

```{r}
mini.subtrain$dollar.description <- str_count(mini.subtrain$item_description,"\\$")
subtrain$dollar.description <-  str_count(subtrain$item_description,"\\$")
validation$dollar.description <- str_count(validation$item_description,"\\$")
test$dollar.description <- str_count(test$item_description,"\\$")
```

####Category name

We noted that there are limited category names. Some of these categories may explain certain amount of variance in the price. Let's look at more closely:

```{r, fig.width=10}
summary.category_name <- mini.subtrain %>% group_by(category_name) %>% summarise(median.log.price = median(log(price)), size = n(), perct_fraction = n()/nrow(mini.subtrain)) %>% arrange(desc(median.log.price))
head(summary.category_name,10)
hist( summary.category_name$median.log.price, col ="navy", breaks = 50)
```
It looks like the median log prices binned by category names have a near-gaussian distribution, we have certain categries that could explain some of the most expensive and cheapest items. Let's define these categories:

```{r}
fancy.categories <- summary.category_name$category_name[summary.category_name$median.log.price > quantile(log(mini.subtrain$price),0.95)]
cheap.categories <- summary.category_name$category_name[summary.category_name$median.log.price < quantile(log(mini.subtrain$price),0.05)]

fancy.categories;cheap.categories
```

Let's make categorical features using these category names:
```{r}
mini.subtrain$fancy.categories <- sapply(mini.subtrain$category_name,function(x){
        if (any(x %in% fancy.categories))
                return(1)
        else
                return(0)
})

ggplot(mini.subtrain, aes(y = log(price), x = factor(fancy.categories)))+
        geom_jitter(alpha= 0.4, col ="green")+
        geom_boxplot(fill="navy")
```

```{r}
mini.subtrain$cheap.categories <- sapply(mini.subtrain$category_name,function(x){
        if (any(x %in% cheap.categories))
                return(1)
        else
                return(0)
})

ggplot(mini.subtrain, aes(y = log(price), x = factor(cheap.categories)))+
        geom_jitter(alpha= 0.4, col ="green")+
        geom_boxplot(fill="navy")
```

These could be some useful features to add into our data sets:

```{r}

subtrain$fancy.categories <- sapply(subtrain$category_name,function(x){
        if (any(x %in% fancy.categories))
                return(1)
        else
                return(0)
})

test$fancy.categories <- sapply(test$category_name,function(x){
        if (any(x %in% fancy.categories))
                return(1)
        else
                return(0)
})

validation$fancy.categories <- sapply(validation$category_name,function(x){
        if (any(x %in% fancy.categories))
                return(1)
        else
                return(0)
})



subtrain$cheap.categories <- sapply(subtrain$category_name,function(x){
        if (any(x %in% cheap.categories))
                return(1)
        else
                return(0)
})

test$cheap.categories <- sapply(test$category_name,function(x){
        if (any(x %in% cheap.categories))
                return(1)
        else
                return(0)
})

validation$cheap.categories <- sapply(validation$category_name,function(x){
        if (any(x %in% cheap.categories))
                return(1)
        else
                return(0)
})
```

#### Brand names

We also noted that there are limited brand names. Some of these may explain certain amount of variance in the price. Let's look at more closely:

```{r, fig.width=10}
summary.brand_name <- mini.subtrain %>% group_by(brand_name) %>% summarise(median.log.price = median(log(price)), size = n(), perct_fraction = n()/nrow(mini.subtrain)) %>% arrange(desc(median.log.price))
head(summary.brand_name,10)
hist( summary.brand_name$median.log.price, col ="navy", breaks = 50)
```

```{r}
fancy.brands <- summary.brand_name$brand_name[summary.brand_name$median.log.price > quantile(log(mini.subtrain$price),0.95)]
cheap.brands <- summary.brand_name$brand_name[summary.brand_name$median.log.price < quantile(log(mini.subtrain$price),0.05)]

fancy.brands;cheap.brands
```

Let's make categorical features using these brand names:
```{r}
mini.subtrain$fancy.brands <- sapply(mini.subtrain$brand_name,function(x){
        if (any(x %in% fancy.brands))
                return(1)
        else
                return(0)
})

ggplot(mini.subtrain, aes(y = log(price), x = factor(fancy.brands)))+
        geom_jitter(alpha= 0.4, col ="green")+
        geom_boxplot(fill="navy")
```

```{r}
mini.subtrain$cheap.brands <- sapply(mini.subtrain$brand_name,function(x){
        if (any(x %in% cheap.brands))
                return(1)
        else
                return(0)
})

ggplot(mini.subtrain, aes(y = log(price), x = factor(cheap.brands)))+
        geom_jitter(alpha= 0.4, col ="green")+
        geom_boxplot(fill="navy")
```

These could be some useful features to add into our data sets:

```{r}

subtrain$fancy.brands <- sapply(subtrain$brand_name,function(x){
        if (any(x %in% fancy.brands))
                return(1)
        else
                return(0)
})

test$fancy.brands <- sapply(test$brand_name,function(x){
        if (any(x %in% fancy.brands))
                return(1)
        else
                return(0)
})

validation$fancy.brands <- sapply(validation$brand_name,function(x){
        if (any(x %in% fancy.brands))
                return(1)
        else
                return(0)
})



subtrain$cheap.brands <- sapply(subtrain$brand_name,function(x){
        if (any(x %in% cheap.brands))
                return(1)
        else
                return(0)
})

test$cheap.brands <- sapply(test$brand_name,function(x){
        if (any(x %in% cheap.brands))
                return(1)
        else
                return(0)
})

validation$cheap.brands <- sapply(validation$brand_name,function(x){
        if (any(x %in% cheap.brands))
                return(1)
        else
                return(0)
})
```

#### Suspect marketing words

Here, lets look at some usual suspect marketing words in the item description:

```{r, fig.width=12, fig.height=12}
suspect <- data.frame(log.price = log(mini.subtrain$price+1))
suspect$sale <- str_count(tolower(mini.subtrain$item_description), "sale")
suspect$free <- str_count(tolower(mini.subtrain$item_description), "free")
suspect$save <- str_count(tolower(mini.subtrain$item_description), "save")
suspect$deal <- str_count(tolower(mini.subtrain$item_description), "deal")
suspect$good <- str_count(tolower(mini.subtrain$item_description), "good")
suspect$steal <- str_count(tolower(mini.subtrain$item_description), "steal")
suspect$now <- str_count(tolower(mini.subtrain$item_description), "now")
suspect$cheap <- str_count(tolower(mini.subtrain$item_description), "cheap")
suspect$buy <- str_count(tolower(mini.subtrain$item_description), "buy")
suspect$excellent <- str_count(tolower(mini.subtrain$item_description), "excellent")
suspect$great <- str_count(tolower(mini.subtrain$item_description), "great")

pairs(log.price ~ .,pch = 19, col = "navy", cex = 0.2, data = suspect)
```

Some of these features can be useful. Let's look at if there is any correlation between them and the outcome:

```{r, fig.width=10,fig.height=10}
library(corrplot)
corrplot(cor(suspect), method = c("shade"), bg = "gray", addgrid.col = "gray")
```

They don't appear to be highly correlated. What happens if we put them into a linear model along with the response variable:

```{r,fig.width=7,fig.height=7}
lmfit <- lm(log.price ~ . , data = suspect)
summary(lmfit)
par(mfrow = c(2,2))
plot(lmfit)
```

Even though the model is not perfect, some of these features could be useful to explain variability. I will add them into our data sets:

```{r}

# Write a function to consistently add the features into data sets:

suspect.marketing.feature.engineer <- function(data.set){

        suspect <- data.frame(dummy= 1:nrow(data.set))
        suspect$sale <- str_count(tolower(data.set$item_description), "sale")
        suspect$free <- str_count(tolower(data.set$item_description), "free")
        suspect$save <- str_count(tolower(data.set$item_description), "save")
        suspect$deal <- str_count(tolower(data.set$item_description), "deal")
        suspect$good <- str_count(tolower(data.set$item_description), "good")
        suspect$steal <- str_count(tolower(data.set$item_description), "steal")
        suspect$now <- str_count(tolower(data.set$item_description), "now")
        suspect$cheap <- str_count(tolower(data.set$item_description), "cheap")
        suspect$buy <- str_count(tolower(data.set$item_description), "buy")
        suspect$excellent <- str_count(tolower(data.set$item_description), "excellent")
        suspect$great <- str_count(tolower(data.set$item_description), "great")
        suspect <- suspect[,-1]
        data.set <- cbind(data.set,suspect)
        
        return(data.set)

}

mini.subtrain <- suspect.marketing.feature.engineer(mini.subtrain)
subtrain <- suspect.marketing.feature.engineer(subtrain)
validation <- suspect.marketing.feature.engineer(validation)
test <- suspect.marketing.feature.engineer(test)
```



####Looking for 'free' or 'unpriced' items v.s. most expensive items

Our first suspect would be there are certain keywords that are frequently associated with free items, which could be a good feature to predict these items:
```{r,cache=TRUE}
sum(subtrain$price == 0)
```

There are about 400 free items in the subtraining set.

```{r}
library(tm); library(SnowballC);library(wordcloud)

# Gratefully learned from:
# http://www.sthda.com/english/wiki/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know

text <- subtrain$item_description[subtrain$price == 0]
docs <- Corpus(VectorSource(text))

######################
# Text transformation
######################
# Transformation is performed using tm_map() function to replace, 
# for example, special characters from the text.

#Replacing “/”, “@” and “|” with space:
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")

####################
# Cleaning the text
####################

# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector if needed
# docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
docs <- tm_map(docs, stemDocument)

################################
# Build a term-document matrix
################################

# Document matrix is a table containing the frequency of the words. Column names are words and row names are documents. The function TermDocumentMatrix() from text mining package can be used as follow :

dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)

###########################
#Generate the Word cloud
###########################

# The importance of words can be illustrated as a word cloud as follow :

set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

# Arguments of the word cloud generator function :
# words : the words to be plotted
# freq : their frequencies
# min.freq : words with frequency below min.freq will not be plotted
# max.words : maximum number of words to be plotted
# random.order : plot words in random order. If false, they will be plotted in decreasing frequency
# rot.per : proportion words with 90 degree rotation (vertical text)
# colors : color words from least to most frequent. Use, for example, colors =“black” for single color.

#################################################
#Explore frequent terms and their associations
#################################################

# You can have a look at the frequent terms in the term-document matrix as follow. In the example below we want to find words that occur at least 50 times :

findFreqTerms(dtm, lowfreq = 50)

# You can analyze the association between frequent terms (i.e., terms which correlate) using findAssocs() function. The R code below identifies which words are associated with “size” in the current text :

findAssocs(dtm, terms = "size", corlimit = 0.3)
# This makes good sense!

########################
#Plot word frequencies
#######################

# The frequency of the first 10 frequent words are plotted :

barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col ="navy", main ="Most frequent words",
        xlab = "Word frequencies", horiz = T)
```


How about we look at the top 1% most expensive items?

```{r}
text <- subtrain$item_description[subtrain$price >= quantile(subtrain$price,0.99)]
docs <- Corpus(VectorSource(text))

######################
# Text transformation
######################
# Transformation is performed using tm_map() function to replace, 
# for example, special characters from the text.

#Replacing “/”, “@” and “|” with space:
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")

####################
# Cleaning the text
####################

# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector if needed
# docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
docs <- tm_map(docs, stemDocument)

################################
# Build a term-document matrix
################################

# Document matrix is a table containing the frequency of the words. Column names are words and row names are documents. The function TermDocumentMatrix() from text mining package can be used as follow :

dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)

###########################
#Generate the Word cloud
###########################

# The importance of words can be illustrated as a word cloud as follow :

set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

# Arguments of the word cloud generator function :
# words : the words to be plotted
# freq : their frequencies
# min.freq : words with frequency below min.freq will not be plotted
# max.words : maximum number of words to be plotted
# random.order : plot words in random order. If false, they will be plotted in decreasing frequency
# rot.per : proportion words with 90 degree rotation (vertical text)
# colors : color words from least to most frequent. Use, for example, colors =“black” for single color.

#################################################
#Explore frequent terms and their associations
#################################################

# You can have a look at the frequent terms in the term-document matrix as follow. In the example below we want to find words that occur at least 500 times :

findFreqTerms(dtm, lowfreq = 500)

# You can analyze the association between frequent terms (i.e., terms which correlate) using findAssocs() function. The R code below identifies which words are associated with “size” in the current text :

findAssocs(dtm, terms = "size", corlimit = 0.3)
# This makes good sense!

########################
#Plot word frequencies
#######################

# The frequency of the first 10 frequent words are plotted :

barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col ="navy", main ="Most frequent words",
        xlab = "Word frequencies", horiz = T)
```

Now let's look at the words in free and expensive items together:

```{r}
# Write a function to streamline text mininig

parse_text <- function(text){
        docs <- Corpus(VectorSource(text))

        ######################
        # Text transformation
        ######################
        # Transformation is performed using tm_map() function to replace, 
        # for example, special characters from the text.
        
        #Replacing “/”, “@” and “|” with space:
        toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
        docs <- tm_map(docs, toSpace, "/")
        docs <- tm_map(docs, toSpace, "@")
        docs <- tm_map(docs, toSpace, "\\|")
        
        ####################
        # Cleaning the text
        ####################
        
        # Convert the text to lower case
        docs <- tm_map(docs, content_transformer(tolower))
        # Remove numbers
        docs <- tm_map(docs, removeNumbers)
        # Remove english common stopwords
        docs <- tm_map(docs, removeWords, stopwords("english"))
        # Remove your own stop word
        # specify your stopwords as a character vector if needed
        # docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
        # Remove punctuations
        docs <- tm_map(docs, removePunctuation)
        # Eliminate extra white spaces
        docs <- tm_map(docs, stripWhitespace)
        # Text stemming
        docs <- tm_map(docs, stemDocument)
        
        ################################
        # Build a term-document matrix
        ################################
        
        # Document matrix is a table containing the frequency of the words. Column names are words and row names are documents. The function TermDocumentMatrix() from text mining package can be used as follow :
        
        dtm <- TermDocumentMatrix(docs)
        m <- as.matrix(dtm)
        v <- sort(rowSums(m),decreasing=TRUE)
        d <- data.frame(Word = as.character(names(v)),freq=v, Freq = (v/sum(v)) * 100)
        
        return(d)
}
```


```{r, fig.width=12, fig.height=7}
sorted.table.free <- parse_text(text = mini.subtrain$item_description[mini.subtrain$price == 0])
sorted.table.expensive <- parse_text(text = subtrain$item_description[subtrain$price >= quantile(subtrain$price,0.99)])

# Get top 100 most frequent words from each group 
sorted.table.free.100 <- head(sorted.table.free,100)
sorted.table.expensive.100 <- head(sorted.table.expensive,100)

merged.table <- merge(sorted.table.free.100,sorted.table.expensive.100, by = c("Word"), all = TRUE,
                      suffixes = c("free","expensive"))
row.names(merged.table) = merged.table$Word
merged.table <- as.matrix(merged.table[,which(grepl("^Freq",colnames(merged.table)))])
#Replace NA's with zero
merged.table[is.na(merged.table[,1]),1] <- 0
merged.table[is.na(merged.table[,2]),2] <- 0
library(pheatmap)
pheatmap(t(merged.table))
```

There could be some potential to seperate price segments from each other by using the frequencies of different words. Let's perform a more extensive segmentation analysis by writing a feature creation algorithm.

####Feature engineering in item description through price segmentation

```{r,eval=FALSE}
####################
# Long operation! 
####################

# Divide the subtraining set into 19 log(price) quantiles + 1 "free" item bin
# Totally 19 segments
percentiles <- seq(0.05, 0.99, 0.05)
merged.table <- parse_text(mini.subtrain$item_description[mini.subtrain$price == 0])[,c(1,3)]
colnames(merged.table)[2] <- paste0("price_percentile_0")
merged.table <- rbind(head(merged.table,10),tail(merged.table,10))
mini.subtrain.nonzero <- mini.subtrain[mini.subtrain$price != 0,]

for(i in seq_along(percentiles)){
        # Perform text frequency mining within the first segment and hold the results
        text <- mini.subtrain.nonzero$item_description[log(mini.subtrain.nonzero$price) >= quantile(log(mini.subtrain.nonzero$price),percentiles[i])]
        sorted.table <- parse_text(text)
        sorted.table <- sorted.table[,c(1,3)]
        colnames(sorted.table)[2] <- paste0("price_percentile_%",percentiles[i]*100)
        sorted.table <- rbind(head(sorted.table,10),tail(sorted.table,10))
        merged.table <- merge(merged.table,sorted.table, by = "Word", all = TRUE)
        # continue until all price segments are finished
} 


merged.table <- merged.table[!is.na(merged.table$Word),]

row.names(merged.table) = merged.table$Word
merged.table <- dplyr::select(merged.table, -Word)
merged.table <- as.matrix(merged.table)

#Replace NA's with zero
for (i in 1:20){merged.table[is.na(merged.table[,i]),i] <- 0}
#Save for future use
saveRDS(merged.table,"merged_table.rds")
```

```{r,fig.width=10, fig.height= 7}
# easy load
merged.table <- readRDS("merged_table.rds")
pheatmap(t.merged.table, cluster_rows = FALSE)
```

The item description text mining this way pretty much brings us noise.

####Feature engineering in brand name through price segmentation

```{r,eval=FALSE}
####################
# Long operation! 
####################

# Divide the subtraining set into 19 log(price) quantiles + 1 "free" item bin
# Totally 19 segments
percentiles <- seq(0.05, 0.99, 0.05)
merged.table <- parse_text(mini.subtrain$brand_name[mini.subtrain$price == 0])[,c(1,3)]
colnames(merged.table)[2] <- paste0("price_percentile_0")
merged.table <- rbind(head(merged.table,10),tail(merged.table,10))

mini.subtrain.nonzero <- mini.subtrain[mini.subtrain$price != 0,]

for(i in seq_along(percentiles)){
        # Perform text frequency mining within the first segment and hold the results
        text <- mini.subtrain.nonzero$brand_name[log(mini.subtrain.nonzero$price) >= quantile(log(mini.subtrain.nonzero$price),percentiles[i])]
        sorted.table <- parse_text(text)
        sorted.table <- sorted.table[,c(1,3)]
        colnames(sorted.table)[2] <- paste0("price_percentile_%",percentiles[i]*100)
        sorted.table <- rbind(head(sorted.table,10),tail(sorted.table,10))
        merged.table <- merge(merged.table,sorted.table, by = "Word", all = TRUE)
        # continue until all price segments are finished
} 


merged.table <- merged.table[!is.na(merged.table$Word),]
merged.table <- unique.data.frame(merged.table)

row.names(merged.table) = merged.table$Word
merged.table <- dplyr::select(merged.table, -Word)
merged.table <- as.matrix(merged.table)

#Replace NA's with zero
for (i in 1:20){merged.table[is.na(merged.table[,i]),i] <- 0}
#Save for future use
saveRDS(merged.table,"merged_table.rds")
```

```{r,fig.width=10, fig.height= 7}
# easy load
merged.table <- readRDS("merged_table.rds")
pheatmap(t(merged.table), cluster_rows = FALSE, scale = "row")
```

A slightly different version of the algorithm is just to compare Top50 most frequent words in brand names:


```{r,eval=FALSE}
####################
# Long operation! 
####################

# Divide the subtraining set into 19 log(price) quantiles + 1 "free" item bin
# Totally 19 segments
percentiles <- seq(0.05, 0.99, 0.05)
merged.table <- parse_text(mini.subtrain$brand_name[mini.subtrain$price == 0])[,c(1,3)]
colnames(merged.table)[2] <- paste0("price_percentile_0")
merged.table <- head(merged.table,50)

mini.subtrain.nonzero <- mini.subtrain[mini.subtrain$price != 0,]

for(i in seq_along(percentiles)){
        # Perform text frequency mining within the first segment and hold the results
        text <- mini.subtrain.nonzero$brand_name[log(mini.subtrain.nonzero$price) >= quantile(log(mini.subtrain.nonzero$price),percentiles[i])]
        sorted.table <- parse_text(text)
        sorted.table <- sorted.table[,c(1,3)]
        colnames(sorted.table)[2] <- paste0("price_percentile_%",percentiles[i]*100)
        sorted.table <- head(sorted.table,50)
        merged.table <- merge(merged.table,sorted.table, by = "Word", all = TRUE)
        # continue until all price segments are finished
} 


merged.table <- merged.table[!is.na(merged.table$Word),]
merged.table <- unique.data.frame(merged.table)

row.names(merged.table) = merged.table$Word
merged.table <- dplyr::select(merged.table, -Word)
merged.table <- as.matrix(merged.table)

#Replace NA's with zero
for (i in 1:20){merged.table[is.na(merged.table[,i]),i] <- 0}
#Save for future use
saveRDS(merged.table,"merged_table.rds")
```

```{r,fig.width=10, fig.height= 7}
# easy load
merged.table <- readRDS("merged_table.rds")
pheatmap(t(merged.table), cluster_rows = F, scale = "row")
```

Aha! Here we notice a few things that can be potentially interesting. 

- appl : enriched in most expensive items
- victoria, secret, lululemon,lularo, pink, michael, kor: these are words that can potentially form gradients across the price segments.

Let's engineer new features to add them into our data sets:

```{r}
# Write a function to streamline feature engineering
word.feature.engineer <- function(text.vector,pattern){
        #returns a vector that gives the counts of given pattern in each of the element of the text vector  
        text.vector <- tolower(text.vector)
        counts <- 0
        for(i in seq_along(text.vector)){
                counts[i] <- str_count(text.vector[i],pattern)
        }
        counts[is.na(counts)] <- 0
        return(counts)
}

victoria.test <- word.feature.engineer(mini.subtrain$brand_name,"victoria")
secret.test <- word.feature.engineer(mini.subtrain$brand_name,"secret")
cor(victoria.test,secret.test) # These two words are tighly associated
```

```{r}
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(victoria.test)))+
        geom_boxplot(fill = c("navy","red"))
```

This doesn't seem to be a fascinating feature.

```{r}
pink.test <- word.feature.engineer(mini.subtrain$brand_name,"pink")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(pink.test)))+
        geom_boxplot(fill = c("navy","red"))
```

```{r}
appl.test <- word.feature.engineer(mini.subtrain$brand_name,"appl")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(appl.test)))+
        geom_boxplot(fill = c("navy","red"))
```
```{r}
michael.test <- word.feature.engineer(mini.subtrain$brand_name,"michael")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(michael.test)))+
        geom_boxplot(fill = c("navy","red"))
```

michael in brandname looks like a useful feature. Let's add into our data sets:

```{r}
mini.subtrain$michael.brand <- word.feature.engineer(mini.subtrain$brand_name,"michael")
subtrain$michael.brand <- word.feature.engineer(subtrain$brand_name,"michael")
validation$michael.brand <- word.feature.engineer(validation$brand_name,"michael")
test$michael.brand <- word.feature.engineer(test$brand_name,"michael")
```


####Feature engineering in category name through price segmentation




```{r, eval=FALSE}
# # Long operation! 
# 
# # Divide the subtrain data into 10 quantiles after setting aside the free items
# subtrain.nonzero <- subtrain[subtrain$price != 0,]
# sorted.table.free.50 <- sorted.table.free[order(sorted.table.free$Freq, decreasing = TRUE),][1:50,]
# merged.table <- sorted.table.free.50[,c(1,3)]
# colnames(merged.table)[2] <- paste0("price_percentile_0")
# # Totally 21 segments
# percentiles <- seq(0.05, 0.99, 0.1)
# for(i in seq_along(percentiles)){
# # For each segment, calculate word frequencies in the item description, get top 50 most frequent words
#         v <- subtrain.nonzero$item_description[subtrain.nonzero$price >= quantile(subtrain.nonzero$price,percentiles[i])]
#         write.table(v,"v.txt")
#         c1 <- scan("v.txt", what = "character", sep = "\n")
#         c2 <- tolower(c1)
#         c3 <- unlist(strsplit(c2, "\\W")) # \\W regex for non-word character
#         c3 <- c3[-(which(c3 == ""))]
#         freq <- table(c3)
#         freq1 <- sort(freq, decreasing = TRUE)
#         freq2 <- (freq1/sum(freq1))*100 # % frequency of the word amonst all words in the section
#         sorted.table <- data.frame(Word = names(freq1),freq2)
#         sorted.table <- sorted.table[order(sorted.table$Freq, decreasing = TRUE),][1:50,]
#         sorted.table <- sorted.table[,c(1,3)]
#         colnames(sorted.table)[2] <- paste0("price_percentile_%",percentiles[i]*100)
# # Merge into a data frame
#         merged.table <- merge(merged.table,sorted.table, by = c("Word"), all = TRUE)
# }
```

```{r, fig.width=10, fig.height= 7}
#Save for future use
# saveRDS(merged.table,"merged_table.rds")
# easy load
merged.table <- readRDS("merged_table.rds")

merged.table <- merged.table[!is.na(merged.table$Word),]

row.names(merged.table) = merged.table$Word
merged.table <- dplyr::select(merged.table, -Word)
merged.table <- as.matrix(merged.table)

#Replace NA's with zero
for (i in 1:11){merged.table[is.na(merged.table[,i]),i] <- 0}

# Cluster into a heatmap
pheatmap(t(merged.table),scale = "row", cluster_rows = FALSE)
```