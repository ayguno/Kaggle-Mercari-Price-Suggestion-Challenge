---
title: "KaggleMercariPriceSuggestionChallengeKernel"
author: "Ozan Aygun"
date: "12/15/2017"
output: 
   html_document:
        toc: true
        number_sections: true
        depth: 4
        theme: cerulean
        highlight: tango
        df_print: paged
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(results = "markup", fig.align = "center",
                      fig.width = 5, fig.height = 4,message=FALSE,warning=FALSE)
```


# Introduction and summary



# Loading the data

```{r, cache=TRUE}
# Need to download files from kaggle (links available only after login!)
# Need to get 'archive' package to make a connection to 7z zipped files!
# devtools::install_github("jimhester/archive")
#library(archive);library(readr)
#train <- data.frame(read_delim(archive_read("train.tsv.7z"), delim = "\t"))
#test <- data.frame(read_delim(archive_read("test.tsv.7z"), delim = "\t"))
#sample_submission <- data.frame(read_delim(archive_read("sample_submission.csv.7z"), delim = ","))
# 
# # Save the training and test sets for easy loading in future
#saveRDS(train, "train.rds");saveRDS(test,"test.rds")
# write.csv(sample_submission,"sample_submission.csv", row.names = FALSE)
unlink(dir(pattern = ".7z")) # Finally delete the 7z files from directory
```

# Summarizing the data
```{r,cache=TRUE}
# Read from the rds objects
train <- readRDS("train.rds") ; test <- readRDS("test.rds") 
summary(train)
summary(test)
```
We have a continuous outcome(price), so this is a regression problem. Most of the features appear strings. So we are likely to perform some text mining and feature engineering.

# Partitioning the training set

The training set we have is fairly large, so it will not be wasteful if we split this data set into two for sub-training and validation respectively. We will explore and train the model on the sub-training set, perform initial validation in the validation set, and submit predictions for the test set provided.

```{r,cache=TRUE}
library(caret)
set.seed(12345)
subtrainID <- createDataPartition(y = train$price, p = 0.5, list = FALSE)
subtrain <- train[subtrainID[,1],]
validation <- train[-subtrainID[,1],]
```

# Exploratory data analysis using sub-training set

For the ease of making plots, I will explore a randomly selected subset (5%) of the subtraining set, I will call this set as **mini.subtrain**. Note that even 10% of this data set contains ~37,000 observations!

```{r,cache= TRUE, fig.align= "center", fig.width=12}
set.seed(12345)
mini.subtrainID <- createDataPartition(y = subtrain$price, p = 0.05, list = FALSE)
mini.subtrain <- subtrain[mini.subtrainID[,1],]
```

Let's start looking at the data,

## Distribution of the outcome

```{r,cache=TRUE, fig.width=10}
par(mfrow = c(1,3))
hist(mini.subtrain$price, breaks = 50, col = "navy")
hist(log(mini.subtrain$price), breaks = 50, col = "navy")
```


## Checking for missing values
```{r, cache=TRUE}
apply(is.na(subtrain),2,sum)
```

It appears that most of the missing values are in the category name and brand name (half of the values are mising)

### Generic items are slightly cheaper

One obvious thing to look at is whether the price of items that are missing such features have significantly different prices:
```{r,cache=TRUE}
library(ggplot2)
ggplot(data = mini.subtrain, aes(y = log(price+1),fill =factor(is.na(category_name)), x = factor(is.na(category_name))))+
        geom_boxplot()

ggplot(data = mini.subtrain, aes(y = log(price+1),fill =factor(is.na(brand_name)), x = factor(is.na(brand_name))))+
        geom_boxplot()
```

It is perhaps expected the items with no brand name will be slightly cheaper. Therefore, this looks like a new feature to add into the data sets:

```{r,cache=TRUE}
library(dplyr)
mini.subtrain <- mutate(mini.subtrain,no.brand_name = ifelse(is.na(brand_name),1,0))
subtrain <- mutate(subtrain,no.brand_name = ifelse(is.na(brand_name),1,0))
validation <- mutate(validation,no.brand_name = ifelse(is.na(brand_name),1,0))
test <- mutate(test,no.brand_name = ifelse(is.na(brand_name),1,0))
```

###Uniqueness profile of features

```{r}
uniqueness <- apply(subtrain,2,function(x){return(length(unique(x)))})
names(uniqueness) <- colnames(subtrain)
uniqueness
```

Except 2 binary variables, and one potential categorical variable (item_condition_id), the rest of the features can be regarded as string, and we will create new features from them using text mining.

First, store the ID variables for all data sets and remove them:

```{r}
mini.subtrain.train_id <- mini.subtrain$train_id
mini.subtrain <- dplyr::select(mini.subtrain,-train_id)

subtrain.train_id <- subtrain$train_id
subtrain <- dplyr::select(subtrain,-train_id)

validation.train_id <- validation$train_id
validation <- dplyr::select(validation,-train_id)

test.test_id <- test$test_id
test <- dplyr::select(test,-test_id)
```

### item condition id in relation to shipping and having a brand name

```{r, fig.width=10, cache=TRUE}
ggplot(data = mini.subtrain, aes(y = log(price+1), x = factor(item_condition_id),
                                fill = factor(item_condition_id)))+
                                facet_grid(factor(shipping) ~ factor(no.brand_name))+
                                geom_boxplot()+
                                geom_jitter(alpha = 0.05,size = 0.2, color = "navy")
```

These features alone do not seem to explain too much variability in the price. We need stronger features!

###Text mining and feature engineering

####Looking for 'free' or 'unpriced' items v.s. most expensive items

Our first suspect would be there are certain keywords that are frequently associated with free items, which could be a good feature to predict these items:
```{r,cache=TRUE}
sum(subtrain$price == 0)
```

There are about 400 free items in the subtraining set.

```{r}
v <- subtrain$item_description[subtrain$price == 0]
write.table(v,"v.txt")
c1 <- scan("v.txt", what = "character", sep = "\n")
c2 <- tolower(c1)
c3 <- unlist(strsplit(c2, "\\W"))
c3 <- c3[-(which(c3 == ""))]
freq <- table(c3)
freq1 <- sort(freq, decreasing = TRUE)
sorted.table <- data.frame(Word = names(freq1),freq1)
ggplot(data = sorted.table[1:20,])+
    geom_point(aes(x = Freq, y = reorder(Word,Freq),size = 0.3), color = "navy")
        
```

Some of the text we pick here are probably things that are not correctly interpreted. They form the most frequent text.

How about we look at the top 5% most expensive items?

```{r,cache=TRUE}
v <- subtrain$item_description[subtrain$price >= quantile(subtrain$price,0.95)]
write.table(v,"v.txt")
c1 <- scan("v.txt", what = "character", sep = "\n")
c2 <- tolower(c1)
c3 <- unlist(strsplit(c2, "\\W"))
c3 <- c3[-(which(c3 == ""))]
freq <- table(c3)
freq1 <- sort(freq, decreasing = TRUE)
sorted.table <- data.frame(Word = names(freq1),freq1)
ggplot(data = sorted.table[1:20,])+
    geom_point(aes(x = Freq, y = reorder(Word,Freq),size = 0.3), color = "navy")
```

