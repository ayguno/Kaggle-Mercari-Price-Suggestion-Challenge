---
title: "KaggleMercariPriceSuggestionChallengeKernel"
author: "Ozan Aygun"
date: "12/15/2017"
output: 
   html_document:
        toc: true
        number_sections: true
        depth: 4
        theme: cerulean
        highlight: tango
        df_print: paged
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(results = "markup", fig.align = "center",
                      fig.width = 5, fig.height = 4,message=FALSE,warning=FALSE)
```


# Introduction and summary



# Loading the data

```{r, cache=TRUE}
# Need to download files from kaggle (links available only after login!)
# Need to get 'archive' package to make a connection to 7z zipped files!
# devtools::install_github("jimhester/archive")
#library(archive);library(readr)
#train <- data.frame(read_delim(archive_read("train.tsv.7z"), delim = "\t"))
#test <- data.frame(read_delim(archive_read("test.tsv.7z"), delim = "\t"))
#sample_submission <- data.frame(read_delim(archive_read("sample_submission.csv.7z"), delim = ","))
# 
# # Save the training and test sets for easy loading in future
#saveRDS(train, "train.rds");saveRDS(test,"test.rds")
# write.csv(sample_submission,"sample_submission.csv", row.names = FALSE)
unlink(dir(pattern = ".7z")) # Finally delete the 7z files from directory
```

# Summarizing the data
```{r,cache=TRUE}
# Read from the rds objects
train <- readRDS("train.rds") ; test <- readRDS("test.rds") 
summary(train)
summary(test)
```
We have a continuous outcome(price), so this is a regression problem. Most of the features appear strings. So we are likely to perform some text mining and feature engineering.

# Partitioning the training set

The training set we have is fairly large, so it will not be wasteful if we split this data set into two for sub-training and validation respectively. We will explore and train the model on the sub-training set, perform initial validation in the validation set, and submit predictions for the test set provided.

```{r,cache=TRUE}
library(caret)
set.seed(12345)
subtrainID <- createDataPartition(y = train$price, p = 0.5, list = FALSE)
subtrain <- train[subtrainID[,1],]
validation <- train[-subtrainID[,1],]
```

# Exploratory data analysis using sub-training set

For the ease of making plots, I will explore a randomly selected subset (5%) of the subtraining set, I will call this set as **mini.subtrain**. Note that even 10% of this data set contains ~37,000 observations!

```{r,cache= TRUE, fig.align= "center", fig.width=12}
set.seed(12345)
mini.subtrainID <- createDataPartition(y = subtrain$price, p = 0.05, list = FALSE)
mini.subtrain <- subtrain[mini.subtrainID[,1],]
```

Let's start looking at the data,

## Distribution of the outcome

```{r,cache=TRUE, fig.width=10}
par(mfrow = c(1,3))
hist(mini.subtrain$price, breaks = 50, col = "navy")
hist(log(mini.subtrain$price), breaks = 50, col = "navy")
```


## Checking for missing values
```{r, cache=TRUE}
apply(is.na(subtrain),2,sum)
```

It appears that most of the missing values are in the category name and brand name (half of the values are mising)

### Generic items are slightly cheaper

One obvious thing to look at is whether the price of items that are missing such features have significantly different prices:
```{r,cache=TRUE}
library(ggplot2)
ggplot(data = mini.subtrain, aes(y = log(price+1),fill =factor(is.na(category_name)), x = factor(is.na(category_name))))+
        geom_boxplot()

ggplot(data = mini.subtrain, aes(y = log(price+1),fill =factor(is.na(brand_name)), x = factor(is.na(brand_name))))+
        geom_boxplot()
```

It is perhaps expected the items with no brand name will be slightly cheaper. Therefore, this looks like a new feature to add into the data sets:

```{r,cache=TRUE}
library(dplyr)
mini.subtrain <- mutate(mini.subtrain,no.brand_name = ifelse(is.na(brand_name),1,0))
subtrain <- mutate(subtrain,no.brand_name = ifelse(is.na(brand_name),1,0))
validation <- mutate(validation,no.brand_name = ifelse(is.na(brand_name),1,0))
test <- mutate(test,no.brand_name = ifelse(is.na(brand_name),1,0))
```

###Uniqueness profile of features

```{r}
uniqueness <- apply(subtrain,2,function(x){return(length(unique(x)))})
names(uniqueness) <- colnames(subtrain)
uniqueness
```

Except 2 binary variables, and one potential categorical variable (item_condition_id), the rest of the features can be regarded as string, and we will create new features from them using text mining.

First, store the ID variables for all data sets and remove them:

```{r}
mini.subtrain.train_id <- mini.subtrain$train_id
mini.subtrain <- dplyr::select(mini.subtrain,-train_id)

subtrain.train_id <- subtrain$train_id
subtrain <- dplyr::select(subtrain,-train_id)

validation.train_id <- validation$train_id
validation <- dplyr::select(validation,-train_id)

test.test_id <- test$test_id
test <- dplyr::select(test,-test_id)
```

### item condition id in relation to shipping and having a brand name

```{r, fig.width=10, cache=TRUE}
ggplot(data = mini.subtrain, aes(y = log(price+1), x = factor(item_condition_id),
                                fill = factor(item_condition_id)))+
                                facet_grid(factor(shipping) ~ factor(no.brand_name))+
                                geom_boxplot()+
                                geom_jitter(alpha = 0.05,size = 0.2, color = "navy")
```

These features alone do not seem to explain too much variability in the price. We need stronger features!

###Text mining and feature engineering

#### Starting simple: special characters and length of the text features

Let's try to understand if there is any relationship between the outcome and the length of various textual features:

```{r,fig.width= 10, fig.height=10}
length.name <- sapply(mini.subtrain$name,nchar) 
length.category <- sapply(mini.subtrain$category_name,nchar)
length.brand <- sapply(mini.subtrain$brand_name,nchar)
length.description <- sapply(mini.subtrain$item_description,nchar)
pairs(log(mini.subtrain$price) ~ length.name + length.category + length.brand + log(length.description),
      pch = 19, col = "navy", cex = 0.2)
```

This pretty much looks like noise. Let's have a look at some special characters:

```{r,fig.width=10}
# Exclamation mark (!)
library(stringr)
excl.name <- str_count(mini.subtrain$name, "!")
excl.category <- str_count(mini.subtrain$category_name,"!")
excl.brand <- str_count(mini.subtrain$brand_name,"!")
excl.description <- str_count(mini.subtrain$item_description,"!")
pairs(log(mini.subtrain$price) ~ excl.name + excl.category + excl.brand + log(excl.description),
      pch = 19, col = "purple", cex = 0.2)

```

It would be worth looking further into the ecxl.name and excl.description:

```{r,fig.width=10}
ggplot(data = mini.subtrain, aes(y = log(price),x= excl.name))+
        geom_point(col = "purple", alpha = 0.4,size =0.9)+
        facet_grid(factor(shipping) ~ factor(item_condition_id))
```

What happens in excl.name X item_condition_id interaction?


```{r,fig.width=10}
ggplot(data = mini.subtrain, aes(y = log(price),x= excl.name * item_condition_id))+
        geom_point(col = "blue", alpha = 0.4,size =1)+
        facet_grid( . ~ factor(shipping))
```

How about excl.description?


```{r,fig.width=10}
ggplot(data = mini.subtrain, aes(y = log(price),x= log(excl.description)))+
        geom_point(col = "purple", alpha = 0.4,size =0.9)+
        facet_grid(factor(shipping) ~ factor(item_condition_id))
```

What happens in excl.name X item_condition_id interaction?


```{r,fig.width=10}
ggplot(data = mini.subtrain, aes(y = log(price),x= log(excl.description) * item_condition_id))+
        geom_point(col = "blue", alpha = 0.4,size =1)+
        facet_grid( . ~ factor(shipping))
```

My intuition is the interaction with item condition makes these two new features somewhat stronger. 

Let's add these two new features into the main data sets:

```{r}
mini.subtrain$log.excl.description <- log(excl.description + 1) * mini.subtrain$item_condition_id
mini.subtrain$excl.name <- excl.name * mini.subtrain$item_condition_id

subtrain$log.excl.description <- log(str_count(subtrain$item_description, "!") + 1) * subtrain$item_condition_id
subtrain$excl.name <- str_count(subtrain$name, "!") * subtrain$item_condition_id

validation$log.excl.description <- log(str_count(validation$item_description, "!") + 1) * validation$item_condition_id
validation$excl.name <- str_count(validation$name, "!") * validation$item_condition_id

test$log.excl.description <- log(str_count(test$item_description, "!") + 1) * test$item_condition_id
test$excl.name <- str_count(test$name, "!") * test$item_condition_id
```

How about any characters excluding letters, commas or numbers?

```{r,fig.width= 8}
special.character.description <- sapply(mini.subtrain$item_description,function(x){
return(nchar(str_trim(gsub("[a-zA-Z]|,|\\.|[0-9]","",x))))})
ggplot(data = mini.subtrain, aes(y = log(price),x= log(special.character.description+1)))+
        geom_point(col = "purple", alpha = 0.4,size =0.9)+
        facet_grid(factor(shipping) ~ factor(item_condition_id))       

```
The special character count in item description looks like just noise.


```{r,fig.width= 8}
special.character.name <- sapply(mini.subtrain$name,function(x){
return(nchar(str_trim(gsub("[a-zA-Z]|,|\\.|[0-9]","",x))))})
ggplot(data = mini.subtrain, aes(y = log(price),x= log(special.character.name+1)))+
        geom_point(col = "navy", alpha = 0.4,size =0.9)+
        facet_grid(factor(shipping) ~ factor(item_condition_id))       

```

What happens in special.character.name X item_condition_id interaction?


```{r,fig.width=10}
ggplot(data = mini.subtrain, aes(y = log(price),x= log(special.character.name+1) * item_condition_id))+
        geom_point(col = "blue", alpha = 0.4,size =1)+
        facet_grid( . ~ factor(shipping))
```

In the case of name, this looks like not a bad feature. 

```{r}
cor(log(special.character.name+1) * mini.subtrain$item_condition_id,mini.subtrain$excl.name)
```
Sounds like we are not adding a highly correlated feature,either.

How about just counting the stars (*) ?

```{r,fig.width= 10, fig.height=10}
star.name <- str_count(mini.subtrain$name,"\\*") 
star.description <- str_count(mini.subtrain$item_description,"\\*")
pairs(log(mini.subtrain$price) ~ log(star.name +1) + log(star.description+1),pch = 19, col = "navy", cex = 0.2)
```

How about just counting the text starting with stars (*) ?

```{r,fig.width= 5, }
start.name <- str_count(mini.subtrain$name,"^\\*") 
start.description <- str_count(mini.subtrain$item_description,"^\\*")
ggplot(mini.subtrain,aes(y = log(price+1), x = factor(start.name))) +
        geom_boxplot()
ggplot(mini.subtrain,aes(y = log(price+1), x = factor(start.description))) +
        geom_boxplot() 
```

These are not so impressive features. 

How about we just count the numbers?


```{r,fig.width= 10 }
number.name <- str_count(mini.subtrain$name,"[0-9]") 
number.description <- str_count(mini.subtrain$item_description,"[0-9]")
pairs(log(mini.subtrain$price) ~ log(number.name +1) + log(number.description+1),pch = 19, col = "navy", cex = 0.2)
```

How about we just count the $ sign?

```{r,fig.width= 10, }
dollar.name <- str_count(mini.subtrain$name,"\\$") 
dollar.description <- str_count(mini.subtrain$item_description,"\\$")
pairs(log(mini.subtrain$price) ~ log(dollar.name +1) + dollar.description,pch = 19, col = "navy", cex = 0.2)
```

dollar.description looks like a good feature!

```{r,fig.width= 8}
ggplot(data = mini.subtrain, aes(y = log(price),x= dollar.description))+
        geom_point(col = "navy", alpha = 0.4,size =0.9)+
        facet_grid(. ~ factor(shipping))       

```

Let's add dollar.description into our data sets:

```{r}
mini.subtrain$dollar.description <- str_count(mini.subtrain$item_description,"\\$")
subtrain$dollar.description <-  str_count(subtrain$item_description,"\\$")
validation$dollar.description <- str_count(validation$item_description,"\\$")
test$dollar.description <- str_count(test$item_description,"\\$")
```

####Category name

We noted that there are limited category names. Some of these categories may explain certain amount of variance in the price. Let's look at more closely:

```{r, fig.width=10}
summary.category_name <- mini.subtrain %>% group_by(category_name) %>% summarise(median.log.price = median(log(price)), size = n(), perct_fraction = n()/nrow(mini.subtrain)) %>% arrange(desc(median.log.price))
head(summary.category_name,10)
hist( summary.category_name$median.log.price, col ="navy", breaks = 50)
```
It looks like the median log prices binned by category names have a near-gaussian distribution, we have certain categries that could explain some of the most expensive and cheapest items. Let's define these categories:

```{r}
fancy.categories <- summary.category_name$category_name[summary.category_name$median.log.price > quantile(log(mini.subtrain$price),0.95)]
cheap.categories <- summary.category_name$category_name[summary.category_name$median.log.price < quantile(log(mini.subtrain$price),0.05)]

fancy.categories;cheap.categories
```

Let's make categorical features using these category names:
```{r}
mini.subtrain$fancy.categories <- sapply(mini.subtrain$category_name,function(x){
        if (any(x %in% fancy.categories))
                return(1)
        else
                return(0)
})

ggplot(mini.subtrain, aes(y = log(price), x = factor(fancy.categories)))+
        geom_jitter(alpha= 0.4, col ="green")+
        geom_boxplot(fill="navy")
```

```{r}
mini.subtrain$cheap.categories <- sapply(mini.subtrain$category_name,function(x){
        if (any(x %in% cheap.categories))
                return(1)
        else
                return(0)
})

ggplot(mini.subtrain, aes(y = log(price), x = factor(cheap.categories)))+
        geom_jitter(alpha= 0.4, col ="green")+
        geom_boxplot(fill="navy")
```

These could be some useful features to add into our data sets:

```{r}

subtrain$fancy.categories <- sapply(subtrain$category_name,function(x){
        if (any(x %in% fancy.categories))
                return(1)
        else
                return(0)
})

test$fancy.categories <- sapply(test$category_name,function(x){
        if (any(x %in% fancy.categories))
                return(1)
        else
                return(0)
})

validation$fancy.categories <- sapply(validation$category_name,function(x){
        if (any(x %in% fancy.categories))
                return(1)
        else
                return(0)
})



subtrain$cheap.categories <- sapply(subtrain$category_name,function(x){
        if (any(x %in% cheap.categories))
                return(1)
        else
                return(0)
})

test$cheap.categories <- sapply(test$category_name,function(x){
        if (any(x %in% cheap.categories))
                return(1)
        else
                return(0)
})

validation$cheap.categories <- sapply(validation$category_name,function(x){
        if (any(x %in% cheap.categories))
                return(1)
        else
                return(0)
})
```

#### Brand names

We also noted that there are limited brand names. Some of these may explain certain amount of variance in the price. Let's look at more closely:

```{r, fig.width=10}
summary.brand_name <- mini.subtrain %>% group_by(brand_name) %>% summarise(median.log.price = median(log(price)), size = n(), perct_fraction = n()/nrow(mini.subtrain)) %>% arrange(desc(median.log.price))
head(summary.brand_name,10)
hist( summary.brand_name$median.log.price, col ="navy", breaks = 50)
```

```{r}
fancy.brands <- summary.brand_name$brand_name[summary.brand_name$median.log.price > quantile(log(mini.subtrain$price),0.95)]
cheap.brands <- summary.brand_name$brand_name[summary.brand_name$median.log.price < quantile(log(mini.subtrain$price),0.05)]

fancy.brands;cheap.brands
```

Let's make categorical features using these brand names:
```{r}
mini.subtrain$fancy.brands <- sapply(mini.subtrain$brand_name,function(x){
        if (any(x %in% fancy.brands))
                return(1)
        else
                return(0)
})

ggplot(mini.subtrain, aes(y = log(price), x = factor(fancy.brands)))+
        geom_jitter(alpha= 0.4, col ="green")+
        geom_boxplot(fill="navy")
```

```{r}
mini.subtrain$cheap.brands <- sapply(mini.subtrain$brand_name,function(x){
        if (any(x %in% cheap.brands))
                return(1)
        else
                return(0)
})

ggplot(mini.subtrain, aes(y = log(price), x = factor(cheap.brands)))+
        geom_jitter(alpha= 0.4, col ="green")+
        geom_boxplot(fill="navy")
```

These could be some useful features to add into our data sets:

```{r}

subtrain$fancy.brands <- sapply(subtrain$brand_name,function(x){
        if (any(x %in% fancy.brands))
                return(1)
        else
                return(0)
})

test$fancy.brands <- sapply(test$brand_name,function(x){
        if (any(x %in% fancy.brands))
                return(1)
        else
                return(0)
})

validation$fancy.brands <- sapply(validation$brand_name,function(x){
        if (any(x %in% fancy.brands))
                return(1)
        else
                return(0)
})



subtrain$cheap.brands <- sapply(subtrain$brand_name,function(x){
        if (any(x %in% cheap.brands))
                return(1)
        else
                return(0)
})

test$cheap.brands <- sapply(test$brand_name,function(x){
        if (any(x %in% cheap.brands))
                return(1)
        else
                return(0)
})

validation$cheap.brands <- sapply(validation$brand_name,function(x){
        if (any(x %in% cheap.brands))
                return(1)
        else
                return(0)
})
```

#### Suspect marketing words

Here, lets look at some usual suspect marketing words in the item description:

```{r, fig.width=12, fig.height=12}
suspect <- data.frame(log.price = log(mini.subtrain$price+1))
suspect$sale <- str_count(tolower(mini.subtrain$item_description), "sale")
suspect$free <- str_count(tolower(mini.subtrain$item_description), "free")
suspect$save <- str_count(tolower(mini.subtrain$item_description), "save")
suspect$deal <- str_count(tolower(mini.subtrain$item_description), "deal")
suspect$good <- str_count(tolower(mini.subtrain$item_description), "good")
suspect$steal <- str_count(tolower(mini.subtrain$item_description), "steal")
suspect$now <- str_count(tolower(mini.subtrain$item_description), "now")
suspect$cheap <- str_count(tolower(mini.subtrain$item_description), "cheap")
suspect$buy <- str_count(tolower(mini.subtrain$item_description), "buy")
suspect$excellent <- str_count(tolower(mini.subtrain$item_description), "excellent")
suspect$great <- str_count(tolower(mini.subtrain$item_description), "great")

pairs(log.price ~ .,pch = 19, col = "navy", cex = 0.2, data = suspect)
```

Some of these features can be useful. Let's look at if there is any correlation between them and the outcome:

```{r, fig.width=10,fig.height=10}
library(corrplot)
corrplot(cor(suspect), method = c("shade"), bg = "gray", addgrid.col = "gray")
```

They don't appear to be highly correlated. What happens if we put them into a linear model along with the response variable:

```{r,fig.width=7,fig.height=7}
lmfit <- lm(log.price ~ . , data = suspect)
summary(lmfit)
par(mfrow = c(2,2))
plot(lmfit)
```

Even though the model is not perfect, some of these features could be useful to explain variability. I will add them into our data sets:

```{r}

# Write a function to consistently add the features into data sets:

suspect.marketing.feature.engineer <- function(data.set){

        suspect <- data.frame(dummy= 1:nrow(data.set))
        suspect$sale <- str_count(tolower(data.set$item_description), "sale")
        suspect$free <- str_count(tolower(data.set$item_description), "free")
        suspect$save <- str_count(tolower(data.set$item_description), "save")
        suspect$deal <- str_count(tolower(data.set$item_description), "deal")
        suspect$good <- str_count(tolower(data.set$item_description), "good")
        suspect$steal <- str_count(tolower(data.set$item_description), "steal")
        suspect$now <- str_count(tolower(data.set$item_description), "now")
        suspect$cheap <- str_count(tolower(data.set$item_description), "cheap")
        suspect$buy <- str_count(tolower(data.set$item_description), "buy")
        suspect$excellent <- str_count(tolower(data.set$item_description), "excellent")
        suspect$great <- str_count(tolower(data.set$item_description), "great")
        suspect <- suspect[,-1]
        data.set <- cbind(data.set,suspect)
        
        return(data.set)

}

mini.subtrain <- suspect.marketing.feature.engineer(mini.subtrain)
subtrain <- suspect.marketing.feature.engineer(subtrain)
validation <- suspect.marketing.feature.engineer(validation)
test <- suspect.marketing.feature.engineer(test)
```



####Looking for 'free' or 'unpriced' items v.s. most expensive items

Our first suspect would be there are certain keywords that are frequently associated with free items, which could be a good feature to predict these items:
```{r,cache=TRUE}
sum(subtrain$price == 0)
```

There are about 400 free items in the subtraining set.

```{r}
library(tm); library(SnowballC);library(wordcloud)

# Gratefully learned from:
# http://www.sthda.com/english/wiki/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know

text <- subtrain$item_description[subtrain$price == 0]
docs <- Corpus(VectorSource(text))

######################
# Text transformation
######################
# Transformation is performed using tm_map() function to replace, 
# for example, special characters from the text.

#Replacing “/”, “@” and “|” with space:
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")

####################
# Cleaning the text
####################

# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector if needed
# docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
docs <- tm_map(docs, stemDocument)

################################
# Build a term-document matrix
################################

# Document matrix is a table containing the frequency of the words. Column names are words and row names are documents. The function TermDocumentMatrix() from text mining package can be used as follow :

dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)

###########################
#Generate the Word cloud
###########################

# The importance of words can be illustrated as a word cloud as follow :

set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

# Arguments of the word cloud generator function :
# words : the words to be plotted
# freq : their frequencies
# min.freq : words with frequency below min.freq will not be plotted
# max.words : maximum number of words to be plotted
# random.order : plot words in random order. If false, they will be plotted in decreasing frequency
# rot.per : proportion words with 90 degree rotation (vertical text)
# colors : color words from least to most frequent. Use, for example, colors =“black” for single color.

#################################################
#Explore frequent terms and their associations
#################################################

# You can have a look at the frequent terms in the term-document matrix as follow. In the example below we want to find words that occur at least 50 times :

findFreqTerms(dtm, lowfreq = 50)

# You can analyze the association between frequent terms (i.e., terms which correlate) using findAssocs() function. The R code below identifies which words are associated with “size” in the current text :

findAssocs(dtm, terms = "size", corlimit = 0.3)
# This makes good sense!

########################
#Plot word frequencies
#######################

# The frequency of the first 10 frequent words are plotted :

barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col ="navy", main ="Most frequent words",
        xlab = "Word frequencies", horiz = T)
```


How about we look at the top 1% most expensive items?

```{r}
text <- subtrain$item_description[subtrain$price >= quantile(subtrain$price,0.99)]
docs <- Corpus(VectorSource(text))

######################
# Text transformation
######################
# Transformation is performed using tm_map() function to replace, 
# for example, special characters from the text.

#Replacing “/”, “@” and “|” with space:
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")

####################
# Cleaning the text
####################

# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector if needed
# docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
docs <- tm_map(docs, stemDocument)

################################
# Build a term-document matrix
################################

# Document matrix is a table containing the frequency of the words. Column names are words and row names are documents. The function TermDocumentMatrix() from text mining package can be used as follow :

dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)

###########################
#Generate the Word cloud
###########################

# The importance of words can be illustrated as a word cloud as follow :

set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

# Arguments of the word cloud generator function :
# words : the words to be plotted
# freq : their frequencies
# min.freq : words with frequency below min.freq will not be plotted
# max.words : maximum number of words to be plotted
# random.order : plot words in random order. If false, they will be plotted in decreasing frequency
# rot.per : proportion words with 90 degree rotation (vertical text)
# colors : color words from least to most frequent. Use, for example, colors =“black” for single color.

#################################################
#Explore frequent terms and their associations
#################################################

# You can have a look at the frequent terms in the term-document matrix as follow. In the example below we want to find words that occur at least 500 times :

findFreqTerms(dtm, lowfreq = 500)

# You can analyze the association between frequent terms (i.e., terms which correlate) using findAssocs() function. The R code below identifies which words are associated with “size” in the current text :

findAssocs(dtm, terms = "size", corlimit = 0.3)
# This makes good sense!

########################
#Plot word frequencies
#######################

# The frequency of the first 10 frequent words are plotted :

barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col ="navy", main ="Most frequent words",
        xlab = "Word frequencies", horiz = T)
```

Now let's look at the words in free and expensive items together:

```{r}
# Write a function to streamline text mininig

parse_text <- function(text){
        docs <- Corpus(VectorSource(text))

        ######################
        # Text transformation
        ######################
        # Transformation is performed using tm_map() function to replace, 
        # for example, special characters from the text.
        
        #Replacing “/”, “@” and “|” with space:
        toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
        docs <- tm_map(docs, toSpace, "/")
        docs <- tm_map(docs, toSpace, "@")
        docs <- tm_map(docs, toSpace, "\\|")
        
        ####################
        # Cleaning the text
        ####################
        
        # Convert the text to lower case
        docs <- tm_map(docs, content_transformer(tolower))
        # Remove numbers
        docs <- tm_map(docs, removeNumbers)
        # Remove english common stopwords
        docs <- tm_map(docs, removeWords, stopwords("english"))
        # Remove your own stop word
        # specify your stopwords as a character vector if needed
        # docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
        # Remove punctuations
        docs <- tm_map(docs, removePunctuation)
        # Eliminate extra white spaces
        docs <- tm_map(docs, stripWhitespace)
        # Text stemming
        docs <- tm_map(docs, stemDocument)
        
        ################################
        # Build a term-document matrix
        ################################
        
        # Document matrix is a table containing the frequency of the words. Column names are words and row names are documents. The function TermDocumentMatrix() from text mining package can be used as follow :
        
        dtm <- TermDocumentMatrix(docs)
        m <- as.matrix(dtm)
        v <- sort(rowSums(m),decreasing=TRUE)
        d <- data.frame(Word = as.character(names(v)),freq=v, Freq = (v/sum(v)) * 100)
        
        return(d)
}
```


```{r, fig.width=12, fig.height=7}
sorted.table.free <- parse_text(text = mini.subtrain$item_description[mini.subtrain$price == 0])
sorted.table.expensive <- parse_text(text = subtrain$item_description[subtrain$price >= quantile(subtrain$price,0.99)])

# Get top 100 most frequent words from each group 
sorted.table.free.100 <- head(sorted.table.free,100)
sorted.table.expensive.100 <- head(sorted.table.expensive,100)

merged.table <- merge(sorted.table.free.100,sorted.table.expensive.100, by = c("Word"), all = TRUE,
                      suffixes = c("free","expensive"))
row.names(merged.table) = merged.table$Word
merged.table <- as.matrix(merged.table[,which(grepl("^Freq",colnames(merged.table)))])
#Replace NA's with zero
merged.table[is.na(merged.table[,1]),1] <- 0
merged.table[is.na(merged.table[,2]),2] <- 0
library(pheatmap)
pheatmap(t(merged.table))
```

There could be some potential to seperate price segments from each other by using the frequencies of different words. Let's perform a more extensive segmentation analysis by writing a feature creation algorithm.

####Feature engineering in item description through price segmentation

```{r,eval=FALSE}
####################
# Long operation! 
####################

# Divide the subtraining set into 19 log(price) quantiles + 1 "free" item bin
# Totally 19 segments
percentiles <- seq(0.05, 0.99, 0.05)
merged.table <- parse_text(mini.subtrain$item_description[mini.subtrain$price == 0])[,c(1,3)]
colnames(merged.table)[2] <- paste0("price_percentile_0")
merged.table <- rbind(head(merged.table,10),tail(merged.table,10))
mini.subtrain.nonzero <- mini.subtrain[mini.subtrain$price != 0,]

for(i in seq_along(percentiles)){
        # Perform text frequency mining within the first segment and hold the results
        text <- mini.subtrain.nonzero$item_description[log(mini.subtrain.nonzero$price) >= quantile(log(mini.subtrain.nonzero$price),percentiles[i])]
        sorted.table <- parse_text(text)
        sorted.table <- sorted.table[,c(1,3)]
        colnames(sorted.table)[2] <- paste0("price_percentile_%",percentiles[i]*100)
        sorted.table <- rbind(head(sorted.table,10),tail(sorted.table,10))
        merged.table <- merge(merged.table,sorted.table, by = "Word", all = TRUE)
        # continue until all price segments are finished
} 


merged.table <- merged.table[!is.na(merged.table$Word),]

row.names(merged.table) = merged.table$Word
merged.table <- dplyr::select(merged.table, -Word)
merged.table <- as.matrix(merged.table)

#Replace NA's with zero
for (i in 1:20){merged.table[is.na(merged.table[,i]),i] <- 0}
#Save for future use
saveRDS(merged.table,"merged_table.rds")
```

```{r,fig.width=10, fig.height= 7}
# easy load
merged.table <- readRDS("merged_table.rds")
pheatmap(t.merged.table, cluster_rows = FALSE)
```

The item description text mining this way pretty much brings us noise.

####Feature engineering in brand name through price segmentation

```{r,eval=FALSE}
####################
# Long operation! 
####################

# Divide the subtraining set into 19 log(price) quantiles + 1 "free" item bin
# Totally 19 segments
percentiles <- seq(0.05, 0.99, 0.05)
merged.table <- parse_text(mini.subtrain$brand_name[mini.subtrain$price == 0])[,c(1,3)]
colnames(merged.table)[2] <- paste0("price_percentile_0")
merged.table <- rbind(head(merged.table,10),tail(merged.table,10))

mini.subtrain.nonzero <- mini.subtrain[mini.subtrain$price != 0,]

for(i in seq_along(percentiles)){
        # Perform text frequency mining within the first segment and hold the results
        text <- mini.subtrain.nonzero$brand_name[log(mini.subtrain.nonzero$price) >= quantile(log(mini.subtrain.nonzero$price),percentiles[i])]
        sorted.table <- parse_text(text)
        sorted.table <- sorted.table[,c(1,3)]
        colnames(sorted.table)[2] <- paste0("price_percentile_%",percentiles[i]*100)
        sorted.table <- rbind(head(sorted.table,10),tail(sorted.table,10))
        merged.table <- merge(merged.table,sorted.table, by = "Word", all = TRUE)
        # continue until all price segments are finished
} 


merged.table <- merged.table[!is.na(merged.table$Word),]
merged.table <- unique.data.frame(merged.table)

row.names(merged.table) = merged.table$Word
merged.table <- dplyr::select(merged.table, -Word)
merged.table <- as.matrix(merged.table)

#Replace NA's with zero
for (i in 1:20){merged.table[is.na(merged.table[,i]),i] <- 0}
#Save for future use
saveRDS(merged.table,"merged_table.rds")
```

```{r,fig.width=10, fig.height= 7}
# easy load
merged.table <- readRDS("merged_table.rds")
pheatmap(t(merged.table), cluster_rows = FALSE, scale = "row")
```

A slightly different version of the algorithm is just to compare Top50 most frequent words in brand names:


```{r,eval=FALSE}
####################
# Long operation! 
####################

# Divide the subtraining set into 19 log(price) quantiles + 1 "free" item bin
# Totally 19 segments
percentiles <- seq(0.05, 0.99, 0.05)
merged.table <- parse_text(mini.subtrain$brand_name[mini.subtrain$price == 0])[,c(1,3)]
colnames(merged.table)[2] <- paste0("price_percentile_0")
merged.table <- head(merged.table,50)

mini.subtrain.nonzero <- mini.subtrain[mini.subtrain$price != 0,]

for(i in seq_along(percentiles)){
        # Perform text frequency mining within the first segment and hold the results
        text <- mini.subtrain.nonzero$brand_name[log(mini.subtrain.nonzero$price) >= quantile(log(mini.subtrain.nonzero$price),percentiles[i])]
        sorted.table <- parse_text(text)
        sorted.table <- sorted.table[,c(1,3)]
        colnames(sorted.table)[2] <- paste0("price_percentile_%",percentiles[i]*100)
        sorted.table <- head(sorted.table,50)
        merged.table <- merge(merged.table,sorted.table, by = "Word", all = TRUE)
        # continue until all price segments are finished
} 


merged.table <- merged.table[!is.na(merged.table$Word),]
merged.table <- unique.data.frame(merged.table)

row.names(merged.table) = merged.table$Word
merged.table <- dplyr::select(merged.table, -Word)
merged.table <- as.matrix(merged.table)

#Replace NA's with zero
for (i in 1:20){merged.table[is.na(merged.table[,i]),i] <- 0}
#Save for future use
saveRDS(merged.table,"merged_table.rds")
```

```{r,fig.width=10, fig.height= 7}
# easy load
merged.table <- readRDS("merged_table.rds")
pheatmap(t(merged.table), cluster_rows = F, scale = "row")
```

Aha! Here we notice a few things that can be potentially interesting. 

- appl : enriched in most expensive items
- victoria, secret, lululemon,lularo, pink, michael, kor: these are words that can potentially form gradients across the price segments.

Let's engineer new features to add them into our data sets:

```{r}
# Write a function to streamline feature engineering
word.feature.engineer <- function(text.vector,pattern){
        #returns a vector that gives the counts of given pattern in each of the element of the text vector  
        text.vector <- tolower(text.vector)
        counts <- 0
        for(i in seq_along(text.vector)){
                counts[i] <- str_count(text.vector[i],pattern)
        }
        counts[is.na(counts)] <- 0
        return(counts)
}

victoria.test <- word.feature.engineer(mini.subtrain$brand_name,"victoria")
secret.test <- word.feature.engineer(mini.subtrain$brand_name,"secret")
cor(victoria.test,secret.test) # These two words are tighly associated
```

```{r}
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(victoria.test)))+
        geom_boxplot(fill = c("navy","red"))
```

This doesn't seem to be a fascinating feature.

```{r}
pink.test <- word.feature.engineer(mini.subtrain$brand_name,"pink")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(pink.test)))+
        geom_boxplot(fill = c("navy","red"))
```

```{r}
appl.test <- word.feature.engineer(mini.subtrain$brand_name,"appl")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(appl.test)))+
        geom_boxplot(fill = c("navy","red"))
```
```{r}
michael.test <- word.feature.engineer(mini.subtrain$brand_name,"michael")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(michael.test)))+
        geom_boxplot(fill = c("navy","red"))
```

michael in brandname looks like a useful feature. Let's add into our data sets:

```{r}
# Write a function to streamline feature engineering as binary variable
word.feature.engineer.binary <- function(text.vector,pattern){
        #returns a vector that checks a given pattern in each of the element of the text vector  
        text.vector <- tolower(text.vector)
        binary.vector <- sapply(text.vector,function(x){
                return(ifelse(grepl(pattern,x),1,0))
        })
        return(binary.vector)
} 
```



```{r,eval=FALSE} 
# Long operation!
mini.subtrain$michael.brand <- word.feature.engineer.binary(mini.subtrain$brand_name,"michael")
subtrain$michael.brand <- word.feature.engineer.binary(subtrain$brand_name,"michael")
validation$michael.brand <- word.feature.engineer.binary(validation$brand_name,"michael")
test$michael.brand <- word.feature.engineer.binary(test$brand_name,"michael")
```

At this point it is necessary to save engineered data sets for future easy loading:

```{r}
saveRDS(mini.subtrain,"mini_subtrain.rds")
saveRDS(subtrain,"subtrain.rds")
saveRDS(validation,"validation.rds")
saveRDS(test,"test.rds")
```

####Feature engineering in category name through price segmentation

Let;s now look at the category names

```{r,eval=FALSE}
####################
# Long operation! 
####################

# Divide the subtraining set into 19 log(price) quantiles + 1 "free" item bin
# Totally 19 segments
percentiles <- seq(0.05, 0.99, 0.05)
merged.table <- parse_text(mini.subtrain$category_name[mini.subtrain$price == 0])[,c(1,3)]
colnames(merged.table)[2] <- paste0("price_percentile_0")
merged.table <- head(merged.table,50)

mini.subtrain.nonzero <- mini.subtrain[mini.subtrain$price != 0,]

for(i in seq_along(percentiles)){
        # Perform text frequency mining within the first segment and hold the results
        text <- mini.subtrain.nonzero$category_name[log(mini.subtrain.nonzero$price) >= quantile(log(mini.subtrain.nonzero$price),percentiles[i])]
        sorted.table <- parse_text(text)
        sorted.table <- sorted.table[,c(1,3)]
        colnames(sorted.table)[2] <- paste0("price_percentile_%",percentiles[i]*100)
        sorted.table <- head(sorted.table,50)
        merged.table <- merge(merged.table,sorted.table, by = "Word", all = TRUE)
        # continue until all price segments are finished
} 


merged.table <- merged.table[!is.na(merged.table$Word),]
merged.table <- unique.data.frame(merged.table)

row.names(merged.table) = merged.table$Word
merged.table <- dplyr::select(merged.table, -Word)
merged.table <- as.matrix(merged.table)

#Replace NA's with zero
for (i in 1:20){merged.table[is.na(merged.table[,i]),i] <- 0}
#Save for future use
saveRDS(merged.table,"merged_table.rds")
```

```{r,fig.width=10, fig.height= 7}
# easy load
merged.table <- readRDS("merged_table.rds")
pheatmap(t(merged.table), cluster_rows = F, scale = "row")
```

Applying a slighly different version of the algorithm:

```{r,eval=FALSE}
####################
# Long operation! 
####################

# Divide the subtraining set into 19 log(price) quantiles + 1 "free" item bin
# Totally 19 segments
percentiles <- seq(0.05, 0.99, 0.05)
merged.table <- parse_text(mini.subtrain$category_name[mini.subtrain$price == 0])[,c(1,3)]
colnames(merged.table)[2] <- paste0("price_percentile_0")
merged.table <- rbind(head(merged.table,10),tail(merged.table,10))

mini.subtrain.nonzero <- mini.subtrain[mini.subtrain$price != 0,]

for(i in seq_along(percentiles)){
        # Perform text frequency mining within the first segment and hold the results
        text <- mini.subtrain.nonzero$category_name[log(mini.subtrain.nonzero$price) >= quantile(log(mini.subtrain.nonzero$price),percentiles[i])]
        sorted.table <- parse_text(text)
        sorted.table <- sorted.table[,c(1,3)]
        colnames(sorted.table)[2] <- paste0("price_percentile_%",percentiles[i]*100)
        sorted.table <- rbind(head(sorted.table,10),tail(sorted.table,10))
        merged.table <- merge(merged.table,sorted.table, by = "Word", all = TRUE)
        # continue until all price segments are finished
} 


merged.table <- merged.table[!is.na(merged.table$Word),]
merged.table <- unique.data.frame(merged.table)

row.names(merged.table) = merged.table$Word
merged.table <- dplyr::select(merged.table, -Word)
merged.table <- as.matrix(merged.table)

#Replace NA's with zero
for (i in 1:20){merged.table[is.na(merged.table[,i]),i] <- 0}
#Save for future use
saveRDS(merged.table,"merged_table.rds")
```

```{r,fig.width=10, fig.height= 7}
# easy load
merged.table <- readRDS("merged_table.rds")
pheatmap(t(merged.table), cluster_rows = FALSE, scale = "row")
```

The category name also looks pretty much noise when explored in this way.

####Feature engineering in name through price segmentation 

```{r,eval=FALSE}
####################
# Long operation! 
####################

# Divide the subtraining set into 19 log(price) quantiles + 1 "free" item bin
# Totally 19 segments
percentiles <- seq(0.05, 0.99, 0.05)
merged.table <- parse_text(mini.subtrain$name[mini.subtrain$price == 0])[,c(1,3)]
colnames(merged.table)[2] <- paste0("price_percentile_0")
merged.table <- rbind(head(merged.table,10),tail(merged.table,10))

mini.subtrain.nonzero <- mini.subtrain[mini.subtrain$price != 0,]

for(i in seq_along(percentiles)){
        # Perform text frequency mining within the first segment and hold the results
        text <- mini.subtrain.nonzero$name[log(mini.subtrain.nonzero$price) >= quantile(log(mini.subtrain.nonzero$price),percentiles[i])]
        sorted.table <- parse_text(text)
        sorted.table <- sorted.table[,c(1,3)]
        colnames(sorted.table)[2] <- paste0("price_percentile_%",percentiles[i]*100)
        sorted.table <- rbind(head(sorted.table,10),tail(sorted.table,10))
        merged.table <- merge(merged.table,sorted.table, by = "Word", all = TRUE)
        # continue until all price segments are finished
} 


merged.table <- merged.table[!is.na(merged.table$Word),]
merged.table <- unique.data.frame(merged.table)

row.names(merged.table) = merged.table$Word
merged.table <- dplyr::select(merged.table, -Word)
merged.table <- as.matrix(merged.table)

#Replace NA's with zero
for (i in 1:20){merged.table[is.na(merged.table[,i]),i] <- 0}
#Save for future use
saveRDS(merged.table,"merged_table.rds")
```

```{r,fig.width=10, fig.height= 7}
# easy load
merged.table <- readRDS("merged_table.rds")
pheatmap(t(merged.table), cluster_rows = FALSE, scale = "row")
```

```{r}
pink.test <- word.feature.engineer.binary(mini.subtrain$name,"pink")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(pink.test)))+
        geom_boxplot(fill = c("navy","red"))
```

```{r}
size.test <- word.feature.engineer.binary(mini.subtrain$name,"size")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(size.test)))+
        geom_boxplot(fill = c("navy","red"))
```

```{r}
jordan.test <- word.feature.engineer.binary(mini.subtrain$name,"jordan")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(jordan.test)))+
        geom_boxplot(fill = c("navy","red"))
```

```{r}
shirt.test <- word.feature.engineer.binary(mini.subtrain$name,"shirt")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(shirt.test)))+
        geom_boxplot(fill = c("navy","red"))
```

```{r}
michael.test <- word.feature.engineer.binary(mini.subtrain$name,"michael")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(michael.test)))+
        geom_boxplot(fill = c("navy","red"))
cor(michael.test,mini.subtrain$michael.brand)
```

```{r}
iphon.test <- word.feature.engineer.binary(mini.subtrain$name,"iphon")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(iphon.test)))+
        geom_boxplot(fill = c("navy","red"))
```

```{r}
bundl.test <- word.feature.engineer.binary(mini.subtrain$name,"bundl")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(bundl.test)))+
        geom_boxplot(fill = c("navy","red"))
```

Let's add 3 of these features into our data sets as well:

```{r, eval=FALSE}
# Long operation!
mini.subtrain$jordan.name <- word.feature.engineer.binary(mini.subtrain$name,"jordan")
subtrain$jordan.name <- word.feature.engineer.binary(subtrain$name,"jordan")
validation$jordan.name <- word.feature.engineer.binary(validation$name,"jordan")
test$jordan.name <- word.feature.engineer.binary(test$brand,"jordan")

mini.subtrain$iphon.name <- word.feature.engineer.binary(mini.subtrain$name,"iphon")
subtrain$iphon.name <- word.feature.engineer.binary(subtrain$name,"iphon")
validation$iphon.name <- word.feature.engineer.binary(validation$name,"iphon")
test$iphon.name <- word.feature.engineer.binary(test$brand,"iphon")

mini.subtrain$bundl.name <- word.feature.engineer.binary(mini.subtrain$name,"bundl")
subtrain$bundl.name <- word.feature.engineer.binary(subtrain$name,"bundl")
validation$bundl.name <- word.feature.engineer.binary(validation$name,"bundl")
test$bundl.name <- word.feature.engineer.binary(test$brand,"bundl")

#At this point it is necessary to save engineered data sets again for future easy loading:
saveRDS(mini.subtrain,"mini_subtrain.rds")
saveRDS(subtrain,"subtrain.rds")
saveRDS(validation,"validation.rds")
saveRDS(test,"test.rds")
```

#### Looking at [rm] names and descriptions

In the data description it was mentioned that this string was used to replce whereever the price was mentioned in the item description or name. Let's see if the occurance of this placeholder can explain some variance in price:

```{r}
placeholder.test <- word.feature.engineer.binary(mini.subtrain$name,"\\[rm]")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(placeholder.test)))+
        geom_boxplot(fill = c("navy","red"))
```

```{r}
placeholder.test <- word.feature.engineer.binary(mini.subtrain$item_description,"\\[rm]")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(placeholder.test)))+
        geom_boxplot(fill = c("navy","red"))
```

```{r}
placeholder.test <- word.feature.engineer(mini.subtrain$name,"\\[rm]")
ggplot(mini.subtrain,aes(y = log(price +1), x = factor(placeholder.test)))+
        geom_boxplot(fill = c("navy","red","lightgreen"))
```

This doesn't seem to be a good feature.

#### Capital letters

This is something that we haven't looked yet. Let's see if the count the occurance of capital letters that could explain anything in the price:

```{r}
capital.letter.counter <- function(text.vector){
        # Return the occurance of capital letters in the given text vector
       capital.letter.count <- sapply(text.vector,function(x){
        return(length(unlist(str_extract_all(x,"[A-Z]"))))})
        
}

capital.letter.test <- capital.letter.counter(mini.subtrain$item_description)
plot(y =log(mini.subtrain$price), x = log(capital.letter.test),pch = 19, col = "navy", cex = 0.2)
```

This pretty much looks like noise.

```{r}
capital.letter.test <- capital.letter.counter(mini.subtrain$name)
plot(y =log(mini.subtrain$price), x = log(capital.letter.test),pch = 19, col = "navy", cex = 0.2)
```

This pretty much looks like noise.

```{r}
capital.letter.test <- capital.letter.counter(mini.subtrain$brand_name)
plot(y =log(mini.subtrain$price), x = log(capital.letter.test),pch = 19, col = "navy", cex = 0.2)
```

Capital letter count in brand_name might have some trend.

```{r}
capital.letter.test <- capital.letter.counter(mini.subtrain$category_name)
plot(y =log(mini.subtrain$price), x = log(capital.letter.test),pch = 19, col = "navy", cex = 0.2)
```

This pretty much looks like noise.

Let's add the capital letter count in the brand name as a new feature to our data sets:

```{r, eval=FALSE}
# Long operation!
mini.subtrain$cap.letter.brand <- capital.letter.counter(mini.subtrain$brand_name)
subtrain$cap.letter.brand <- capital.letter.counter(subtrain$brand_name)
validation$cap.letter.brand <- capital.letter.counter(validation$brand_name)
test$cap.letter.brand <- capital.letter.counter(test$brand_name)

#At this point it is necessary to save engineered data sets again for future easy loading:
saveRDS(mini.subtrain,"mini_subtrain.rds")
saveRDS(subtrain,"subtrain.rds")
saveRDS(validation,"validation.rds")
saveRDS(test,"test.rds")
```
#### Locking down the data sets

At this point we will finalize feature selection and end up with the final data sets to use them for training of the models and validation.

First, we need to remove string features from data sets:

```{r}
mini.subtrain <-dplyr::select(mini.subtrain, -name, -item_description,-category_name, -brand_name)
subtrain <-dplyr::select(subtrain, -name, -item_description,-category_name, -brand_name)
validation <-dplyr::select(validation, -name, -item_description,-category_name, -brand_name)
test <-dplyr::select(test, -name, -item_description,-category_name, -brand_name)
```

Then, let's look at the correlation between the remaining features:

```{r, fig.width=10,fig.height=10}
library(corrplot)
corrplot(cor(mini.subtrain), method = c("shade"), bg = "gray", addgrid.col = "gray")
```

We don't notice strong correlation between the remaining features. Therefore, we will remain with the final features and lock down the data sets at this point:

```{r,eval=FALSE}
#At this point it is necessary to save engineered data sets again for future easy loading:
saveRDS(mini.subtrain,"mini_subtrain_locked.rds")
saveRDS(subtrain,"subtrain_locked.rds")
saveRDS(validation,"validation_locked.rds")
saveRDS(test,"test_locked.rds")
```

Reload the locked data sets for training our models:

```{r}
mini.subtrain<- readRDS("mini_subtrain_locked.rds")
subtrain <- readRDS("subtrain_locked.rds")
validation <- readRDS("validation_locked.rds")
test <- readRDS("test_locked.rds")
```

We will fit our models using the log(price+1) as the response and will convert back when making our predictions. Let's make this conversion in the data sets:

```{r}
library(dplyr)
mini.subtrain<- mini.subtrain %>% mutate(log.price = log(price +1)) %>% select(-price)
subtrain <- subtrain %>% mutate(log.price = log(price +1)) %>% select(-price)
validation <- validation %>% mutate(log.price = log(price +1)) %>% select(-price)
```

Have a last check at missing data before we move to modeling:

```{r}
apply(is.na(subtrain),2, sum)
apply(is.na(validation),2, sum)
apply(is.na(test),2, sum)
```

Thankfully, the test set is complete. We have very few missing cases in subtrain and validation sets. We will remove them since we have fairly large data sets to train our models.

```{r}
subtrain <- subtrain[complete.cases(subtrain),]
validation <- subtrain[complete.cases(validation),]

```


# Training predictive models for estimating item price

Let's start with training stand alone regression models to estimate the price. Our first approach will be training models using the subtrain set, and trying to estimate our cross-validated error. We will then estimate the error in the validation test to go back and fine tune the models as necessary.

## Lasso regression

We have a long data set, significantly more observations than the number of predictors. Lasso would be a good place to start to see how much we can perform by using a linear model.

```{r}

library(glmnet)

# We create a matrix of predictors
x.training = model.matrix(log.price ~ .-1, data = subtrain)
x.validation = model.matrix( ~ .-1, data = validation[,-27])

y = subtrain$log.price

# Fit lasso regression to training data
fit.lasso <- glmnet(x.training,y, family = "gaussian") #family = "gaussian" for regression
plot(fit.lasso, xvar = "lambda", label = TRUE)
plot(fit.lasso, xvar = "dev", label = TRUE)

```

###Choosing the optimal lambda by using cross-validation

It is best to choose the optimal lambda using cross-validation, with the aim of minimizing **mean squared error:**:

```{r,cache=TRUE}
#cv.lasso will be the list containing the coefficients and information of our optimal model fit using cross-validation (10 fold by default)
set.seed(123)
cv.lasso <- cv.glmnet(x.training,y, family = "gaussian")
plot(cv.lasso)
```

Notice that what we are doing here is actually computationally intense.

1. We get very fine grids of lambda values.
2. Using each lambda value, we perform 10-fold cross validation:

- We fit a model using the Sum of Squares convex optimization and obtain cross-validated error (average of 10 model fits)

3. We continue steps 1 and 2 for all fine grids of lambda values in the range.

The two vertical lines are produced in the plot, the left one marks the model with min error and the middle one shows a little more restricted model 1 standard error away from the minimum error.

**When choosing the optimal model, we might prefer to get the more restricted model that is indicated by the second vertical line, which is default in glmnet package, due to the 1 standard error conversion.**

The final (optimal) model coefficients we obtained by cross-validation and the non-zero features remaining:

```{r}
head(coef(cv.lasso))
```

Note that we only have few features remaining in the linear lasso model. It is nice to see that out of the 5 features remaining, 3 of them are those we engineered from the data set!

###Making predictions using the best lasso model fit

Let's first start with the subtrain set:

```{r, fig.align='center', fig.width=6, fig.height=5}
pred <- predict(cv.lasso, newx = x.training)

#Let's write a function to streamline our analyses of the various models we will train:

log.rmse.training <- function(pred,observed,method){

#pred: vector of predicted values (in log scale)
#observed: vector of observed values (in log scale)
#method: the method name used to built the predictive model        
        
#Calculating the root mean squared error:
rmse.training = sqrt(mean((pred - observed)^2))

#Calculating the Pearson correlation:
cor.training = cor(observed,pred)

plot(x = pred, y = observed, cex = 0.5, col = "navy", pch = 19, 
     main = method,xlab = "Log(Predicted)", ylab = "Log(Observed)")
text(x = max(pred), y = min(observed)+0.7,
     labels = paste0("RMSE: ",rmse.training))
text(x = max(pred), y = min(observed)+0.2,
     labels = paste0("Pr.cor: ",cor.training))
}

log.rmse.training(pred = pred, observed = subtrain$log.price, method = "Lasso")
```

It is clear that the linear model does not explain the prices well. As we expected, we need to pursue different approaches to build our model.

## Support vector machines with a radial kernel with cross validation

We will use parallel processing since training models on the large data sets is computationally intense:

```{r}
# Long operation in a standard computer!
library(caret)
library(parallel);library(doParallel)

# Configure parallel computation
cluster <- makeCluster(detectCores() - 1) # LEave one core for the OS
registerDoParallel(cluster)

# Configure trainControl object
fitControl <- trainControl(method = "cv", number = 5, allowParallel = TRUE)

#Train the model
set.seed(1235432)
SVM <- train(log.price ~ ., data = subtrain,method = "svmRadial", trControl = fitControl, verbose = FALSE) 
#save the model for future use
saveRDS(SVM,"SVM.rds")

# Stop and De-register parallel processing cluster
stopCluster(cluster)
registerDoSEQ()
```

```{r}
# reload the saved model
SVM <- readRDS("SVM.rds")
log.rmse.training(pred = predict(SVM,subtrain),observed = training.processed$Log.SalePrice,method = " Support Vector Machines")
```











