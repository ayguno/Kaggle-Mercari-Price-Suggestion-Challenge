---
title: "KaggleMercariPriceSuggestionChallengeKernel"
author: "Ozan Aygun"
date: "12/15/2017"
output: 
   html_document:
        toc: true
        number_sections: true
        depth: 4
        theme: cerulean
        highlight: tango
        df_print: paged
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(results = "markup", fig.align = "center",
                      fig.width = 5, fig.height = 4,message=FALSE,warning=FALSE)
```


# Introduction and summary



# Loading the data

```{r, cache=TRUE}
# Need to download files from kaggle (links available only after login!)
# Need to get 'archive' package to make a connection to 7z zipped files!
# devtools::install_github("jimhester/archive")
#library(archive);library(readr)
#train <- data.frame(read_delim(archive_read("train.tsv.7z"), delim = "\t"))
#test <- data.frame(read_delim(archive_read("test.tsv.7z"), delim = "\t"))
#sample_submission <- data.frame(read_delim(archive_read("sample_submission.csv.7z"), delim = ","))
# 
# # Save the training and test sets for easy loading in future
#saveRDS(train, "train.rds");saveRDS(test,"test.rds")
# write.csv(sample_submission,"sample_submission.csv", row.names = FALSE)
unlink(dir(pattern = ".7z")) # Finally delete the 7z files from directory
```

# Summarizing the data
```{r,cache=TRUE}
# Read from the rds objects
train <- readRDS("train.rds") ; test <- readRDS("test.rds") 
summary(train)
summary(test)
```
We have a continuous outcome(price), so this is a regression problem. Most of the features appear strings. So we are likely to perform some text mining and feature engineering.

# Partitioning the training set

The training set we have is fairly large, so it will not be wasteful if we split this data set into two for sub-training and validation respectively. We will explore and train the model on the sub-training set, perform initial validation in the validation set, and submit predictions for the test set provided.

```{r,cache=TRUE}
library(caret)
set.seed(12345)
subtrainID <- createDataPartition(y = train$price, p = 0.5, list = FALSE)
subtrain <- train[subtrainID[,1],]
validation <- train[-subtrainID[,1],]
```

# Exploratory data analysis using sub-training set

For the ease of making plots, I will explore a randomly selected subset (5%) of the subtraining set, I will call this set as **mini.subtrain**. Note that even 10% of this data set contains ~37,000 observations!

```{r,cache= TRUE, fig.align= "center", fig.width=12}
set.seed(12345)
mini.subtrainID <- createDataPartition(y = subtrain$price, p = 0.05, list = FALSE)
mini.subtrain <- subtrain[mini.subtrainID[,1],]
```

Let's start looking at the data,

## Distribution of the outcome

```{r,cache=TRUE, fig.width=10}
par(mfrow = c(1,3))
hist(mini.subtrain$price, breaks = 50, col = "navy")
hist(log(mini.subtrain$price), breaks = 50, col = "navy")
```


## Checking for missing values
```{r, cache=TRUE}
apply(is.na(subtrain),2,sum)
```

It appears that most of the missing values are in the category name and brand name (half of the values are mising)

### Generic items are slightly cheaper

One obvious thing to look at is whether the price of items that are missing such features have significantly different prices:
```{r,cache=TRUE}
library(ggplot2)
ggplot(data = mini.subtrain, aes(y = log(price+1),fill =factor(is.na(category_name)), x = factor(is.na(category_name))))+
        geom_boxplot()

ggplot(data = mini.subtrain, aes(y = log(price+1),fill =factor(is.na(brand_name)), x = factor(is.na(brand_name))))+
        geom_boxplot()
```

It is perhaps expected the items with no brand name will be slightly cheaper. Therefore, this looks like a new feature to add into the data sets:

```{r,cache=TRUE}
library(dplyr)
mini.subtrain <- mutate(mini.subtrain,no.brand_name = ifelse(is.na(brand_name),1,0))
subtrain <- mutate(subtrain,no.brand_name = ifelse(is.na(brand_name),1,0))
validation <- mutate(validation,no.brand_name = ifelse(is.na(brand_name),1,0))
test <- mutate(test,no.brand_name = ifelse(is.na(brand_name),1,0))
```

###Uniqueness profile of features

```{r}
uniqueness <- apply(subtrain,2,function(x){return(length(unique(x)))})
names(uniqueness) <- colnames(subtrain)
uniqueness
```

Except 2 binary variables, and one potential categorical variable (item_condition_id), the rest of the features can be regarded as string, and we will create new features from them using text mining.

First, store the ID variables for all data sets and remove them:

```{r}
mini.subtrain.train_id <- mini.subtrain$train_id
mini.subtrain <- dplyr::select(mini.subtrain,-train_id)

subtrain.train_id <- subtrain$train_id
subtrain <- dplyr::select(subtrain,-train_id)

validation.train_id <- validation$train_id
validation <- dplyr::select(validation,-train_id)

test.test_id <- test$test_id
test <- dplyr::select(test,-test_id)
```

### item condition id in relation to shipping and having a brand name

```{r, fig.width=10, cache=TRUE}
ggplot(data = mini.subtrain, aes(y = log(price+1), x = factor(item_condition_id),
                                fill = factor(item_condition_id)))+
                                facet_grid(factor(shipping) ~ factor(no.brand_name))+
                                geom_boxplot()+
                                geom_jitter(alpha = 0.05,size = 0.2, color = "navy")
```

These features alone do not seem to explain too much variability in the price. We need stronger features!

###Text mining and feature engineering

#### Starting simple: special characters and length of the text features

Let's try to understand if there is any relationship between the outcome and the length of various textual features:

```{r,fig.width= 10, fig.height=10}
length.name <- sapply(mini.subtrain$name,nchar) 
length.category <- sapply(mini.subtrain$category_name,nchar)
length.brand <- sapply(mini.subtrain$brand_name,nchar)
length.description <- sapply(mini.subtrain$item_description,nchar)
pairs(log(mini.subtrain$price) ~ length.name + length.category + length.brand + length.description,
      pch = 19, col = "navy", cex = 0.2)
```

This pretty much looks like noise. Let's have a look at some special characters:

```{r,fig.width=10}
# Exclamation mark (!)
library(stringr)
excl.name <- str_count(mini.subtrain$name, "!")
excl.category <- str_count(mini.subtrain$category_name,"!")
excl.brand <- str_count(mini.subtrain$brand_name,"!")
excl.description <- str_count(mini.subtrain$item_description,"!")
pairs(log(mini.subtrain$price) ~ excl.name + excl.category + excl.brand + excl.description,
      pch = 19, col = "purple", cex = 0.2)

```

It would be worth looking further into the ecxl.name and excl.description:

```{r,fig.width=10}
ggplot(data = mini.subtrain, aes(y = log(price),x= excl.name))+
        geom_point(col = "purple", alpha = 0.4,size =0.9)+
        facet_grid(factor(shipping) ~ factor(item_condition_id))
```

What happens in excl.name X item_condition_id interaction?


```{r,fig.width=10}
ggplot(data = mini.subtrain, aes(y = log(price),x= excl.name * item_condition_id))+
        geom_point(col = "blue", alpha = 0.4,size =1)+
        facet_grid( . ~ factor(shipping))
```

How about excl.description?


```{r,fig.width=10}
ggplot(data = mini.subtrain, aes(y = log(price),x= log(excl.description)))+
        geom_point(col = "purple", alpha = 0.4,size =0.9)+
        facet_grid(factor(shipping) ~ factor(item_condition_id))
```

What happens in excl.name X item_condition_id interaction?


```{r,fig.width=10}
ggplot(data = mini.subtrain, aes(y = log(price),x= log(excl.description) * item_condition_id))+
        geom_point(col = "blue", alpha = 0.4,size =1)+
        facet_grid( . ~ factor(shipping))
```

My intuition is the interaction with item condition makes these two new features somewhat stronger. 

Let's add these two new features into the main data sets:

```{r}
mini.subtrain$log.excl.description <- log(excl.description + 1) * mini.subtrain$item_condition_id
mini.subtrain$excl.name <- excl.name * mini.subtrain$item_condition_id

subtrain$log.excl.description <- log(str_count(subtrain$item_description, "!") + 1) * subtrain$item_condition_id
subtrain$excl.name <- str_count(subtrain$name, "!") * subtrain$item_condition_id

validation$log.excl.description <- log(str_count(validation$item_description, "!") + 1) * validation$item_condition_id
validation$excl.name <- str_count(validation$name, "!") * validation$item_condition_id

test$log.excl.description <- log(str_count(test$item_description, "!") + 1) * test$item_condition_id
test$excl.name <- str_count(test$name, "!") * test$item_condition_id
```

How about any characters excluding letters, commas or numbers?

```{r}
special.character <- sapply(mini.subtrain$item_description,function(x){
return(nchar(str_trim(gsub("[a-zA-Z]|,|\\.|[0-9]","",x))})
```


####Looking for 'free' or 'unpriced' items v.s. most expensive items

Our first suspect would be there are certain keywords that are frequently associated with free items, which could be a good feature to predict these items:
```{r,cache=TRUE}
sum(subtrain$price == 0)
```

There are about 400 free items in the subtraining set.

```{r, fig.height=8}
v <- subtrain$item_description[subtrain$price == 0]
write.table(v,"v.txt")
c1 <- scan("v.txt", what = "character", sep = "\n")
c2 <- tolower(c1)
c3 <- unlist(strsplit(c2, "\\W")) # \\W regex for non-word character
c3 <- c3[-(which(c3 == ""))]
freq <- table(c3)
freq1 <- sort(freq, decreasing = TRUE)
freq2 <- (freq1/sum(freq1))*100 # % frequency of the word amonst all words in the section
sorted.table.free <- data.frame(Word = names(freq1),freq2)
ggplot(data = sorted.table.free[1:50,])+
    geom_point(aes(x = Freq, y = reorder(Word,Freq),size = 0.3), color = "navy")
        
```

Some of the text we pick here are probably things that are not correctly interpreted. They form the most frequent text.

How about we look at the top 1% most expensive items?

```{r,fig.height=8}
v <- subtrain$item_description[subtrain$price >= quantile(subtrain$price,0.99)]
write.table(v,"v.txt")
c1 <- scan("v.txt", what = "character", sep = "\n")
c2 <- tolower(c1)
c3 <- unlist(strsplit(c2, "\\W")) # \\W regex for non-word character
c3 <- c3[-(which(c3 == ""))]
freq <- table(c3)
freq1 <- sort(freq, decreasing = TRUE)
freq2 <- (freq1/sum(freq1))*100 # % frequency of the word amonst all words in the section
sorted.table.expensive <- data.frame(Word = names(freq1),freq2)
ggplot(data = sorted.table.expensive[1:50,])+
    geom_point(aes(x = Freq, y = reorder(Word,Freq),size = 0.3), color = "navy")
```

Now let's look at the words in free and expensive items together:

```{r, fig.width=12, fig.height=7}
# Get top 50 most frequent words from each group after removing the most frequent words

sorted.table.free.100 <- sorted.table.free[order(sorted.table.free$Freq, decreasing = TRUE),][40:90,]
sorted.table.expensive.100 <-sorted.table.expensive[order(sorted.table.expensive$Freq,decreasing = TRUE),][40:90,]

merged.table <- merge(sorted.table.free.100,sorted.table.expensive.100, by = c("Word"), all = TRUE,
                      suffixes = c("free","expensive"))
row.names(merged.table) = merged.table$Word
merged.table <- as.matrix(merged.table[,c(3,5)])
#Replace NA's with zero
merged.table[is.na(merged.table[,1]),1] <- 0
merged.table[is.na(merged.table[,2]),2] <- 0
library(pheatmap)
pheatmap(t(merged.table))
```

There could be some potential to seperate price segments from each other by using the frequencies of different words. Let's perform a more extensive segmentation analysis.

```{r, eval=FALSE}
# Long operation! 

# Divide the subtrain data into 10 quantiles after setting aside the free items
subtrain.nonzero <- subtrain[subtrain$price != 0,]
sorted.table.free.50 <- sorted.table.free[order(sorted.table.free$Freq, decreasing = TRUE),][1:50,]
merged.table <- sorted.table.free.50[,c(1,3)]
colnames(merged.table)[2] <- paste0("price_percentile_0")
# Totally 21 segments
percentiles <- seq(0.05, 0.99, 0.1)
for(i in seq_along(percentiles)){
# For each segment, calculate word frequencies in the item description, get top 50 most frequent words
        v <- subtrain.nonzero$item_description[subtrain.nonzero$price >= quantile(subtrain.nonzero$price,percentiles[i])]
        write.table(v,"v.txt")
        c1 <- scan("v.txt", what = "character", sep = "\n")
        c2 <- tolower(c1)
        c3 <- unlist(strsplit(c2, "\\W")) # \\W regex for non-word character
        c3 <- c3[-(which(c3 == ""))]
        freq <- table(c3)
        freq1 <- sort(freq, decreasing = TRUE)
        freq2 <- (freq1/sum(freq1))*100 # % frequency of the word amonst all words in the section
        sorted.table <- data.frame(Word = names(freq1),freq2)
        sorted.table <- sorted.table[order(sorted.table$Freq, decreasing = TRUE),][1:50,]
        sorted.table <- sorted.table[,c(1,3)]
        colnames(sorted.table)[2] <- paste0("price_percentile_%",percentiles[i]*100)
# Merge into a data frame
        merged.table <- merge(merged.table,sorted.table, by = c("Word"), all = TRUE)
}
```

```{r, fig.width=10, fig.height= 7}
#Save for future use
# saveRDS(merged.table,"merged_table.rds")
# easy load
merged.table <- readRDS("merged_table.rds")

merged.table <- merged.table[!is.na(merged.table$Word),]

row.names(merged.table) = merged.table$Word
merged.table <- dplyr::select(merged.table, -Word)
merged.table <- as.matrix(merged.table)

#Replace NA's with zero
for (i in 1:11){merged.table[is.na(merged.table[,i]),i] <- 0}

# Cluster into a heatmap
pheatmap(t(merged.table),scale = "row", cluster_rows = FALSE)
```